{
  "Lib": {
    "Version": 779,
    "Items": {
      "26566QM5": {
        "Version": 650,
        "Title": "Accurately Measuring Global Risk of Ampliﬁcation Attacks using AmpMap",
        "Abstract": "Many recent DDoS attacks rely on ampliﬁcation, where an attacker induces public servers to generate a large volume of network trafﬁc to a victim. In this paper, we argue for a low-footprint Internet health monitoring service that can systematically and continuously quantify this risk to inform mitigation efforts. Unfortunately, the problem is challenging because ampliﬁcation is a complex function of query (header) values and server instances. As such, existing techniques that enumerate the total number of servers or focus on a speciﬁc ampliﬁcation-inducing query are fundamentally imprecise. In designing AmpMap, we leverage key structural insights to develop an efﬁcient approach that searches across the space of protocol headers and servers. Using AmpMap, we scanned thousands of servers for 6 UDP-based protocols. We ﬁnd that relying on prior recommendations to block or rate-limit speciﬁc queries still leaves open substantial residual risk as they miss many other ampliﬁcation-inducing query patterns. We also observe signiﬁcant variability across servers and protocols, and thus prior approaches that rely on server census can substantially misestimate ampliﬁcation risk.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Soo-Jin",
            "lastName": "Moon"
          },
          {
            "firstName": "Yucheng",
            "lastName": "Yin"
          },
          {
            "firstName": "Rahul Anand",
            "lastName": "Sharma"
          },
          {
            "firstName": "Yifei",
            "lastName": "Yuan"
          },
          {
            "firstName": "Jonathan M",
            "lastName": "Spring"
          },
          {
            "firstName": "Vyas",
            "lastName": "Sekar"
          }
        ],
        "Attachments": {
          "IRZDTT2U": {
            "Version": 668,
            "ContentType": "application/pdf",
            "Filename": "Moon et al. - Accurately Measuring Global Risk of Ampliﬁcation A.pdf"
          }
        }
      },
      "2FFX7SZS": {
        "Version": 721,
        "Title": "Learning Discrete Energy-based Models via Auxiliary-variable Local Exploration",
        "Abstract": "Discrete structures play an important role in applications like program language modeling and software engineering. Current approaches to predicting complex structures typically consider autoregressive models for their tractability, with some sacriﬁce in ﬂexibility. Energy-based models (EBMs) on the other hand offer a more ﬂexible and thus more powerful approach to modeling such distributions, but require partition function estimation. In this paper we propose ALOE, a new algorithm for learning conditional and unconditional EBMs for discrete structured data, where parameter gradients are estimated using a learned sampler that mimics local search. We show that the energy function and sampler can be trained efﬁciently via a new variational form of power iteration, achieving a better trade-off between ﬂexibility and tractability. Experimentally, we show that learning local search leads to signiﬁcant improvements in challenging application domains. Most notably, we present an energy model guided fuzzer for software testing that achieves comparable performance to well engineered fuzzing engines like libfuzzer.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Hanjun",
            "lastName": "Dai"
          },
          {
            "firstName": "Rishabh",
            "lastName": "Singh"
          },
          {
            "firstName": "Bo",
            "lastName": "Dai"
          },
          {
            "firstName": "Charles",
            "lastName": "Sutton"
          },
          {
            "firstName": "Dale",
            "lastName": "Schuurmans"
          }
        ],
        "Attachments": {
          "KJA2NA2E": {
            "Version": 722,
            "ContentType": "application/pdf",
            "Filename": "Dai et al. - Learning Discrete Energy-based Models via Auxiliar.pdf"
          }
        }
      },
      "2FTL5LT8": {
        "Version": 330,
        "Title": "INSTRIM: Lightweight Instrumentation for Coverage-guided Fuzzing",
        "Abstract": "Empowered by instrumentation, coverage-guided fuzzing monitors the program execution path taken by an input, and prioritizes inputs based on their contribution to code coverage. Although instrumenting every basic block ensures full visibility, it slows down the fuzzer and thus the speed of vulnerability discovery. This paper shows that thanks to common program structures (e.g., directed acyclic subgraphs and simple loops) and compiler optimization (e.g., knowledge of incoming edges), it is possible to accurately reconstruct coverage information by instrumenting only a small fraction of basic blocks. Speciﬁcally, we formulate the problem as a path differentiation problem on the control ﬂow graph, and propose an efﬁcient algorithm to select basic blocks that need to be instrumented so that different execution paths remain differentiable. We extend AFL to support such CFG-aware instrumentation. Our experiment results conﬁrm that, compared with full instrumentation, our CFG-aware instrumentation only needs to instrument about 20% of basic blocks while offering 1.04–1.78x speedup during fuzzing. Finally, we highlight several technical challenges and promising research directions to further improve instrumentation for fuzzing.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Chin-Chia",
            "lastName": "Hsu"
          },
          {
            "firstName": "Che-Yu",
            "lastName": "Wu"
          },
          {
            "firstName": "Hsu-Chun",
            "lastName": "Hsiao"
          },
          {
            "firstName": "Shih-Kun",
            "lastName": "Huang"
          }
        ],
        "Attachments": {
          "BQRNTHRC": {
            "Version": 331,
            "ContentType": "application/pdf",
            "Filename": "Hsu et al. - 2018 - INSTRIM Lightweight Instrumentation for Coverage-.pdf"
          }
        }
      },
      "2IH8QN7G": {
        "Version": 500,
        "Title": "Fuzzing Android: a recipe for uncovering vulnerabilities inside system components in Android",
        "Abstract": "The paper focuses on a fuzzing approach that can be used to uncover different types of vulnerabilities inside multiple core system components of the Android OS. The paper will introduce the general idea behind this approach and how it applies to several real-life targets from the Android OS backed up by discovered vulnerabilities. The list of components that were targeted and found vulnerable contains: the Stagefright framework, the mediaserver process, the Android APK install process, the installd daemon, dex2oat, ART.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Alexandru",
            "lastName": "Blanda"
          }
        ],
        "Attachments": {
          "PVJMLKWE": {
            "Version": 501,
            "ContentType": "application/pdf",
            "Filename": "Blanda - Fuzzing Android a recipe for uncovering vulnerabi.pdf"
          }
        }
      },
      "2KCWLHLL": {
        "Version": 443,
        "Title": "TOFU: Target-Oriented FUzzer",
        "Abstract": "Program fuzzing---providing randomly constructed inputs to a computer program---has proved to be a powerful way to uncover bugs, find security vulnerabilities, and generate test inputs that increase code coverage. In many applications, however, one is interested in a target-oriented approach-one wants to find an input that causes the program to reach a specific target point in the program. We have created TOFU (for Target-Oriented FUzzer) to address the directed fuzzing problem. TOFU's search is biased according to a distance metric that scores each input according to how close the input's execution trace gets to the target locations. TOFU is also input-structure aware (i.e., the search makes use of a specification of a superset of the program's allowed inputs). Our experiments on xmllint show that TOFU is 28% faster than AFLGo, while reaching 45% more targets. Moreover, both distance-guided search and exploitation of knowledge of the input structure contribute significantly to TOFU's performance.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Zi",
            "lastName": "Wang"
          },
          {
            "firstName": "Ben",
            "lastName": "Liblit"
          },
          {
            "firstName": "Thomas",
            "lastName": "Reps"
          }
        ],
        "Attachments": {
          "4K7DSDYX": {
            "Version": 444,
            "ContentType": "application/pdf",
            "Filename": "Wang et al. - 2020 - TOFU Target-Oriented FUzzer.pdf"
          }
        }
      },
      "2PYUSB9F": {
        "Version": 277,
        "Title": "Statistical Comparisons of Classiﬁers over Multiple Data Sets",
        "Abstract": "While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classiﬁers: the Wilcoxon signed ranks test for comparison of two classiﬁers and the Friedman test with the corresponding post-hoc tests for comparison of more classiﬁers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Janez",
            "lastName": "Demsˇar"
          },
          {
            "firstName": "Janez",
            "lastName": "Demsar"
          }
        ],
        "Attachments": {
          "2J6V9QFC": {
            "Version": 278,
            "ContentType": "application/pdf",
            "Filename": "Demsˇar and Demsar - Statistical Comparisons of Classiﬁers over Multipl.pdf"
          }
        }
      },
      "2WNTCYPZ": {
        "Version": 771,
        "Title": "Language-Agnostic Representation Learning of Source Code from Structure and Context",
        "Abstract": "",
        "ItemType": "",
        "Creators": [],
        "Attachments": {
          "AZJXIBY6": {
            "Version": 771,
            "ContentType": "application/pdf",
            "Filename": "2021 - Language-Agnostic Representation Learning of Sourc.pdf"
          }
        }
      },
      "35MBSMPM": {
        "Version": 534,
        "Title": "Speculative Memory Checkpointing",
        "Abstract": "High-frequency memory checkpointing is an important technique in several application domains, such as automatic error recovery (where frequent checkpoints allow the system to transparently mask failures) and application debugging (where frequent checkpoints enable fast and accurate time-traveling support). Unfortunately, existing (typically incremental) checkpointing frameworks incur substantial performance overhead in high-frequency memory checkpointing applications, thus discouraging their adoption in practice. This paper presents Speculative Memory Checkpointing (SMC ), a new low-overhead technique for high-frequency memory checkpointing. Our motivating analysis identiﬁes key bottlenecks in existing frameworks and demonstrates that the performance of traditional incremental checkpointing strategies in high-frequency checkpointing scenarios is not optimal. To ﬁll the gap, SMC relies on working set estimation algorithms to eagerly checkpoint the memory pages that belong to the writable working set of the running program and only lazily checkpoint the memory pages that do not. Our experimental results demonstrate that SMC is eﬀective in reducing the performance overhead of prior solutions, is robust to variations in the workload, and incurs modest memory overhead compared to traditional incremental checkpointing.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Dirk",
            "lastName": "Vogt"
          },
          {
            "firstName": "Armando",
            "lastName": "Miraglia"
          },
          {
            "firstName": "Georgios",
            "lastName": "Portokalidis"
          },
          {
            "firstName": "Herbert",
            "lastName": "Bos"
          },
          {
            "firstName": "Andy",
            "lastName": "Tanenbaum"
          },
          {
            "firstName": "Cristiano",
            "lastName": "Giuffrida"
          }
        ],
        "Attachments": {
          "7775HSK9": {
            "Version": 535,
            "ContentType": "application/pdf",
            "Filename": "Vogt et al. - 2015 - Speculative Memory Checkpointing.pdf"
          }
        }
      },
      "3JF4VI7Z": {
        "Version": 102,
        "Title": "Fuzzification: Anti-Fuzzing Techniques",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Jinho",
            "lastName": "Jung"
          },
          {
            "firstName": "Hong",
            "lastName": "Hu"
          },
          {
            "firstName": "David",
            "lastName": "Solodukhin"
          },
          {
            "firstName": "Daniel",
            "lastName": "Pagan"
          },
          {
            "firstName": "Kyu Hyung",
            "lastName": "Lee"
          },
          {
            "firstName": "Taesoo",
            "lastName": "Kim"
          }
        ],
        "Attachments": {
          "NUZY7X8K": {
            "Version": 103,
            "ContentType": "application/pdf",
            "Filename": "Jung et al. - 2019 - Fuzzification Anti-Fuzzing Techniques.pdf"
          }
        }
      },
      "3ZGZ884I": {
        "Version": 582,
        "Title": "The FORTRAN automatic coding system",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "J. W.",
            "lastName": "Backus"
          },
          {
            "firstName": "H.",
            "lastName": "Stern"
          },
          {
            "firstName": "I.",
            "lastName": "Ziller"
          },
          {
            "firstName": "R. A.",
            "lastName": "Hughes"
          },
          {
            "firstName": "R.",
            "lastName": "Nutt"
          },
          {
            "firstName": "R. J.",
            "lastName": "Beeber"
          },
          {
            "firstName": "S.",
            "lastName": "Best"
          },
          {
            "firstName": "R.",
            "lastName": "Goldberg"
          },
          {
            "firstName": "L. M.",
            "lastName": "Haibt"
          },
          {
            "firstName": "H. L.",
            "lastName": "Herrick"
          },
          {
            "firstName": "R. A.",
            "lastName": "Nelson"
          },
          {
            "firstName": "D.",
            "lastName": "Sayre"
          },
          {
            "firstName": "P. B.",
            "lastName": "Sheridan"
          }
        ],
        "Attachments": {
          "6P5NG3HE": {
            "Version": 583,
            "ContentType": "application/pdf",
            "Filename": "Backus et al. - 1957 - The FORTRAN automatic coding system.pdf"
          }
        }
      },
      "3ZMNAPIY": {
        "Version": 561,
        "Title": "SonicBOOM: The 3rd Generation Berkeley Out-of-Order Machine",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Jerry",
            "lastName": "Zhao"
          },
          {
            "firstName": "Ben",
            "lastName": "Korpan"
          },
          {
            "firstName": "Abraham",
            "lastName": "Gonzalez"
          },
          {
            "firstName": "Krste",
            "lastName": "Asanovic"
          }
        ],
        "Attachments": {
          "E3XH5VNC": {
            "Version": 562,
            "ContentType": "application/pdf",
            "Filename": "Zhao et al. - SonicBOOM The 3rd Generation Berkeley Out-of-Orde.pdf"
          }
        }
      },
      "4F53E37U": {
        "Version": 358,
        "Title": "IJON: Exploring Deep State Spaces via Fuzzing",
        "Abstract": "Although current fuzz testing (fuzzing) methods are highly effective, there are still many situations such as complex state machines where fully automated approaches fail. State-ofthe-art fuzzing methods offer very limited ability for a human to interact and aid the fuzzer in such cases. More speciﬁcally, most current approaches are limited to adding a dictionary or new seed inputs to guide the fuzzer. When dealing with complex programs, these mechanisms are unable to uncover new parts of the code base.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Sergej",
            "lastName": "Schumilo"
          },
          {
            "firstName": "Ali",
            "lastName": "Abbasi"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "GJQ8LLXV": {
            "Version": 359,
            "ContentType": "application/pdf",
            "Filename": "Aschermann et al. - IJON Exploring Deep State Spaces via Fuzzing.pdf"
          }
        }
      },
      "4FZAFIFH": {
        "Version": 11,
        "Title": "Meltdown",
        "Abstract": "The security of computer systems fundamentally relies on memory isolation, e.g., kernel address ranges are marked as non-accessible and are protected from user access. In this paper, we present Meltdown. Meltdown exploits side effects of out-of-order execution on modern processors to read arbitrary kernel-memory locations including personal data and passwords. Out-of-order execution is an indispensable performance feature and present in a wide range of modern processors. The attack works on different Intel microarchitectures since at least 2010 and potentially other processors are affected. The root cause of Meltdown is the hardware. The attack is independent of the operating system, and it does not rely on any software vulnerabilities. Meltdown breaks all security assumptions given by address space isolation as well as paravirtualized environments and, thus, every security mechanism building upon this foundation. On affected systems, Meltdown enables an adversary to read memory of other processes or virtual machines in the cloud without any permissions or privileges, affecting millions of customers and virtually every user of a personal computer. We show that the KAISER defense mechanism for KASLR has the important (but inadvertent) side effect of impeding Meltdown. We stress that KAISER must be deployed immediately to prevent large-scale exploitation of this severe information leakage.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Moritz",
            "lastName": "Lipp"
          },
          {
            "firstName": "Michael",
            "lastName": "Schwarz"
          },
          {
            "firstName": "Daniel",
            "lastName": "Gruss"
          },
          {
            "firstName": "Thomas",
            "lastName": "Prescher"
          },
          {
            "firstName": "Werner",
            "lastName": "Haas"
          },
          {
            "firstName": "Stefan",
            "lastName": "Mangard"
          },
          {
            "firstName": "Paul",
            "lastName": "Kocher"
          },
          {
            "firstName": "Daniel",
            "lastName": "Genkin"
          },
          {
            "firstName": "Yuval",
            "lastName": "Yarom"
          },
          {
            "firstName": "Mike",
            "lastName": "Hamburg"
          }
        ],
        "Attachments": {
          "INK7VRGP": {
            "Version": 12,
            "ContentType": "application/pdf",
            "Filename": "Lipp et al. - 2018 - Meltdown.pdf"
          }
        }
      },
      "4LZKIS2Y": {
        "Version": 560,
        "Title": "UniFuzz: Optimizing Distributed Fuzzing via Dynamic Centralized Task Scheduling",
        "Abstract": "Fuzzing is one of the most efﬁcient technology for vulnerability detection. Since the fuzzing process is computingintensive and the performance improved by algorithm optimization is limited, recent research seeks to improve fuzzing performance by utilizing parallel computing. However, parallel fuzzing has to overcome challenges such as task conﬂicts, scalability in a distributed environment, synchronization overhead, and workload imbalance. In this paper, we design and implement UniFuzz, a distributed fuzzing optimization based on a dynamic centralized task scheduling. UniFuzz evaluates and distributes seeds in a centralized manner to avoid task conﬂicts. It uses a “request-response” scheme to dynamically distribute fuzzing tasks, which avoids workload imbalance. Besides, UniFuzz can adaptively switch the role of computing cores between evaluating, and fuzzing, which avoids the potential bottleneck of seed evaluation. To improve synchronization efﬁciency, UniFuzz shares different fuzzing information in a different way according to their characteristics, and the average overhead of synchronization is only about 0.4%. We evaluated UniFuzz with real-world programs, and the results show that UniFuzz outperforms state-ofthe-art tools, such as AFL, PAFL and EnFuzz. Most importantly, the experiment reveals a counter-intuitive result that parallel fuzzing can achieve a super-linear acceleration to the singlecore fuzzing. We made a detailed explanation and proved it with additional experiments. UniFuzz also discovered 16 real-world vulnerabilities.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Xu",
            "lastName": "Zhou"
          },
          {
            "firstName": "Pengfei",
            "lastName": "Wang"
          },
          {
            "firstName": "Chenyifan",
            "lastName": "Liu"
          },
          {
            "firstName": "Tai",
            "lastName": "Yue"
          },
          {
            "firstName": "Yingying",
            "lastName": "Liu"
          },
          {
            "firstName": "Congxi",
            "lastName": "Song"
          },
          {
            "firstName": "Kai",
            "lastName": "Lu"
          },
          {
            "firstName": "Qidi",
            "lastName": "Yin"
          }
        ],
        "Attachments": {
          "F8CK4PNA": {
            "Version": 557,
            "ContentType": "application/pdf",
            "Filename": "Zhou et al. - 2020 - UniFuzz Optimizing Distributed Fuzzing via Dynami.pdf"
          }
        }
      },
      "4UFP8LQS": {
        "Version": 396,
        "Title": "Intriguing Properties of Adversarial ML Attacks in the Problem Space",
        "Abstract": "Recent research efforts on adversarial ML have investigated problem-space attacks, focusing on the generation of real evasive objects in domains where, unlike images, there is no clear inverse mapping to the feature space (e.g., software). However, the design, comparison, and real-world implications of problem-space attacks remain underexplored. This paper makes two major contributions. First, we propose a novel formalization for adversarial ML evasion attacks in the problem-space, which includes the definition of a comprehensive set of constraints on available transformations, preserved semantics, robustness to preprocessing, and plausibility. We shed light on the relationship between feature space and problem space, and we introduce the concept of side-effect features as the byproduct of the inverse feature-mapping problem. This enables us to define and prove necessary and sufficient conditions for the existence of problem-space attacks. We further demonstrate the expressive power of our formalization by using it to describe several attacks from related literature across different domains. Second, building on our formalization, we propose a novel problem-space attack on Android malware that overcomes past limitations. Experiments on a dataset with 170K Android apps from 2017 and 2018 show the practical feasibility of evading a state-of-the-art malware classifier along with its hardened version. Our results demonstrate that \"adversarial-malware as a service\" is a realistic threat, as we automatically generate thousands of realistic and inconspicuous adversarial applications at scale, where on average it takes only a few minutes to generate an adversarial app. Our formalization of problem-space attacks paves the way to more principled research in this domain.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Fabio",
            "lastName": "Pierazzi"
          },
          {
            "firstName": "Feargus",
            "lastName": "Pendlebury"
          },
          {
            "firstName": "Jacopo",
            "lastName": "Cortellazzi"
          },
          {
            "firstName": "Lorenzo",
            "lastName": "Cavallaro"
          }
        ],
        "Attachments": {
          "U4IJFJVP": {
            "Version": 397,
            "ContentType": "application/pdf",
            "Filename": "Pierazzi et al. - 2020 - Intriguing Properties of Adversarial ML Attacks in.pdf"
          }
        }
      },
      "4UYQUNVF": {
        "Version": 616,
        "Title": "APL since 1978",
        "Abstract": "The Evolution of APL, the HOPL I paper by Falkoff and Iverson on APL, recounted the fundamental design principles which shaped the implementation of the APL language in 1966, and the early uses and other influences which shaped its first decade of enhancements. In the 40 years that have elapsed since HOPL I, several dozen APL implementations have come and gone. In the first decade or two, interpreters were typically born and buried along with the hardware or operating system that they were created for. More recently, the use of C as an implementation language provided APL interpreters with greater longevity and portability. APL started its life on IBM mainframes which were time-shared by multiple users. As the demand for computing resources grew and costs dropped, APL first moved in-house to mainframes, then to mini- and micro-computers. Today, APL runs on PCs and tablets, Apples and Raspberry Pis, smartphones and watches. The operating systems, and the software application platforms that APL runs on, have evolved beyond recognition. Tools like database systems have taken over many of the tasks that were initially implemented in APL or provided by the APL system, and new capabilities like parallel hardware have also changed the focus of design and implementation efforts through the years. The first wave of significant language enhancements occurred shortly after HOPL I, resulting in so-called second-generation APL systems. The most important feature of the second generation is the addition of general arrays—in which any item of an array can be another array—and a number of new functions and operators aligned with, if not always motivated by, the new data structures. The majority of implementations followed IBM’s path with APL2 “floating” arrays; others aligned themselves with SHARP APL and “grounded” arrays. While the APL2 style of APL interpreters came to dominate the mainstream of the APL community, two new cousins of APL descended from the SHARP APL family tree: J (created by Iverson and Hui) and k (created by Arthur Whitney). We attempt to follow a reasonable number of threads through the last 40 years, to identify the most important factors that have shaped the evolution of APL. We will discuss the details of what we believe are the most significant language features that made it through the occasionally unnatural selection imposed by the loss of habitats that disappeared with hardware, software platforms, and business models. The history of APL now spans six decades. It is still the case, as Falkoff and Iverson remarked at the end of the HOPL I paper, that: Although this is not the place to discuss the future, it should be remarked that the evolution of APL is far from finished.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Roger K. W.",
            "lastName": "Hui"
          },
          {
            "firstName": "Morten J.",
            "lastName": "Kromberg"
          }
        ],
        "Attachments": {
          "AX48MUTV": {
            "Version": 598,
            "ContentType": "application/pdf",
            "Filename": "Hui and Kromberg - 2020 - APL since 1978.pdf"
          },
          "JEJ4I27S": {
            "Version": 616,
            "ContentType": "application/pdf",
            "Filename": "Hui and Kromberg - 2020 - APL since 1978.pdf"
          }
        }
      },
      "4YEID9YP": {
        "Version": 684,
        "Title": "NAUTILUS: Fishing for Deep Bugs with Grammars",
        "Abstract": "Fuzz testing is a well-known method for efﬁciently identifying bugs in programs. Unfortunately, when programs that require highly-structured inputs such as interpreters are fuzzed, many fuzzing methods struggle to pass the syntax checks: interpreters often process inputs in multiple stages, ﬁrst syntactic and then semantic correctness is checked. Only if both checks are passed, the interpreted code gets executed. This prevents fuzzers from executing “deeper” — and hence potentially more interesting — code. Typically, two valid inputs that lead to the execution of different features in the target program require too many mutations for simple mutation-based fuzzers to discover: making small changes like bit ﬂips usually only leads to the execution of error paths in the parsing engine. So-called grammar fuzzers are able to pass the syntax checks by using ContextFree Grammars. Feedback can signiﬁcantly increase the efﬁciency of fuzzing engines and is commonly used in state-of-the-art mutational fuzzers which do not use grammars. Yet, current grammar fuzzers do not make use of code coverage, i.e., they do not know whether any input triggers new functionality.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Tommaso",
            "lastName": "Frassetto"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          },
          {
            "firstName": "Patrick",
            "lastName": "Jauernig"
          },
          {
            "firstName": "Ahmad-Reza",
            "lastName": "Sadeghi"
          },
          {
            "firstName": "Daniel",
            "lastName": "Teuchert"
          }
        ],
        "Attachments": {
          "49YVZ8VK": {
            "Version": 709,
            "ContentType": "application/pdf",
            "Filename": "Aschermann et al. - 2019 - NAUTILUS Fishing for Deep Bugs with Grammars.pdf"
          }
        }
      },
      "4Z9G6PK4": {
        "Version": 517,
        "Title": "NXNSAttack: Recursive DNS Ineﬃciencies and Vulnerabilities",
        "Abstract": "The Domain Name System (DNS) infrastructure, a most critical system the Internet depends on, has recently been the target for diﬀerent DDoS and other cyber-attacks, e.g., the notorious Mirai botnet. While these attacks can be destructive to both recursive and authoritative DNS servers, little is known about how recursive resolvers operate under such attacks (e.g., NXDomain, water-torture). In this paper, we point out a new vulnerability and show an attack, the NXNSAttack, that exploits the way DNS recursive resolvers operate when receiving NS referral response that contains nameservers but without their corresponding IP addresses (i.e., missing glue-records). We show that the number of DNS messages exchanged in a typical resolution process might be much higher in practice than what is expected in theory, mainly due to a proactive resolution of name-servers’ IP addresses. We show how this ineﬃciency becomes a bottleneck and might be used to mount a devastating attack against either or both, recursive resolvers and authoritative servers. The NXNSAttack is more eﬀective than the NXDomain attack: i) It reaches an ampliﬁcation factor of more than 1620x on the number of packets exchanged by the recursive resolver. ii) Besides the negative cache, the attack also saturates the ‘NS’ resolver caches. In an attempt to mitigate the attack impact, we propose enhancements to the recursive resolvers algorithm to prevent unnecessary proactive fetches. Finally, we implement our Max1Fetch enhancement on the BIND resolver and show that Max1Fetch does not degrade the recursive resolvers performance, throughput and latency, by testing it on real-world traﬃc data-sets.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Lior",
            "lastName": "Shaﬁr"
          },
          {
            "firstName": "Yehuda",
            "lastName": "Afek"
          },
          {
            "firstName": "Anat",
            "lastName": "Bremler-Barr"
          }
        ],
        "Attachments": {
          "HSIGTRUT": {
            "Version": 518,
            "ContentType": "application/pdf",
            "Filename": "Shaﬁr et al. - NXNSAttack Recursive DNS Ineﬃciencies and Vulnera.pdf"
          }
        }
      },
      "52ZVGM39": {
        "Version": 480,
        "Title": "DLDay18_paper_27.pdf",
        "Abstract": "",
        "ItemType": "",
        "Creators": null,
        "Attachments": {}
      },
      "5AV3PNP5": {
        "Version": 158,
        "Title": "TensorFuzz: Debugging Neural Networks with Coverage-Guided Fuzzing",
        "Abstract": "Machine learning models are notoriously difficult to interpret and debug. This is particularly true of neural networks. In this work, we introduce automated software testing techniques for neural networks that are well-suited to discovering errors which occur only for rare inputs. Specifically, we develop coverage-guided fuzzing (CGF) methods for neural networks. In CGF, random mutations of inputs to a neural network are guided by a coverage metric toward the goal of satisfying user-specified constraints. We describe how fast approximate nearest neighbor algorithms can provide this coverage metric. We then discuss the application of CGF to the following goals: finding numerical errors in trained neural networks, generating disagreements between neural networks and quantized versions of those networks, and surfacing undesirable behavior in character level language models. Finally, we release an open source library called TensorFuzz that implements the described techniques.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Augustus",
            "lastName": "Odena"
          },
          {
            "firstName": "Ian",
            "lastName": "Goodfellow"
          }
        ],
        "Attachments": {
          "DVD8BY5J": {
            "Version": 161,
            "ContentType": "application/pdf",
            "Filename": "Odena and Goodfellow - 2018 - TensorFuzz Debugging Neural Networks with Coverag.pdf"
          }
        }
      },
      "5ILGUIQP": {
        "Version": 361,
        "Title": "Category Theory in Context",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Emily",
            "lastName": "Riehl"
          }
        ],
        "Attachments": {
          "AYQZASHC": {
            "Version": 362,
            "ContentType": "application/pdf",
            "Filename": "Riehl - Category Theory in Context.pdf"
          }
        }
      },
      "5J7X8SL4": {
        "Version": 247,
        "Title": "Not All Coverage Measurements Are Equal: Fuzzing by Coverage Accounting for Input Prioritization",
        "Abstract": "Coverage-based fuzzing has been actively studied and widely adopted for ﬁnding vulnerabilities in real-world software applications. With coverage information, such as statement coverage and transition coverage, as the guidance of input mutation, coverage-based fuzzing can generate inputs that cover more code and thus ﬁnd more vulnerabilities without prerequisite information such as input format. Current coveragebased fuzzing tools treat covered code equally. All inputs that contribute to new statements or transitions are kept for future mutation no matter what the statements or transitions are and how much they impact security. Although this design is reasonable from the perspective of software testing that aims at full code coverage, it is inefﬁcient for vulnerability discovery since that 1) current techniques are still inadequate to reach full coverage within a reasonable amount of time, and that 2) we always want to discover vulnerabilities early so that it can be ﬁxed promptly. Even worse, due to the non-discriminative code coverage treatment, current fuzzing tools suffer from recent anti-fuzzing techniques and become much less effective in ﬁnding vulnerabilities from programs enabled with anti-fuzzing schemes.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yanhao",
            "lastName": "Wang"
          },
          {
            "firstName": "Xiangkun",
            "lastName": "Jia"
          },
          {
            "firstName": "Yuwei",
            "lastName": "Liu"
          },
          {
            "firstName": "Kyle",
            "lastName": "Zeng"
          },
          {
            "firstName": "Tiffany",
            "lastName": "Bao"
          },
          {
            "firstName": "Dinghao",
            "lastName": "Wu"
          },
          {
            "firstName": "Purui",
            "lastName": "Su"
          }
        ],
        "Attachments": {
          "92WABRVA": {
            "Version": 248,
            "ContentType": "application/pdf",
            "Filename": "Wang et al. - 2020 - Not All Coverage Measurements Are Equal Fuzzing b.pdf"
          }
        }
      },
      "5LCMVEH4": {
        "Version": 266,
        "Title": "Continuity and robustness of programs",
        "Abstract": "Computer scientists have long believed that software is different from physical systems in one fundamental way: while the latter have continuous dynamics, the former do not. In this paper, we argue that notions of continuity from mathematical analysis are relevant and interesting even for software. First, we demonstrate that many everyday programs are continuous (i.e., arbitrarily small changes to their inputs only cause arbitrarily small changes to their outputs) or Lipschitz continuous (i.e., when their inputs change, their outputs change at most proportionally). Second, we give an mostly-automatic framework for verifying that a program is continuous or Lipschitz, showing that traditional, discrete approaches to proving programs correct can be extended to reason about these properties. An immediate application of our analysis is in reasoning about the robustness of programs that execute on uncertain inputs. In the longer run, it raises hopes for a toolkit for reasoning about programs that freely combines logical and analytical mathematics.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Swarat",
            "lastName": "Chaudhuri"
          },
          {
            "firstName": "Sumit",
            "lastName": "Gulwani"
          },
          {
            "firstName": "Roberto",
            "lastName": "Lublinerman"
          }
        ],
        "Attachments": {
          "M9ZABQLV": {
            "Version": 267,
            "ContentType": "application/pdf",
            "Filename": "Chaudhuri et al. - 2012 - Continuity and robustness of programs.pdf"
          }
        }
      },
      "5MU2456M": {
        "Version": 204,
        "Title": "Not all bytes are equal: Neural byte sieve for fuzzing",
        "Abstract": "We present a technique using neural networks learning patterns in the input files from past fuzzing explorations to guide future fuzzing explorations.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "William",
            "lastName": "Blum"
          },
          {
            "firstName": "Mohit",
            "lastName": "Rajpal"
          },
          {
            "firstName": "Rishabh",
            "lastName": "Singh"
          }
        ],
        "Attachments": {
          "E4JCZEUG": {
            "Version": 205,
            "ContentType": "application/pdf",
            "Filename": "Blum et al. - 2017 - Not all bytes are equal Neural byte sieve for fuz.pdf"
          }
        }
      },
      "5YCGK49U": {
        "Version": 87,
        "Title": "Smart Greybox Fuzzing",
        "Abstract": "Coverage-based greybox fuzzing (CGF) is one of the most successful methods for automated vulnerability detection. Given a seed file (as a sequence of bits), CGF randomly flips, deletes or bits to generate new files. CGF iteratively constructs (and fuzzes) a seed corpus by retaining those generated files which enhance coverage. However, random bitflips are unlikely to produce valid files (or valid chunks in files), for applications processing complex file formats. In this work, we introduce smart greybox fuzzing (SGF) which leverages a high-level structural representation of the seed file to generate new files. We define innovative mutation operators that work on the virtual file structure rather than on the bit level which allows SGF to explore completely new input domains while maintaining file validity. We introduce a novel validity-based power schedule that enables SGF to spend more time generating files that are more likely to pass the parsing stage of the program, which can expose vulnerabilities much deeper in the processing logic. Our evaluation demonstrates the effectiveness of SGF. On several libraries that parse structurally complex files, our tool AFLSmart explores substantially more paths (up to 200%) and exposes more vulnerabilities than baseline AFL. Our tool AFLSmart has discovered 42 zero-day vulnerabilities in widely-used, well-tested tools and libraries; so far 17 CVEs were assigned.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Van-Thuan",
            "lastName": "Pham"
          },
          {
            "firstName": "Marcel",
            "lastName": "Böhme"
          },
          {
            "firstName": "Andrew E.",
            "lastName": "Santosa"
          },
          {
            "firstName": "Alexandru Răzvan",
            "lastName": "Căciulescu"
          },
          {
            "firstName": "Abhik",
            "lastName": "Roychoudhury"
          }
        ],
        "Attachments": {
          "7FRYJ8M9": {
            "Version": 88,
            "ContentType": "application/pdf",
            "Filename": "Pham et al. - 2018 - Smart Greybox Fuzzing.pdf"
          }
        }
      },
      "65LQJIWE": {
        "Version": 624,
        "Title": "The evolution of Smalltalk: from Smalltalk-72 through Squeak",
        "Abstract": "This paper presents a personal view of the evolution of six generations of Smalltalk in which the author played a part, starting with Smalltalk-72 and progressing through Smalltalk-80 to Squeak and Etoys. It describes the forces that brought each generation into existence, the technical innovations that characterized it, and the growth in understanding of object-orientation and personal computing that emerged. It summarizes what that generation achieved and how it affected the future, both within the evolving group of developers and users, and in the outside world. The early Smalltalks were not widely accessible because they ran only on proprietary Xerox hardware; because of this, few people have experience with these important historical artifacts. To make them accessible, the paper provides links to live simulations that can be run in present-day web browsers. These simulations offer the ability to run pre-defined scripts, but also allow the user to go off-script, browse the details of the implementation, and try anything that could be done in the original system. An appendix includes anecdotal and technical aspects of how examples of each generation of Smalltalk were recovered, and how order was teased out of chaos to the point that these old systems could be brought back to life.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Daniel",
            "lastName": "Ingalls"
          }
        ],
        "Attachments": {
          "Z75GKX7S": {
            "Version": 633,
            "ContentType": "application/pdf",
            "Filename": "Ingalls - 2020 - The evolution of Smalltalk from Smalltalk-72 thro.pdf"
          }
        }
      },
      "65VZE8CX": {
        "Version": 244,
        "Title": "FuzzGuard: Filtering out Unreachable Inputs in Directed Grey-box Fuzzing through Deep Learning",
        "Abstract": "Recently, directed grey-box fuzzing (DGF) becomes popular in the ﬁeld of software testing. Different from coverage-based fuzzing whose goal is to increase code coverage for triggering more bugs, DGF is designed to check whether a piece of potentially buggy code (e.g., string operations) really contains a bug. Ideally, all the inputs generated by DGF should reach the target buggy code until triggering the bug. It is a waste of time when executing with unreachable inputs. Unfortunately, in real situations, large numbers of the generated inputs cannot let a program execute to the target, greatly impacting the efﬁciency of fuzzing, especially when the buggy code is embedded in the code guarded by various constraints.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Peiyuan",
            "lastName": "Zong"
          },
          {
            "firstName": "Tao",
            "lastName": "Lv"
          },
          {
            "firstName": "Dawei",
            "lastName": "Wang"
          },
          {
            "firstName": "Zizhuang",
            "lastName": "Deng"
          },
          {
            "firstName": "Ruigang",
            "lastName": "Liang"
          },
          {
            "firstName": "Kai",
            "lastName": "Chen"
          }
        ],
        "Attachments": {
          "Z7VGEVU8": {
            "Version": 245,
            "ContentType": "application/pdf",
            "Filename": "Zong et al. - FuzzGuard Filtering out Unreachable Inputs in Dir.pdf"
          }
        }
      },
      "6S4E6I5D": {
        "Version": 378,
        "Title": "The IX Operating System: Combining Low Latency, High Throughput, and Efficiency in a Protected Dataplane",
        "Abstract": "The conventional wisdom is that aggressive networking requirements, such as high packet rates for small messages and μs-scale tail latency, are best addressed outside the kernel, in a user-level networking stack. We present ix, a dataplane operating system that provides high I/O performance and high resource efficiency while maintaining the protection and isolation benefits of existing kernels. ix uses hardware virtualization to separate management and scheduling functions of the kernel (control plane) from network processing (dataplane). The dataplane architecture builds upon a native, zero-copy API and optimizes for both bandwidth and latency by dedicating hardware threads and networking queues to dataplane instances, processing bounded batches of packets to completion, and eliminating coherence traffic and multicore synchronization. The control plane dynamically adjusts core allocations and voltage/frequency settings to meet service-level objectives. We demonstrate that ix outperforms Linux and a user-space network stack significantly in both throughput and end-to-end latency. Moreover, ix improves the throughput of a widely deployed, key-value store by up to 6.4× and reduces tail latency by more than 2× . With three varying load patterns, the control plane saves 46%--54% of processor energy, and it allows background jobs to run at 35%--47% of their standalone throughput.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Adam",
            "lastName": "Belay"
          },
          {
            "firstName": "George",
            "lastName": "Prekas"
          },
          {
            "firstName": "Mia",
            "lastName": "Primorac"
          },
          {
            "firstName": "Ana",
            "lastName": "Klimovic"
          },
          {
            "firstName": "Samuel",
            "lastName": "Grossman"
          },
          {
            "firstName": "Christos",
            "lastName": "Kozyrakis"
          },
          {
            "firstName": "Edouard",
            "lastName": "Bugnion"
          }
        ],
        "Attachments": {
          "FV8CP3N3": {
            "Version": 379,
            "ContentType": "application/pdf",
            "Filename": "Belay et al. - 2016 - The IX Operating System Combining Low Latency, Hi.pdf"
          }
        }
      },
      "6TJ8UGLQ": {
        "Version": 418,
        "Title": "Scalable Graph-based Bug Search for Firmware Images",
        "Abstract": "Because of rampant security breaches in IoT devices, searching vulnerabilities in massive IoT ecosystems is more crucial than ever. Recent studies have demonstrated that control-ﬂow graph (CFG) based bug search techniques can be effective and accurate in IoT devices across different architectures. However, these CFG-based bug search approaches are far from being scalable to handle an enormous amount of IoT devices in the wild, due to their expensive graph matching overhead. Inspired by rich experience in image and video search, we propose a new bug search scheme which addresses the scalability challenge in existing cross-platform bug search techniques and further improves search accuracy. Unlike existing techniques that directly conduct searches based upon raw features (CFGs) from the binary code, we convert the CFGs into high-level numeric feature vectors. Compared with the CFG feature, high-level numeric feature vectors are more robust to code variation across different architectures, and can easily achieve realtime search by using state-of-the-art hashing techniques.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Qian",
            "lastName": "Feng"
          },
          {
            "firstName": "Rundong",
            "lastName": "Zhou"
          },
          {
            "firstName": "Chengcheng",
            "lastName": "Xu"
          },
          {
            "firstName": "Yao",
            "lastName": "Cheng"
          },
          {
            "firstName": "Brian",
            "lastName": "Testa"
          },
          {
            "firstName": "Heng",
            "lastName": "Yin"
          }
        ],
        "Attachments": {
          "8MQZLTHB": {
            "Version": 419,
            "ContentType": "application/pdf",
            "Filename": "Feng et al. - 2016 - Scalable Graph-based Bug Search for Firmware Image.pdf"
          }
        }
      },
      "6Y6ZEPEP": {
        "Version": 475,
        "Title": "Using Deep Learning to Solve Computer Security Challenges: A Survey",
        "Abstract": "Although using machine learning techniques to solve computer security challenges is not a new idea, the rapidly emerging Deep Learning technology has recently triggered a substantial amount of interests in the computer security community. This paper seeks to provide a dedicated review of the very recent research works on using Deep Learning techniques to solve computer security challenges. In particular, the review covers eight computer security problems being solved by applications of Deep Learning: security-oriented program analysis, defending return-oriented programming (ROP) attacks, achieving control-flow integrity (CFI), defending network attacks, malware classification, system-event-based anomaly detection, memory forensics, and fuzzing for software security.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yoon-Ho",
            "lastName": "Choi"
          },
          {
            "firstName": "Peng",
            "lastName": "Liu"
          },
          {
            "firstName": "Zitong",
            "lastName": "Shang"
          },
          {
            "firstName": "Haizhou",
            "lastName": "Wang"
          },
          {
            "firstName": "Zhilong",
            "lastName": "Wang"
          },
          {
            "firstName": "Lan",
            "lastName": "Zhang"
          },
          {
            "firstName": "Junwei",
            "lastName": "Zhou"
          },
          {
            "firstName": "Qingtian",
            "lastName": "Zou"
          }
        ],
        "Attachments": {
          "K9EW2NBT": {
            "Version": 476,
            "ContentType": "application/pdf",
            "Filename": "Choi et al. - 2019 - Using Deep Learning to Solve Computer Security Cha.pdf"
          }
        }
      },
      "7IG5FUM9": {
        "Version": 510,
        "Title": "SYMBION: Interleaving Symbolic with Concrete Execution",
        "Abstract": "Symbolic execution is a powerful technique for exploring programs and generating inputs that drive them into speciﬁc states. However, symbolic execution is also known to suffer from severe limitations, which prevent its application to real-world software. For example, symbolically executing programs requires modeling their interactions with the surrounding environment (e.g., libraries, operating systems). Unfortunately, models are usually created manually, introducing considerable approximations of the programs behaviors and signiﬁcant imprecision in the analysis. In addition, as the complexity of the system under analysis grows, additional models are needed, making this process unsustainable. For these reasons, in this paper we propose a novel technique that allows interleaving symbolic execution with concrete execution, focusing the symbolic exploration only on interesting portions of code. We call this approach interleaved symbolic execution. The key idea of our approach is to re-use the concrete environment to run the input program, and then synchronize the results of the environment interactions with the symbolic execution engine. As a consequence, our approach does not make any assumption about such interactions, and it is agnostic with respect to the concrete environment. We implement a prototype for this technique, SYMBION, and we demonstrate its effectiveness by analyzing real-world malware, showing that it allows us to effectively skip complex portions of code that do not need to be analyzed symbolically.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Fabio",
            "lastName": "Gritti"
          },
          {
            "firstName": "Lorenzo",
            "lastName": "Fontana"
          },
          {
            "firstName": "Eric",
            "lastName": "Gustafson"
          },
          {
            "firstName": "Fabio",
            "lastName": "Pagani"
          },
          {
            "firstName": "Andrea",
            "lastName": "Continella"
          },
          {
            "firstName": "Christopher",
            "lastName": "Kruegel"
          },
          {
            "firstName": "Giovanni",
            "lastName": "Vigna"
          }
        ],
        "Attachments": {
          "89I7WNDE": {
            "Version": 511,
            "ContentType": "application/pdf",
            "Filename": "Gritti et al. - SYMBION Interleaving Symbolic with Concrete Execu.pdf"
          }
        }
      },
      "7NAZXGEQ": {
        "Version": 589,
        "Title": "A history of Haskell: being lazy with class",
        "Abstract": "This paper describes the history of Haskell, including its genesis and principles, technical contributions, implementations and tools, and applications and impact.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Paul",
            "lastName": "Hudak"
          },
          {
            "firstName": "John",
            "lastName": "Hughes"
          },
          {
            "firstName": "Simon",
            "lastName": "Peyton Jones"
          },
          {
            "firstName": "Philip",
            "lastName": "Wadler"
          }
        ],
        "Attachments": {
          "UL6EUQ9P": {
            "Version": 707,
            "ContentType": "application/pdf",
            "Filename": "Hudak et al. - 2007 - A history of Haskell being lazy with class.pdf"
          }
        }
      },
      "7Z4CWJT6": {
        "Version": 136,
        "Title": "Context-Sensitive Fencing: Securing Speculative Execution via Microcode Customization",
        "Abstract": "This paper describes context-sensitive fencing (CSF), a microcode-level defense against multiple variants of Spectre. CSF leverages the ability to dynamically alter the decoding of the instruction stream, to seamlessly inject new micro-ops, including fences, only when dynamic conditions indicate they are needed. This enables the processor to protect against the attack, but with minimal impact on the efficacy of key performance features such as speculative execution.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Mohammadkazem",
            "lastName": "Taram"
          },
          {
            "firstName": "Ashish",
            "lastName": "Venkat"
          },
          {
            "firstName": "Dean",
            "lastName": "Tullsen"
          }
        ],
        "Attachments": {
          "4Z5FVWNR": {
            "Version": 137,
            "ContentType": "application/pdf",
            "Filename": "Taram et al. - 2019 - Context-Sensitive Fencing Securing Speculative Ex.pdf"
          }
        }
      },
      "8BRRS6T5": {
        "Version": 531,
        "Title": "Time-Travel Testing of Android Apps",
        "Abstract": "Android testing tools generate sequences of input events to exercise the state space of the app-under-test. Existing search-based techniques systematically evolve a population of event sequences so as to achieve certain objectives such as maximal code coverage. The hope is that the mutation of fit event sequences leads to the generation of even fitter sequences. However, the evolution of event sequences may be ineffective. Our key insight is that pertinent app states which contributed to the original sequence’s fitness may not be reached by a mutated event sequence. The original path through the state space is truncated at the point of mutation.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Zhen",
            "lastName": "Dong"
          }
        ],
        "Attachments": {
          "UP47HILY": {
            "Version": 532,
            "ContentType": "application/pdf",
            "Filename": "Dong - Time-Travel Testing of Android Apps.pdf"
          }
        }
      },
      "8DPNBLS8": {
        "Version": 463,
        "Title": "Efficient Binary-Level Coverage Analysis",
        "Abstract": "Code coverage analysis plays an important role in the software testing process. More recently, the remarkable effectiveness of coverage feedback has triggered a broad interest in feedback-guided fuzzing. In this work, we introduce bcov, a tool for binary-level coverage analysis. Our tool statically instruments x86-64 binaries in the ELF format without compiler support. We implement several techniques to improve efficiency and scale to large real-world software. First, we bring Agrawals [1] probe pruning technique to binary-level instrumentation and effectively leverage its superblocks to reduce overhead. Second, we introduce sliced microexecution, a robust technique for jump table analysis which improves CFG precision and enables us to instrument jump table entries. Additionally, smaller instructions in x86-64 pose a challenge for inserting detours. To address this challenge, we aggressively exploit padding bytes and systematically host detours in neighboring basic blocks. We evaluate bcov on a corpus of 95 binaries compiled from eight popular and well-tested packages like FFmpeg and LLVM. Two instrumentation policies, with different edge-level precision, are used to patch all functions in this corpus - over 1.6 million functions. Our precise policy has average performance and memory overheads of 14% and 22% respectively. Instrumented binaries do not introduce any test regressions. The reported coverage is highly accurate with an average F-score of 99.86%. Finally, our jump table analysis is comparable to that of IDA Pro on gcc binaries and outperforms it on clang binaries.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "M. Ammar Ben",
            "lastName": "Khadra"
          },
          {
            "firstName": "Dominik",
            "lastName": "Stoffel"
          },
          {
            "firstName": "Wolfgang",
            "lastName": "Kunz"
          }
        ],
        "Attachments": {
          "X2LVXEZ8": {
            "Version": 464,
            "ContentType": "application/pdf",
            "Filename": "Khadra et al. - 2020 - Efficient Binary-Level Coverage Analysis.pdf"
          }
        }
      },
      "8F979T58": {
        "Version": 200,
        "Title": "NEUZZ: Efficient Fuzzing with Neural Program Smoothing",
        "Abstract": "Fuzzing has become the de facto standard technique for finding software vulnerabilities. However, even state-of-the-art fuzzers are not very efficient at finding hard-to-trigger software bugs. Most popular fuzzers use evolutionary guidance to generate inputs that can trigger different bugs. Such evolutionary algorithms, while fast and simple to implement, often get stuck in fruitless sequences of random mutations. Gradient-guided optimization presents a promising alternative to evolutionary guidance. Gradient-guided techniques have been shown to significantly outperform evolutionary algorithms at solving high-dimensional structured optimization problems in domains like machine learning by efficiently utilizing gradients or higher-order derivatives of the underlying function. However, gradient-guided approaches are not directly applicable to fuzzing as real-world program behaviors contain many discontinuities, plateaus, and ridges where the gradient-based methods often get stuck. We observe that this problem can be addressed by creating a smooth surrogate function approximating the discrete branching behavior of target program. In this paper, we propose a novel program smoothing technique using surrogate neural network models that can incrementally learn smooth approximations of a complex, real-world program's branching behaviors. We further demonstrate that such neural network models can be used together with gradient-guided input generation schemes to significantly improve the fuzzing efficiency. Our extensive evaluations demonstrate that NEUZZ significantly outperforms 10 state-of-the-art graybox fuzzers on 10 real-world programs both at finding new bugs and achieving higher edge coverage. NEUZZ found 31 unknown bugs that other fuzzers failed to find in 10 real world programs and achieved 3X more edge coverage than all of the tested graybox fuzzers for 24 hours running.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Dongdong",
            "lastName": "She"
          },
          {
            "firstName": "Kexin",
            "lastName": "Pei"
          },
          {
            "firstName": "Dave",
            "lastName": "Epstein"
          },
          {
            "firstName": "Junfeng",
            "lastName": "Yang"
          },
          {
            "firstName": "Baishakhi",
            "lastName": "Ray"
          },
          {
            "firstName": "Suman",
            "lastName": "Jana"
          }
        ],
        "Attachments": {
          "C2WSPKMM": {
            "Version": 201,
            "ContentType": "application/pdf",
            "Filename": "She et al. - 2019 - NEUZZ Efficient Fuzzing with Neural Program Smoot.pdf"
          }
        }
      },
      "8HYZEJXG": {
        "Version": 368,
        "Title": "Machine Learning-Based Analysis of Program Binaries: A Comprehensive Study",
        "Abstract": "Binary code analysis is crucial in various software engineering tasks, such as malware detection, code refactoring, and plagiarism detection. With the rapid growth of software complexity and the increasing number of heterogeneous computing platforms, binary analysis is particularly critical and more important than ever. Traditionally adopted techniques for binary code analysis are facing multiple challenges, such as the need for cross-platform analysis, high scalability and speed, and improved fidelity, to name a few. To meet these challenges, machine learning-based binary code analysis frameworks attract substantial attention due to their automated feature extraction and drastically reduced efforts needed on large-scale programs. In this paper, we provide the taxonomy of machine learning-based binary code analysis, describe the recent advances and key findings on the topic, and discuss the key challenges and opportunities. Finally, we present our thoughts for future directions on this topic.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Hongfa",
            "lastName": "Xue"
          },
          {
            "firstName": "Shaowen",
            "lastName": "Sun"
          },
          {
            "firstName": "Guru",
            "lastName": "Venkataramani"
          },
          {
            "firstName": "Tian",
            "lastName": "Lan"
          }
        ],
        "Attachments": {
          "TJ29CU9R": {
            "Version": 369,
            "ContentType": "application/pdf",
            "Filename": "Xue et al. - 2019 - Machine Learning-Based Analysis of Program Binarie.pdf"
          }
        }
      },
      "8TV7GHU6": {
        "Version": 755,
        "Title": "AddressSanitizer: A Fast Address Sanity Checker",
        "Abstract": "Memory access bugs, including buffer overﬂows and uses of freed heap memory, remain a serious problem for programming languages like C and C++. Many memory error detectors exist, but most of them are either slow or detect a limited set of bugs, or both.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Konstantin",
            "lastName": "Serebryany"
          },
          {
            "firstName": "Derek",
            "lastName": "Bruening"
          },
          {
            "firstName": "Alexander",
            "lastName": "Potapenko"
          },
          {
            "firstName": "Dmitry",
            "lastName": "Vyukov"
          }
        ],
        "Attachments": {
          "Q96ZBN7E": {
            "Version": 756,
            "ContentType": "application/pdf",
            "Filename": "Serebryany et al. - AddressSanitizer A Fast Address Sanity Checker.pdf"
          }
        }
      },
      "9EV6D4FN": {
        "Version": 176,
        "Title": "ret2spec: Speculative Execution Using Return Stack Buffers",
        "Abstract": "Speculative execution is an optimization technique that has been part of CPUs for over a decade. It predicts the outcome and target of branch instructions to avoid stalling the execution pipeline. However, until recently, the security implications of speculative code execution have not been studied. In this paper, we investigate a special type of branch predictor that is responsible for predicting return addresses. To the best of our knowledge, we are the first to study return address predictors and their consequences for the security of modern software. In our work, we show how return stack buffers (RSBs), the core unit of return address predictors, can be used to trigger misspeculations. Based on this knowledge, we propose two new attack variants using RSBs that give attackers similar capabilities as the documented Spectre attacks. We show how local attackers can gain arbitrary speculative code execution across processes, e.g., to leak passwords another user enters on a shared system. Our evaluation showed that the recent Spectre countermeasures deployed in operating systems can also cover such RSB-based cross-process attacks. Yet we then demonstrate that attackers can trigger misspeculation in JIT environments in order to leak arbitrary memory content of browser processes. Reading outside the sandboxed memory region with JIT-compiled code is still possible with 80\\% accuracy on average.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Giorgi",
            "lastName": "Maisuradze"
          },
          {
            "firstName": "Christian",
            "lastName": "Rossow"
          }
        ],
        "Attachments": {
          "7QTJQKY7": {
            "Version": 177,
            "ContentType": "application/pdf",
            "Filename": "Maisuradze and Rossow - 2018 - ret2spec Speculative Execution Using Return Stack.pdf"
          }
        }
      },
      "9FHJXPXE": {
        "Version": 193,
        "Title": "FUDGE: Fuzz Driver Generation at Scale",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Domagoj",
            "lastName": "Babic"
          },
          {
            "firstName": "Stefan",
            "lastName": "Bucur"
          },
          {
            "firstName": "Yaohui",
            "lastName": "Chen"
          },
          {
            "firstName": "Franjo",
            "lastName": "Ivancic"
          },
          {
            "firstName": "Tim",
            "lastName": "King"
          },
          {
            "firstName": "Markus",
            "lastName": "Kusano"
          },
          {
            "firstName": "Caroline",
            "lastName": "Lemieux"
          },
          {
            "firstName": "László",
            "lastName": "Szekeres"
          },
          {
            "firstName": "Wei",
            "lastName": "Wang"
          }
        ],
        "Attachments": {
          "5UXL9JBX": {
            "Version": 194,
            "ContentType": "application/pdf",
            "Filename": "Babic et al. - 2019 - FUDGE Fuzz Driver Generation at Scale.pdf"
          }
        }
      },
      "9GERSHSE": {
        "Version": 624,
        "Title": "The history of Standard ML",
        "Abstract": "The ML family of strict functional languages, which includes F#, OCaml, and Standard ML, evolved from the Meta Language of the LCF theorem proving system developed by Robin Milner and his research group at the University of Edinburgh in the 1970s. This paper focuses on the history of Standard ML, which plays a central role in this family of languages, as it was the first to include the complete set of features that we now associate with the name “ML” (i.e., polymorphic type inference, datatypes with pattern matching, modules, exceptions, and mutable state). Standard ML, and the ML family of languages, have had enormous influence on the world of programming language design and theory. ML is the foremost exemplar of a functional programming language with strict evaluation (call-by-value) and static typing. The use of parametric polymorphism in its type system, together with the automatic inference of such types, has influenced a wide variety of modern languages (where polymorphism is often referred to as generics). It has popularized the idea of datatypes with associated case analysis by pattern matching. The module system of Standard ML extends the notion of type-level parameterization to large-scale programming with the notion of parametric modules, or functors. Standard ML also set a precedent by being a language whose design included a formal definition with an associated metatheory of mathematical proofs (such as soundness of the type system). A formal definition was one of the explicit goals from the beginning of the project. While some previous languages had rigorous definitions, these definitions were not integral to the design process, and the formal part was limited to the language syntax and possibly dynamic semantics or static semantics, but not both. The paper covers the early history of ML, the subsequent efforts to define a standard ML language, and the development of its major features and its formal definition. We also review the impact that the language had on programming-language research.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "David",
            "lastName": "MacQueen"
          },
          {
            "firstName": "Robert",
            "lastName": "Harper"
          },
          {
            "firstName": "John",
            "lastName": "Reppy"
          }
        ],
        "Attachments": {
          "2FXEUIA3": {
            "Version": 626,
            "ContentType": "application/pdf",
            "Filename": "MacQueen et al. - 2020 - The history of Standard ML.pdf"
          }
        }
      },
      "9JN66RPC": {
        "Version": 224,
        "Title": "SLAKE: Facilitating Slab Manipulation for Exploiting Vulnerabilities in the Linux Kernel",
        "Abstract": "To determine the exploitability for a kernel vulnerability, a secu- rity analyst usually has to manipulate slab and thus demonstrate the capability of obtaining the control over a program counter or performing privilege escalation. However, this is a lengthy process because (1) an analyst typically has no clue about what objects and system calls are useful for kernel exploitation and (2) he lacks the knowledge of manipulating a slab and obtaining the desired layout. In the past, researchers have proposed various techniques to facilitate exploit development. Unfortunately, none of them can be easily applied to address these challenges. On the one hand, this is because of the complexity of the Linux kernel. On the other hand, this is due to the dynamics and non-deterministic of slab variations. In this work, we tackle the challenges above from two perspectives. First, we use static and dynamic analysis techniques to explore the kernel objects, and the corresponding system calls useful for exploitation. Second, we model commonly-adopted exploitation methods and develop a technical approach to facilitate the slab layout adjustment. By extending LLVM as well as Syzkaller, we implement our techniques and name their combination after SLAKE. We evaluate SLAKE by using 27 real-world kernel vulnerabilities, demonstrating that it could not only diversify the ways to perform kernel exploitation but also sometimes escalate the exploitability of kernel vulnerabilities.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yueqi",
            "lastName": "Chen"
          },
          {
            "firstName": "Xinyu",
            "lastName": "Xing"
          }
        ],
        "Attachments": {
          "TWCJWRMY": {
            "Version": 225,
            "ContentType": "application/pdf",
            "Filename": "Chen and Xing - 2019 - SLAKE Facilitating Slab Manipulation for Exploiti.pdf"
          }
        }
      },
      "9SLMRN25": {
        "Version": 576,
        "Title": "AFL++: Combining Incremental Steps of Fuzzing Research",
        "Abstract": "In this paper, we present AFL++, a community-driven opensource tool that incorporates state-of-the-art fuzzing research, to make the research comparable, reproducible, combinable and — most importantly – useable. It offers a variety of novel features, for example its Custom Mutator API, able to extend the fuzzing process at many stages. With it, mutators for speciﬁc targets can also be written by experienced security testers. We hope for AFL++ to become a new baseline tool not only for current, but also for future research, as it allows to test new techniques quickly, and evaluate not only the effectiveness of the single technique versus the state-of-theart, but also in combination with other techniques. The paper gives an evaluation of hand-picked fuzzing technologies —shining light on the fact that while each novel fuzzing method can increase performance in some targets — it decreases performance for other targets. This is an insight future fuzzing research should consider in their evaluations.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Andrea",
            "lastName": "Fioraldi"
          },
          {
            "firstName": "Dominik",
            "lastName": "Maier"
          },
          {
            "firstName": "Heiko",
            "lastName": "Eißfeldt"
          },
          {
            "firstName": "Marc",
            "lastName": "Heuse"
          }
        ],
        "Attachments": {
          "R7PZ9K5Q": {
            "Version": 577,
            "ContentType": "application/pdf",
            "Filename": "Fioraldi et al. - AFL++ Combining Incremental Steps of Fuzzing Rese.pdf"
          }
        }
      },
      "9UDSLE8D": {
        "Version": 291,
        "Title": "A Feature-Oriented Corpus for Understanding, Evaluating and Improving Fuzz Testing",
        "Abstract": "Fuzzing is a promising technique for detecting security vulnerabilities. Newly developed fuzzers are typically evaluated in terms of the number of bugs found on vulnerable programs/binaries. However,existing corpora usually do not capture the features that prevent fuzzers from finding bugs, leading to ambiguous conclusions on the pros and cons of the fuzzers evaluated. A typical example is that Driller detects more bugs than AFL, but its evaluation cannot establish if the advancement of Driller stems from the concolic execution or not, since, for example, its ability in resolving a dataset`s magic values is unclear. In this paper, we propose to address the above problem by generating corpora based on search-hampering features. As a proof-of-concept, we have designed FEData, a prototype corpus that currently focuses on four search-hampering features to generate vulnerable programs for fuzz testing. Unlike existing corpora that can only answer \"how\", FEData can also further answer \"why\" by exposing (or understanding) the reasons for the identified weaknesses in a fuzzer. The \"why\" information serves as the key to the improvement of fuzzers.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Xiaogang",
            "lastName": "Zhu"
          },
          {
            "firstName": "Xiaotao",
            "lastName": "Feng"
          },
          {
            "firstName": "Tengyun",
            "lastName": "Jiao"
          },
          {
            "firstName": "Sheng",
            "lastName": "Wen"
          },
          {
            "firstName": "Yang",
            "lastName": "Xiang"
          },
          {
            "firstName": "Seyit",
            "lastName": "Camtepe"
          },
          {
            "firstName": "Jingling",
            "lastName": "Xue"
          }
        ],
        "Attachments": {
          "L5QWGQYM": {
            "Version": 292,
            "ContentType": "application/pdf",
            "Filename": "Zhu et al. - 2019 - A Feature-Oriented Corpus for Understanding, Evalu.pdf"
          }
        }
      },
      "9XTANHMR": {
        "Version": 520,
        "Title": "CallStranger CVE-2020-12695",
        "Abstract": "",
        "ItemType": "",
        "Creators": [],
        "Attachments": {
          "5M3LAB5F": {
            "Version": 521,
            "ContentType": "text/html",
            "Filename": "callstranger.com.html"
          }
        }
      },
      "9ZBIWLRN": {
        "Version": 715,
        "Title": "Programming with Arrows",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "David",
            "lastName": "Hutchison"
          },
          {
            "firstName": "Takeo",
            "lastName": "Kanade"
          },
          {
            "firstName": "Josef",
            "lastName": "Kittler"
          },
          {
            "firstName": "Jon M.",
            "lastName": "Kleinberg"
          },
          {
            "firstName": "Friedemann",
            "lastName": "Mattern"
          },
          {
            "firstName": "John C.",
            "lastName": "Mitchell"
          },
          {
            "firstName": "Moni",
            "lastName": "Naor"
          },
          {
            "firstName": "Oscar",
            "lastName": "Nierstrasz"
          },
          {
            "firstName": "C.",
            "lastName": "Pandu Rangan"
          },
          {
            "firstName": "Bernhard",
            "lastName": "Steffen"
          },
          {
            "firstName": "Madhu",
            "lastName": "Sudan"
          },
          {
            "firstName": "Demetri",
            "lastName": "Terzopoulos"
          },
          {
            "firstName": "Dough",
            "lastName": "Tygar"
          },
          {
            "firstName": "Moshe Y.",
            "lastName": "Vardi"
          },
          {
            "firstName": "Gerhard",
            "lastName": "Weikum"
          },
          {
            "firstName": "Varmo",
            "lastName": "Vene"
          },
          {
            "firstName": "Tarmo",
            "lastName": "Uustalu"
          },
          {
            "firstName": "John",
            "lastName": "Hughes"
          }
        ],
        "Attachments": {
          "HBJWTLRC": {
            "Version": 716,
            "ContentType": "application/pdf",
            "Filename": "Hughes - 2005 - Programming with Arrows.pdf"
          }
        }
      },
      "A7AHGU4N": {
        "Version": 407,
        "Title": "Fuzzing: On the Exponential Cost of Vulnerability Discovery",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marcel",
            "lastName": "Böhme"
          },
          {
            "firstName": "Brandon",
            "lastName": "Falk"
          }
        ],
        "Attachments": {
          "LW67AZJA": {
            "Version": 408,
            "ContentType": "application/pdf",
            "Filename": "Böhme and Falk - 2020 - Fuzzing On the Exponential Cost of Vulnerability .pdf"
          }
        }
      },
      "AIHUENDT": {
        "Version": 624,
        "Title": "Thriving in a crowded and changing world: C++ 2006&#x2013;2020",
        "Abstract": "By 2006, C++ had been in widespread industrial use for 20 years. It contained parts that had survived unchanged since introduced into C in the early 1970s as well as features that were novel in the early 2000s. From 2006 to 2020, the C++ developer community grew from about 3 million to about 4.5 million. It was a period where new programming models emerged, hardware architectures evolved, new application domains gained massive importance, and quite a few well-financed and professionally marketed languages fought for dominance. How did C++ -- an older language without serious commercial backing -- manage to thrive in the face of all that? This paper focuses on the major changes to the ISO C++ standard for the 2011, 2014, 2017, and 2020 revisions. The standard library is about 3/4 of the C++20 standard, but this paper's primary focus is on language features and the programming techniques they support. The paper contains long lists of features documenting the growth of C++. Significant technical points are discussed and illustrated with short code fragments. In addition, it presents some failed proposals and the discussions that led to their failure. It offers a perspective on the bewildering flow of facts and features across the years. The emphasis is on the ideas, people, and processes that shaped the language. Themes include efforts to preserve the essence of C++ through evolutionary changes, to simplify its use, to improve support for generic programming, to better support compile-time programming, to extend support for concurrency and parallel programming, and to maintain stable support for decades' old code. The ISO C++ standard evolves through a consensus process. Inevitably, there is competition among proposals and clashes (usually polite ones) over direction, design philosophies, and principles. The committee is now larger and more active than ever, with as many as 250 people turning up to week-long meetings three times a year and many more taking part electronically. We try (not always successfully) to mitigate the effects of design by committee, bureaucratic paralysis, and excessive enthusiasm for a variety of language fashions. Specific language-technical topics include the memory model, concurrency and parallelism, compile-time computation, move-semantics, exceptions, lambda expressions, and modules. Designing a mechanism for specifying a template's requirements on its arguments that is sufficiently flexible and precise yet doesn't impose run-time costs turned out to be hard. The repeated attempts to design ``concepts'' to do that have their roots back in the 1980s and touch upon many key design issues for C++ and for generic programming. The description is based on personal participation in the key events and design decisions, backed by the thousands of papers and hundreds of meeting minutes in the ISO C++ standards committee's archives.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Bjarne",
            "lastName": "Stroustrup"
          }
        ],
        "Attachments": {
          "C6THRHFB": {
            "Version": 631,
            "ContentType": "application/pdf",
            "Filename": "Stroustrup - 2020 - Thriving in a crowded and changing world C++ 2006.pdf"
          }
        }
      },
      "AK2RLL85": {
        "Version": 221,
        "Title": "Supervised Sequence Labelling with Recurrent Neural Networks",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Alex",
            "lastName": "Graves"
          }
        ],
        "Attachments": {
          "JWJFQASM": {
            "Version": 222,
            "ContentType": "application/pdf",
            "Filename": "Graves - 2012 - Supervised Sequence Labelling with Recurrent Neura.pdf"
          }
        }
      },
      "ASLBLNBW": {
        "Version": 494,
        "Title": "Medusa: Microarchitectural Data Leakage via Automated Attack Synthesis",
        "Abstract": "In May 2019, a new class of transient execution attack based on Meltdown called microarchitectural data sampling (MDS), was disclosed. MDS enables adversaries to leak secrets across security domains by collecting data from shared CPU resources such as data cache, ﬁll buffers, and store buffers. These resources may temporarily hold data that belongs to other processes and privileged contexts, which could falsely be forwarded to memory accesses of an adversary.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Daniel",
            "lastName": "Moghimi"
          },
          {
            "firstName": "Moritz",
            "lastName": "Lipp"
          },
          {
            "firstName": "Berk",
            "lastName": "Sunar"
          },
          {
            "firstName": "Michael",
            "lastName": "Schwarz"
          }
        ],
        "Attachments": {
          "92TQRZHK": {
            "Version": 495,
            "ContentType": "application/pdf",
            "Filename": "Moghimi et al. - Medusa Microarchitectural Data Leakage via Automa.pdf"
          }
        }
      },
      "B3QMDKT2": {
        "Version": 263,
        "Title": "A Probabilistic Analysis of the Efficiency of Automated Software Testing",
        "Abstract": "We study the relative efﬁciencies of the random and systematic approaches to automated software testing. Using a simple but realistic set of assumptions, we propose a general model for software testing and deﬁne sampling strategies for random (R) and systematic (S0) testing, where each sampling is associated with a sampling cost: 1 and c units of time, respectively. The two most important goals of software testing are: (i) achieving in minimal time a given degree of conﬁdence x in a program’s correctness and (ii) discovering a maximal number of errors within a given time bound nˆ. For both (i) and (ii), we show that there exists a bound on c beyond which R performs better than S0 on the average. Moreover for (i), this bound depends asymptotically only on x. We show that the efﬁciency of R can be ﬁtted to the exponential curve. Using these results we design a hybrid strategy H that starts with R and switches to S0 when S0 is expected to discover more errors per unit time. In our experiments we ﬁnd that H performs similarly or better than the most efﬁcient of both and that S0 may need to be signiﬁcantly faster than our bounds suggest to retain efﬁciency over R.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marcel",
            "lastName": "Bohme"
          },
          {
            "firstName": "Soumya",
            "lastName": "Paul"
          }
        ],
        "Attachments": {
          "GYEMC47U": {
            "Version": 264,
            "ContentType": "application/pdf",
            "Filename": "Bohme and Paul - 2016 - A Probabilistic Analysis of the Efficiency of Auto.pdf"
          }
        }
      },
      "BEWRRQQV": {
        "Version": 169,
        "Title": "Andreas Zeller's Blog: When Results Are All That Matters: Consequences",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Andreas",
            "lastName": "Zeller"
          }
        ],
        "Attachments": {
          "28JP2SJ8": {
            "Version": 170,
            "ContentType": "text/html",
            "Filename": "when-results-are-all-that-matters.html"
          }
        }
      },
      "BU5UWWVB": {
        "Version": 585,
        "Title": "A history of Erlang",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Joe",
            "lastName": "Armstrong"
          }
        ],
        "Attachments": {
          "6VFNCFZ3": {
            "Version": 586,
            "ContentType": "application/pdf",
            "Filename": "Armstrong - 2007 - A history of Erlang.pdf"
          }
        }
      },
      "BX8W976X": {
        "Version": 777,
        "Title": "Growing a Test Corpus with Bonsai Fuzzing",
        "Abstract": "This paper presents a coverage-guided grammarbased fuzzing technique for automatically synthesizing a corpus of concise test inputs. We walk-through a case study of a compiler designed for education and the corresponding problem of generating meaningful test cases to provide to students. The prior state-of-the-art solution is a combination of fuzzing and test-case reduction techniques such as variants of deltadebugging. Our key insight is that instead of attempting to minimize convoluted fuzzer-generated test inputs, we can instead grow concise test inputs by construction using a form of iterative deepening. We call this approach bonsai fuzzing. Experimental results show that bonsai fuzzing can generate test corpora having inputs that are 16–45% smaller in size on average as compared to a fuzz-then-reduce approach, while achieving approximately the same code coverage and fault-detection capability.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Vasudev",
            "lastName": "Vikram"
          },
          {
            "firstName": "Rohan",
            "lastName": "Padhye"
          },
          {
            "firstName": "Koushik",
            "lastName": "Sen"
          }
        ],
        "Attachments": {
          "FCFSCTA4": {
            "Version": 778,
            "ContentType": "application/pdf",
            "Filename": "Vikram et al. - 2021 - Growing a Test Corpus with Bonsai Fuzzing.pdf"
          }
        }
      },
      "C3LNDUQS": {
        "Version": 506,
        "Title": "Efficient Evolutionary Fuzzing for Android Application Installation Process",
        "Abstract": "Source code analysis techniques used for automated software testing are insufﬁcient to ﬁnd security ﬂaws in programs. Therefore, security researchers have been employing also fuzzing techniques for ﬁnding bugs and vulnerabilities in target programs. With the proliferation of mobile devices, researchers have started to explore the use of fuzz tests on mobile platforms. While most of these studies are GUI-based and implemented at the application level, the detection of vulnerabilities in lower levels is very critical due to affecting a broader range of Android users. Therefore, in this study, a new approach is proposed to fuzz testing for Android application installation process. The use of a search heuristic namely genetic algorithms is investigated for efﬁcient fuzz testing on DEX (Dalvik EXecutable) ﬁles. The proposed black box fuzzing tool called GFuzz is shown to be able to produce more unique crashes in Android in a shorter time than recently proposed similar approaches and to detect new and existing bugs.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Veysel",
            "lastName": "Hatas"
          },
          {
            "firstName": "Sevil",
            "lastName": "Sen"
          },
          {
            "firstName": "John A.",
            "lastName": "Clark"
          }
        ],
        "Attachments": {
          "HVCF9DH4": {
            "Version": 508,
            "ContentType": "application/pdf",
            "Filename": "Hatas et al. - 2019 - Efficient Evolutionary Fuzzing for Android Applica.pdf"
          }
        }
      },
      "C7NZV27N": {
        "Version": 776,
        "Title": "Partial computation of programs (Futamura projections)",
        "Abstract": "",
        "ItemType": "",
        "Creators": null,
        "Attachments": {}
      },
      "CQ7GLTP6": {
        "Version": 348,
        "Title": "STADS: Software Testing as Species Discovery",
        "Abstract": "A fundamental challenge of software testing is the statistically well-grounded extrapolation from program behaviors observed during testing. For instance, a security researcher who has run the fuzzer for a week has currently no means (i) to estimate the total number of feasible program branches, given that only a fraction has been covered so far, (ii) to estimate the additional time required to cover 10% more branches, or (iii) to assess the residual risk that a vulnerability exists when no vulnerability has been discovered. Failing to discover a vulnerability, does not mean that none exists---even if the fuzzer was run for a week (or a year). Hence, testing provides no formal correctness guarantees. In this article, I establish an unexpected connection with the otherwise unrelated scientific field of ecology, and introduce a statistical framework that models Software Testing and Analysis as Discovery of Species (STADS). For instance, in order to study the species diversity of arthropods in a tropical rain forest, ecologists would first sample a large number of individuals from that forest, determine their species, and extrapolate from the properties observed in the sample to properties of the whole forest. The estimation (i) of the total number of species, (ii) of the additional sampling effort required to discover 10% more species, or (iii) of the probability to discover a new species are classical problems in ecology. The STADS framework draws from over three decades of research in ecological biostatistics to address the fundamental extrapolation challenge for automated test generation. Our preliminary empirical study demonstrates a good estimator performance even for a fuzzer with adaptive sampling bias---AFL, a state-of-the-art vulnerability detection tool. The STADS framework provides statistical correctness guarantees with quantifiable accuracy.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marcel",
            "lastName": "Böhme"
          }
        ],
        "Attachments": {
          "FIWZXCM4": {
            "Version": 75,
            "ContentType": "application/pdf",
            "Filename": "Böhme - 2018 - STADS Software Testing as Species Discovery.pdf"
          }
        }
      },
      "CTJB8K6C": {
        "Version": 455,
        "Title": "Fine Grained Dataflow Tracking with Proximal Gradients",
        "Abstract": "Dataflow tracking with Dynamic Taint Analysis (DTA) is an important method in systems security with many applications, including exploit analysis, guided fuzzing, and side-channel information leak detection. However, DTA is fundamentally limited by the boolean nature of taint labels, which provide no information about the significance of detected dataflows and lead to false positives/negatives on complex real world programs. We introduce proximal gradient analysis (PGA), a novel theoretically grounded approach that can track more accurate and fine-grained dataflow information than dynamic taint analysis. We observe that the gradients of neural networks precisely track dataflow and have been used widely for different data-flow-guided tasks like generating adversarial inputs and interpreting their decisions. However, programs, unlike neural networks, contain many discontinuous operations for which gradients cannot be computed. Our key insight is that we can efficiently approximate gradients over discontinuous operations by computing proximal gradients, a mathematically rigorous generalization of gradients for discontinuous functions. Proximal gradients allow us to apply the chain rule of calculus to accurately compose and propagate gradients over a program with minimal error. We compare our prototype PGA implementation two state of the art DTA implementations, DataFlowSanitizer and libdft, on 7 real-world programs. Our results show that PGA can improve the F1 accuracy of data flow tracking by up to 33% over taint tracking without introducing any significant overhead (<5% on average). We further demonstrate the effectiveness of PGA by discovering 23 previously unknown security vulnerabilities and 2 side-channel leaks, and analyzing 9 existing CVEs in the tested programs.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Gabriel",
            "lastName": "Ryan"
          },
          {
            "firstName": "Abhishek",
            "lastName": "Shah"
          },
          {
            "firstName": "Dongdong",
            "lastName": "She"
          },
          {
            "firstName": "Koustubha",
            "lastName": "Bhat"
          },
          {
            "firstName": "Suman",
            "lastName": "Jana"
          }
        ],
        "Attachments": {
          "65IN8KHY": {
            "Version": 457,
            "ContentType": "application/pdf",
            "Filename": "Ryan et al. - 2019 - Fine Grained Dataflow Tracking with Proximal Gradi.pdf"
          }
        }
      },
      "D25PYS6R": {
        "Version": 3,
        "Title": "EnFuzz: Ensemble Fuzzing with Seed Synchronization among Diverse Fuzzers",
        "Abstract": "Fuzzing is widely used for software vulnerability detection. There are various kinds of fuzzers with different fuzzing strategies, and most of them perform well on their targets. However, in industry practice and empirical study, the performance and generalization ability of those well-designed fuzzing strategies are challenged by the complexity and diversity of real-world applications. In this paper, inspired by the idea of ensemble learning, we first propose an ensemble fuzzing approach EnFuzz, that integrates multiple fuzzing strategies to obtain better performance and generalization ability than that of any constituent fuzzer alone. First, we define the diversity of the base fuzzers and choose those most recent and well-designed fuzzers as base fuzzers. Then, EnFuzz ensembles those base fuzzers with seed synchronization and result integration mechanisms. For evaluation, we implement EnFuzz , a prototype basing on four strong open-source fuzzers (AFL, AFLFast, AFLGo, FairFuzz), and test them on Google's fuzzing test suite, which consists of widely used real-world applications. The 24-hour experiment indicates that, with the same resources usage, these four base fuzzers perform variously on different applications, while EnFuzz shows better generalization ability and always outperforms others in terms of path coverage, branch coverage and crash discovery. Even compared with the best cases of AFL, AFLFast, AFLGo and FairFuzz, EnFuzz discovers 26.8%, 117%, 38.8% and 39.5% more unique crashes, executes 9.16%, 39.2%, 19.9% and 20.0% more paths and covers 5.96%, 12.0%, 21.4% and 11.1% more branches respectively.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yuanliang",
            "lastName": "Chen"
          },
          {
            "firstName": "Yu",
            "lastName": "Jiang"
          },
          {
            "firstName": "Fuchen",
            "lastName": "Ma"
          },
          {
            "firstName": "Jie",
            "lastName": "Liang"
          },
          {
            "firstName": "Mingzhe",
            "lastName": "Wang"
          },
          {
            "firstName": "Chijin",
            "lastName": "Zhou"
          },
          {
            "firstName": "Zhuo",
            "lastName": "Su"
          },
          {
            "firstName": "Xun",
            "lastName": "Jiao"
          }
        ],
        "Attachments": {
          "7USMTLSR": {
            "Version": 4,
            "ContentType": "application/pdf",
            "Filename": "Chen et al. - 2018 - EnFuzz Ensemble Fuzzing with Seed Synchronization.pdf"
          }
        }
      },
      "DBWGWZKG": {
        "Version": 624,
        "Title": "The early history of F#",
        "Abstract": "This paper describes the genesis and early history of the F# programming language. I start with the origins of strongly-typed functional programming (FP) in the 1970s, 80s and 90s. During the same period, Microsoft was founded and grew to dominate the software industry. In 1997, as a response to Java, Microsoft initiated internal projects which eventually became the .NET programming framework and the C# language. From 1997 the worlds of academic functional programming and industry combined at Microsoft Research, Cambridge. The researchers engaged with the company through Project 7, the initial effort to bring multiple languages to .NET, leading to the initiation of .NET Generics in 1998 and F# in 2002. F# was one of several responses by advocates of strongly-typed functional programming to the \"object-oriented tidal wave\" of the mid-1990s. The development of the core features of F# 1.0 happened from 2004-2007, and I describe the decision-making process that led to the \"productization\" of F# by Microsoft in 2007-10 and the release of F# 2.0. The origins of F#'s characteristic features are covered: object programming, quotations, statically resolved type parameters, active patterns, computation expressions, async, units-of-measure and type providers. I describe key developments in F# since 2010, including F# 3.0-4.5, and its evolution as an open source, cross-platform language with multiple delivery channels. I conclude by examining some uses of F# and the influence F# has had on other languages so far.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Don",
            "lastName": "Syme"
          }
        ],
        "Attachments": {
          "ZZEBX4SS": {
            "Version": 634,
            "ContentType": "application/pdf",
            "Filename": "Syme - 2020 - The early history of F#.pdf"
          }
        }
      },
      "DBX3USJX": {
        "Version": 152,
        "Title": "NeuFuzz: Efficient Fuzzing With Deep Neural Network",
        "Abstract": "Coverage-guided graybox fuzzing is one of the most popular and effective techniques for discovering vulnerabilities due to its nature of high speed and scalability. However, the existing techniques generally focus on code coverage but not on vulnerable code. These techniques aim to cover as many paths as possible rather than to explore paths that are more likely to be vulnerable. When selecting the seeds to test, the existing fuzzers usually treat all seed inputs equally, ignoring the fact that paths exercised by different seed inputs are not equally vulnerable. This results in wasting time testing uninteresting paths rather than vulnerable paths, thus reducing the efficiency of vulnerability detection. In this paper, we present a solution, NeuFuzz, using the deep neural network to guide intelligent seed selection during graybox fuzzing to alleviate the aforementioned limitation. In particular, the deep neural network is used to learn the hidden vulnerability pattern from a large number of vulnerable and clean program paths to train a prediction model to classify whether paths are vulnerable. The fuzzer then prioritizes seed inputs that are capable of covering the likely to be vulnerable paths and assigns more mutation energy (i.e., the number of inputs to be generated) to these seeds. We implemented a prototype of NeuFuzz based on an existing fuzzer PTfuzz and evaluated it on two different test suites: LAVA-M and nine real-world applications. The experimental results showed that NeuFuzz can find more vulnerabilities than the existing fuzzers in less time. We have found 28 new security bugs in these applications, 21 of which have been assigned as CVE IDs.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yunchao",
            "lastName": "Wang"
          },
          {
            "firstName": "Zehui",
            "lastName": "Wu"
          },
          {
            "firstName": "Qiang",
            "lastName": "Wei"
          },
          {
            "firstName": "Qingxian",
            "lastName": "Wang"
          }
        ],
        "Attachments": {
          "X35RGQWY": {
            "Version": 153,
            "ContentType": "application/pdf",
            "Filename": "Wang et al. - 2019 - NeuFuzz Efficient Fuzzing With Deep Neural Networ.pdf"
          }
        }
      },
      "DC22RIUK": {
        "Version": 752,
        "Title": "EcoFuzz: Adaptive Energy-Saving Greybox Fuzzing as a Variant of the Adversarial Multi-Armed Bandit",
        "Abstract": "Fuzzing is one of the most effective approaches for identifying security vulnerabilities. As a state-of-the-art coverage-based greybox fuzzer, AFL is a highly effective and widely used technique. However, AFL allocates excessive energy (i.e., the number of test cases generated by the seed) to seeds that exercise the high-frequency paths and can not adaptively adjust the energy allocation, thus wasting a signiﬁcant amount of energy. Moreover, the current Markov model for modeling coverage-based greybox fuzzing is not profound enough. This paper presents a variant of the Adversarial Multi-Armed Bandit model for modeling AFL’s power schedule process. We ﬁrst explain the challenges in AFL’s scheduling algorithm by using the reward probability that generates a test case for discovering a new path. Moreover, we illustrated the three states of the seeds set and developed a unique adaptive scheduling algorithm as well as a probability-based search strategy. These approaches are implemented on top of AFL in an adaptive energy-saving greybox fuzzer called EcoFuzz. EcoFuzz is examined against other six AFL-type tools on 14 real-world subjects over 490 CPU days. According to the results, EcoFuzz could attain 214% of the path coverage of AFL with reducing 32% test cases generation of that of AFL. Besides, EcoFuzz identiﬁed 12 vulnerabilities in GNU Binutils and other software. We also extended EcoFuzz to test some IoT devices and found a new vulnerability in the SNMP component.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Tai",
            "lastName": "Yue"
          },
          {
            "firstName": "Pengfei",
            "lastName": "Wang"
          },
          {
            "firstName": "Yong",
            "lastName": "Tang"
          },
          {
            "firstName": "Enze",
            "lastName": "Wang"
          },
          {
            "firstName": "Bo",
            "lastName": "Yu"
          },
          {
            "firstName": "Kai",
            "lastName": "Lu"
          },
          {
            "firstName": "Xu",
            "lastName": "Zhou"
          }
        ],
        "Attachments": {
          "R322VN5L": {
            "Version": 753,
            "ContentType": "application/pdf",
            "Filename": "Yue et al. - EcoFuzz Adaptive Energy-Saving Greybox Fuzzing as.pdf"
          }
        }
      },
      "DEQC3FRG": {
        "Version": 339,
        "Title": "Physics, Topology, Logic and Computation: A Rosetta Stone",
        "Abstract": "In physics, Feynman diagrams are used to reason about quantum processes. In the 1980s, it became clear that underlying these diagrams is a powerful analogy between quantum physics and topology: namely, a linear operator behaves very much like a \"cobordism\". Similar diagrams can be used to reason about logic, where they represent proofs, and computation, where they represent programs. With the rise of interest in quantum cryptography and quantum computation, it became clear that there is extensive network of analogies between physics, topology, logic and computation. In this expository paper, we make some of these analogies precise using the concept of \"closed symmetric monoidal category\". We assume no prior knowledge of category theory, proof theory or computer science.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "John C.",
            "lastName": "Baez"
          },
          {
            "firstName": "Mike",
            "lastName": "Stay"
          }
        ],
        "Attachments": {
          "3SLY2Y7Y": {
            "Version": 340,
            "ContentType": "application/pdf",
            "Filename": "Baez and Stay - 2009 - Physics, Topology, Logic and Computation A Rosett.pdf"
          }
        }
      },
      "DLJFZP73": {
        "Version": 564,
        "Title": "Zeror: Speed Up Fuzzing with Coverage-sensitive Tracing and Scheduling",
        "Abstract": "Coverage-guided fuzzing is one of the most popular software testing techniques for vulnerability detection. While effective, current fuzzing methods suffer from significant performance penalty due to instrumentation overhead, which limits its practical use. Existing solutions improve the fuzzing speed by decreasing instrumentation overheads but sacrificing coverage accuracy, which results in unstable performance of vulnerability detection. In this paper, we propose a coverage-sensitive tracing and scheduling framework Zeror that can improve the performance of existing fuzzers, especially in their speed and vulnerability detection. The Zeror is mainly made up of two parts: (1) a self-modifying tracing mechanism to provide a zero-overhead instrumentation for more effective coverage collection, and (2) a real-time scheduling mechanism to support adaptive switch between the zero-overhead instrumented binary and the fully instrumented binary for better vulnerability detection. In this way, Zeror is able to decrease collection overhead and preserve fine-grained coverage for guidance. For evaluation, we implement a prototype of Zeror and evaluate it on Google fuzzer-test-suite, which consists of 24 widely-used applications. The results show that Zeror performs better than existing fuzzing speed-up frameworks such as Untracer and INSTRIM, improves the execution speed of the state-of-the-art fuzzers such as AFL and MOPT by 159.80%, helps them achieve better coverage (averagely 10.14% for AFL, 6.91% for MOPT) and detect vulnerabilities faster (averagely 29.00% for AFL, 46.99% for MOPT).",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Chijin",
            "lastName": "Zhou"
          },
          {
            "firstName": "Mingzhe",
            "lastName": "Wang"
          },
          {
            "firstName": "Jie",
            "lastName": "Liang"
          },
          {
            "firstName": "Zhe",
            "lastName": "Liu"
          },
          {
            "firstName": "Yu",
            "lastName": "Jiang"
          }
        ],
        "Attachments": {
          "5VM9W29R": {
            "Version": 565,
            "ContentType": "application/pdf",
            "Filename": "Zhou et al. - 2020 - Zeror Speed Up Fuzzing with Coverage-sensitive Tr.pdf"
          }
        }
      },
      "DW97M9TE": {
        "Version": 429,
        "Title": "Discriminative Embeddings of Latent Variable Models for Structured Data",
        "Abstract": "Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. We propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are $10,000$ times smaller, while at the same time achieving the state-of-the-art predictive performance.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Hanjun",
            "lastName": "Dai"
          },
          {
            "firstName": "Bo",
            "lastName": "Dai"
          },
          {
            "firstName": "Le",
            "lastName": "Song"
          }
        ],
        "Attachments": {
          "7WIHYUVF": {
            "Version": 430,
            "ContentType": "application/pdf",
            "Filename": "Dai et al. - 2020 - Discriminative Embeddings of Latent Variable Model.pdf"
          }
        }
      },
      "E9PP9XAW": {
        "Version": 283,
        "Title": "Full-speed Fuzzing: Reducing Fuzzing Overhead through Coverage-guided Tracing",
        "Abstract": "Of coverage-guided fuzzing's three main components: (1) testcase generation, (2) code coverage tracing, and (3) crash triage, code coverage tracing is a dominant source of overhead. Coverage-guided fuzzers trace every testcase's code coverage through either static or dynamic binary instrumentation, or more recently, using hardware support. Unfortunately, tracing all testcases incurs significant performance penalties---even when the overwhelming majority of testcases and their coverage information are discarded because they do not increase code coverage. To eliminate needless tracing by coverage-guided fuzzers, we introduce the notion of coverage-guided tracing. Coverage-guided tracing leverages two observations: (1) only a fraction of generated testcases increase coverage, and thus require tracing; and (2) coverage-increasing testcases become less frequent over time. Coverage-guided tracing works by encoding the current frontier of code coverage in the target binary so that it self-reports when a testcase produces new coverage---without tracing. This acts as a filter for tracing; restricting the expense of tracing to only coverage-increasing testcases. Thus, coverage-guided tracing chooses to tradeoff increased coverage-increasing-testcase handling time for the ability to execute testcases initially at native speed. To show the potential of coverage-guided tracing, we create an implementation based on the static binary instrumentor Dyninst called UnTracer. We evaluate UnTracer using eight real-world binaries commonly used by the fuzzing community. Experiments show that after only an hour of fuzzing, UnTracer's average overhead is below 1%, and after 24-hours of fuzzing, UnTracer approaches 0% overhead, while tracing every testcase with popular white- and black-box-binary tracers AFL-Clang, AFL-QEMU, and AFL-Dyninst incurs overheads of 36%, 612%, and 518%, respectively.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Stefan",
            "lastName": "Nagy"
          },
          {
            "firstName": "Matthew",
            "lastName": "Hicks"
          }
        ],
        "Attachments": {
          "HCHGHXLY": {
            "Version": 284,
            "ContentType": "application/pdf",
            "Filename": "Nagy and Hicks - 2019 - Full-speed Fuzzing Reducing Fuzzing Overhead thro.pdf"
          }
        }
      },
      "EEI8J7CT": {
        "Version": 579,
        "Title": "CollAFL: Path Sensitive Fuzzing",
        "Abstract": "Coverage-guided fuzzing is a widely used and effective solution to ﬁnd software vulnerabilities. Tracking code coverage and utilizing it to guide fuzzing are crucial to coverageguided fuzzers. However, tracking full and accurate path coverage is infeasible in practice due to the high instrumentation overhead. Popular fuzzers (e.g., AFL) often use coarse coverage information, e.g., edge hit counts stored in a compact bitmap, to achieve highly efﬁcient greybox testing. Such inaccuracy and incompleteness in coverage introduce serious limitations to fuzzers. First, it causes path collisions, which prevent fuzzers from discovering potential paths that lead to new crashes. More importantly, it prevents fuzzers from making wise decisions on fuzzing strategies.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Shuitao",
            "lastName": "Gan"
          },
          {
            "firstName": "Chao",
            "lastName": "Zhang"
          },
          {
            "firstName": "Xiaojun",
            "lastName": "Qin"
          },
          {
            "firstName": "Xuwen",
            "lastName": "Tu"
          },
          {
            "firstName": "Kang",
            "lastName": "Li"
          },
          {
            "firstName": "Zhongyu",
            "lastName": "Pei"
          },
          {
            "firstName": "Zuoning",
            "lastName": "Chen"
          }
        ],
        "Attachments": {
          "489FSCL5": {
            "Version": 580,
            "ContentType": "application/pdf",
            "Filename": "Gan et al. - 2018 - CollAFL Path Sensitive Fuzzing.pdf"
          }
        }
      },
      "ETFVDJLM": {
        "Version": 718,
        "Title": "Probabilistic Naming of Functions in Stripped Binaries",
        "Abstract": "Debugging symbols in binary executables carry the names of functions and global variables. When present, they greatly simplify the process of reverse engineering, but they are almost always removed (stripped) for deployment. We present the design and implementation of punstrip, a tool which combines a probabilistic fingerprint of binary code based on high-level features with a probabilistic graphical model to learn the relationship between function names and program structure. As there are many naming conventions and developer styles, functions from different applications do not necessarily have the exact same name, even if they implement the exact same functionality. We therefore evaluate punstrip across three levels of name matching: exact; an approach based on natural language processing of name components; and using Symbol2Vec, a new embedding of function names based on random walks of function call graphs. We show that our approach is able to recognize functions compiled across different compilers and optimization levels and then demonstrate that punstrip can predict semantically similar function names based on code structure. We evaluate our approach over open source C binaries from the Debian Linux distribution and compare against the state of the art.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "James",
            "lastName": "Patrick-Evans"
          },
          {
            "firstName": "Lorenzo",
            "lastName": "Cavallaro"
          },
          {
            "firstName": "Johannes",
            "lastName": "Kinder"
          }
        ],
        "Attachments": {
          "99HRUHB8": {
            "Version": 719,
            "ContentType": "application/pdf",
            "Filename": "Patrick-Evans et al. - 2020 - Probabilistic Naming of Functions in Stripped Bina.pdf"
          }
        }
      },
      "EV4TK9W4": {
        "Version": 421,
        "Title": "ON PACE, PROGRESS, AND EMPIRICAL RIGOR",
        "Abstract": "The ﬁeld of ML is distinguished both by rapid innovation and rapid dissemination of results. While the pace of progress has been extraordinary by any measure, in this paper we explore potential issues that we believe to be arising as a result. In particular, we observe that the rate of empirical advancement may not have been matched by consistent increase in the level of empirical rigor across the ﬁeld as a whole. This short position paper highlights examples where progress has actually been slowed as a result, offers thoughts on incentive structures currently at play, and gives suggestions as seeds for discussions on productive change.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "D",
            "lastName": "Sculley"
          },
          {
            "firstName": "Jasper",
            "lastName": "Snoek"
          },
          {
            "firstName": "Ali",
            "lastName": "Rahimi"
          },
          {
            "firstName": "Alex",
            "lastName": "Wiltschko"
          }
        ],
        "Attachments": {
          "HXVYZ4J8": {
            "Version": 422,
            "ContentType": "application/pdf",
            "Filename": "Sculley et al. - 2018 - ON PACE, PROGRESS, AND EMPIRICAL RIGOR.pdf"
          }
        }
      },
      "EZNL3YBN": {
        "Version": 416,
        "Title": "Suzzer: A Vulnerability-Guided Fuzzer Based on Deep Learning",
        "Abstract": "Fuzzing is a simple and eﬀective way to ﬁnd software bugs. Most state-of-the-art fuzzers focus on improving code coverage to enhance the possibility of causing crashes. However, a software program oftentimes has only a fairly small portion that contains vulnerabilities, leading coverage-based fuzzers to work poorly most of the time. To address this challenge, we propose Suzzer, a vulnerability-guided fuzzer, to concentrate on testing code blocks that are more likely to contain bugs. Suzzer has a light-weight static analyzer to extract ACFG vector from target programs. In order to determine which code blocks are more vulnerable, Suzzer is equipped with prediction models which get the prior probability of each ACFG vector. The prediction models will guide Suzzer to generate test inputs with higher vulnerability scores, thus improving the eﬃciency of ﬁnding bugs. We evaluate Suzzer using two diﬀerent datasets: artiﬁcial LAVA-M dataset and a set of real-world programs. The results demonstrate that in the best case of short-term fuzzing, Suzzer saved 64.5% of the time consumed to discover vulnerabilities compared to VUzzer.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Zhe",
            "lastName": "Liu"
          },
          {
            "firstName": "Moti",
            "lastName": "Yung"
          },
          {
            "firstName": "Yuyue",
            "lastName": "Zhao"
          },
          {
            "firstName": "Yangyang",
            "lastName": "Li"
          },
          {
            "firstName": "Tengfei",
            "lastName": "Yang"
          },
          {
            "firstName": "Haiyong",
            "lastName": "Xie"
          }
        ],
        "Attachments": {
          "3Y6QIZ76": {
            "Version": 417,
            "ContentType": "application/pdf",
            "Filename": "Zhao et al. - 2020 - Suzzer A Vulnerability-Guided Fuzzer Based on Dee.pdf"
          }
        }
      },
      "FAERZ6YT": {
        "Version": 269,
        "Title": "T-Fuzz: Fuzzing by Program Transformation",
        "Abstract": "Fuzzing is a simple yet effective approach to discover software bugs utilizing randomly generated inputs. However, it is limited by coverage and cannot find bugs hidden in deep execution paths of the program because the randomly generated inputs fail complex sanity checks, e.g., checks on magic values, checksums, or hashes. To improve coverage, existing approaches rely on imprecise heuristics or complex input mutation techniques (e.g., symbolic execution or taint analysis) to bypass sanity checks. Our novel method tackles coverage from a different angle: by removing sanity checks in the target program. T-Fuzz leverages a coverage-guided fuzzer to generate inputs. Whenever the fuzzer can no longer trigger new code paths, a light-weight, dynamic tracing based technique detects the input checks that the fuzzer-generated inputs fail. These checks are then removed from the target program. Fuzzing then continues on the transformed program, allowing the code protected by the removed checks to be triggered and potential bugs discovered. Fuzzing transformed programs to find bugs poses two challenges: (1) removal of checks leads to over-approximation and false positives, and (2) even for true bugs, the crashing input on the transformed program may not trigger the bug in the original program. As an auxiliary post-processing step, T-Fuzz leverages a symbolic execution-based approach to filter out false positives and reproduce true bugs in the original program. By transforming the program as well as mutating the input, T-Fuzz covers more code and finds more true bugs than any existing technique. We have evaluated T-Fuzz on the DARPA Cyber Grand Challenge dataset, LAVA-M dataset and 4 real-world programs (pngfix, tiffinfo, magick and pdftohtml). For the CGC dataset, T-Fuzz finds bugs in 166 binaries, Driller in 121, and AFL in 105. In addition, found 3 new bugs in previously-fuzzed programs and libraries.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Hui",
            "lastName": "Peng"
          },
          {
            "firstName": "Yan",
            "lastName": "Shoshitaishvili"
          },
          {
            "firstName": "Mathias",
            "lastName": "Payer"
          }
        ],
        "Attachments": {
          "Q8HA847P": {
            "Version": 270,
            "ContentType": "application/pdf",
            "Filename": "Peng et al. - 2018 - T-Fuzz Fuzzing by Program Transformation.pdf"
          }
        }
      },
      "FJC2CPTE": {
        "Version": 172,
        "Title": "A Course in Interacting Particle Systems",
        "Abstract": "These lecture notes give an introduction to the theory of interacting particle systems. The main subjects are the construction using generators and graphical representations, the mean field limit, stochastic order, duality, and the relation to oriented percolation. An attempt is made to give a large number of examples beyond the classical voter, contact and Ising processes and to illustrate these based on numerical simulations.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Jan M.",
            "lastName": "Swart"
          }
        ],
        "Attachments": {
          "TMXENMQ3": {
            "Version": 174,
            "ContentType": "application/pdf",
            "Filename": "Swart - 2020 - A Course in Interacting Particle Systems.pdf"
          }
        }
      },
      "FKFQQ9N2": {
        "Version": 605,
        "Title": "The birth of Prolog",
        "Abstract": "The programming language, Prolog, was born of a project aimed not at producing a programming language but at processing natural languages; in this case, French. The project gave rise to a preliminary version of Prolog at the end of 1971 and a more definitive version at the end of 1972. This article gives the history of this project and describes in detail the preliminary and then the final versions of Prolog. The authors also felt it appropriate to describe the Q-systems since it was a language which played a prominent part in Prolog's genesis.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Alain",
            "lastName": "Colmerauer"
          },
          {
            "firstName": "Philippe",
            "lastName": "Roussel"
          }
        ],
        "Attachments": {
          "N2QRT53Z": {
            "Version": 608,
            "ContentType": "application/pdf",
            "Filename": "234286.pdf"
          },
          "S9SZWH4S": {
            "Version": 606,
            "ContentType": "application/pdf",
            "Filename": "Colmerauer and Roussel - 1993 - The birth of Prolog.pdf"
          }
        }
      },
      "FXHDMRZ4": {
        "Version": 372,
        "Title": "Evolution Strategies as a Scalable Alternative to Reinforcement Learning",
        "Abstract": "We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Tim",
            "lastName": "Salimans"
          },
          {
            "firstName": "Jonathan",
            "lastName": "Ho"
          },
          {
            "firstName": "Xi",
            "lastName": "Chen"
          },
          {
            "firstName": "Szymon",
            "lastName": "Sidor"
          },
          {
            "firstName": "Ilya",
            "lastName": "Sutskever"
          }
        ],
        "Attachments": {
          "TT4F3STB": {
            "Version": 373,
            "ContentType": "application/pdf",
            "Filename": "Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf"
          }
        }
      },
      "GFEFF3JU": {
        "Version": 3,
        "Title": "Angora: Efficient Fuzzing by Principled Search",
        "Abstract": "Fuzzing is a popular technique for finding software bugs. However, the performance of the state-of-the-art fuzzers leaves a lot to be desired. Fuzzers based on symbolic execution produce quality inputs but run slow, while fuzzers based on random mutation run fast but have difficulty producing quality inputs. We propose Angora, a new mutation-based fuzzer that outperforms the state-of-the-art fuzzers by a wide margin. The main goal of Angora is to increase branch coverage by solving path constraints without symbolic execution. To solve path constraints efficiently, we introduce several key techniques: scalable byte-level taint tracking, context-sensitive branch count, search based on gradient descent, and input length exploration. On the LAVA-M data set, Angora found almost all the injected bugs, found more bugs than any other fuzzer that we compared with, and found eight times as many bugs as the second-best fuzzer in the program who. Angora also found 103 bugs that the LAVA authors injected but could not trigger. We also tested Angora on eight popular, mature open source programs. Angora found 6, 52, 29, 40 and 48 new bugs in file, jhead, nm, objdump and size, respectively. We measured the coverage of Angora and evaluated how its key techniques contribute to its impressive performance.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Peng",
            "lastName": "Chen"
          },
          {
            "firstName": "Hao",
            "lastName": "Chen"
          }
        ],
        "Attachments": {
          "F4QAATHS": {
            "Version": 5,
            "ContentType": "application/pdf",
            "Filename": "Chen and Chen - 2018 - Angora Efficient Fuzzing by Principled Search.pdf"
          }
        }
      },
      "H3Y6SLR4": {
        "Version": 106,
        "Title": "AntiFuzz: Impeding Fuzzing Audits of Binary Executables",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Emre",
            "lastName": "Güler"
          },
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Ali",
            "lastName": "Abbasi"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "BYTK6L4X": {
            "Version": 107,
            "ContentType": "application/pdf",
            "Filename": "Güler et al. - 2019 - AntiFuzz Impeding Fuzzing Audits of Binary Execut.pdf"
          }
        }
      },
      "H4JIM4J5": {
        "Version": 503,
        "Title": "FANS: Fuzzing Android Native System Services via Automated Interface Analysis",
        "Abstract": "Android native system services provide essential supports and fundamental functionalities for user apps. Finding vulnerabilities in them is crucial for Android security. Fuzzing is one of the most popular vulnerability discovery solutions, yet faces several challenges when applied to Android native system services. First, such services are invoked via a special interprocess communication (IPC) mechanism, namely binder, via service-speciﬁc interfaces. Thus, the fuzzer has to recognize all interfaces and generate interface-speciﬁc test cases automatically. Second, effective test cases should satisfy the interface model of each interface. Third, the test cases should also satisfy the semantic requirements, including variable dependencies and interface dependencies.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Baozheng",
            "lastName": "Liu"
          },
          {
            "firstName": "Chao",
            "lastName": "Zhang"
          },
          {
            "firstName": "Guang",
            "lastName": "Gong"
          },
          {
            "firstName": "Yishun",
            "lastName": "Zeng"
          },
          {
            "firstName": "Haifeng",
            "lastName": "Ruan"
          },
          {
            "firstName": "Jianwei",
            "lastName": "Zhuge"
          }
        ],
        "Attachments": {
          "ULADFUH7": {
            "Version": 504,
            "ContentType": "application/pdf",
            "Filename": "Liu et al. - FANS Fuzzing Android Native System Services via A.pdf"
          }
        }
      },
      "HG8KBMMX": {
        "Version": 726,
        "Title": "Fuzzing Based on Function Importance by Attributed Call Graph",
        "Abstract": "Fuzzing has become one of the important methods for vulnerability detecting. The existing fuzzing tools represented by AFL use heuristic algorithms to guide the direction of fuzzing which exposes great randomness. What’s more, AFL ﬁlters seeds only by execution time and seed length. Meanwhile there is no in-depth consideration of the instruction information covered by the trace.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Wenshuo",
            "lastName": "Wang"
          },
          {
            "firstName": "Liang",
            "lastName": "Cheng"
          },
          {
            "firstName": "Yang",
            "lastName": "Zhang"
          }
        ],
        "Attachments": {
          "JNWU4N4B": {
            "Version": 727,
            "ContentType": "application/pdf",
            "Filename": "Wang et al. - 2020 - Fuzzing Based on Function Importance by Attributed.pdf"
          }
        }
      },
      "HREPZ9RE": {
        "Version": 180,
        "Title": "A Systematic Evaluation of Transient Execution Attacks and Defenses",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Claudio",
            "lastName": "Canella"
          },
          {
            "firstName": "Jo Van",
            "lastName": "Bulck"
          },
          {
            "firstName": "Michael",
            "lastName": "Schwarz"
          },
          {
            "firstName": "Moritz",
            "lastName": "Lipp"
          },
          {
            "firstName": "Benjamin von",
            "lastName": "Berg"
          },
          {
            "firstName": "Philipp",
            "lastName": "Ortner"
          },
          {
            "firstName": "Frank",
            "lastName": "Piessens"
          },
          {
            "firstName": "Dmitry",
            "lastName": "Evtyushkin"
          },
          {
            "firstName": "Daniel",
            "lastName": "Gruss"
          }
        ],
        "Attachments": {
          "HKV9C33U": {
            "Version": 181,
            "ContentType": "application/pdf",
            "Filename": "Canella et al. - 2019 - A Systematic Evaluation of Transient Execution Att.pdf"
          }
        }
      },
      "HSL8SRCF": {
        "Version": 349,
        "Title": "Ankou: Guiding Grey-box Fuzzing towardsCombinatorial Difference",
        "Abstract": "Grey-box fuzzing is an evolutionary process, which maintains and evolves a population of test cases with the help of a fitness function. Fitness functions used by current grey-box fuzzers are not informative in that they cannot distinguish different program executions as long as those executions achieve the same coverage. The problem is that current fitness functions only consider a union of data, but not their combination. As such, fuzzers often get stuck in a local optimum during their search. In this paper, we introduce Ankou, the first grey-box fuzzer that recognizes different combinations of execution information, and present several scalability challenges encountered while designing and implementing Ankou. Our experimental results show that Ankou is 1.94× and 8.0× more effective in finding bugs than AFL and Angora, respectively.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Valentin J M",
            "lastName": "Manès"
          },
          {
            "firstName": "Soomin",
            "lastName": "Kim"
          },
          {
            "firstName": "Sang Kil",
            "lastName": "Cha"
          }
        ],
        "Attachments": {
          "3ECX4FWP": {
            "Version": 242,
            "ContentType": "application/pdf",
            "Filename": "Manès et al. - 2020 - Ankou Guiding Grey-box Fuzzing towardsCombinatori.pdf"
          }
        }
      },
      "HX7DFZ49": {
        "Version": 723,
        "Title": "UNIFUZZ: A Holistic and Pragmatic Metrics-Driven Platform for Evaluating Fuzzers",
        "Abstract": "A ﬂurry of fuzzing tools (fuzzers) have been proposed in the literature, aiming at detecting software vulnerabilities effectively and efﬁciently. To date, it is however still challenging to compare fuzzers due to the inconsistency of the benchmarks, performance metrics, and/or environments for evaluation, which buries the useful insights and thus impedes the discovery of promising fuzzing primitives. In this paper, we design and develop UNIFUZZ, an open-source and metrics-driven platform for assessing fuzzers in a comprehensive and quantitative manner. Speciﬁcally, UNIFUZZ to date has incorporated 35 usable fuzzers, a benchmark of 20 real-world programs, and six categories of performance metrics. We ﬁrst systematically study the usability of existing fuzzers, ﬁnd and ﬁx a number of ﬂaws, and integrate them into UNIFUZZ. Based on the study, we propose a collection of pragmatic performance metrics to evaluate fuzzers from six complementary perspectives. Using UNIFUZZ, we conduct in-depth evaluations of several prominent fuzzers including AFL [1], AFLFast [2], Angora [3], Honggfuzz [4], MOPT [5], QSYM [6], T-Fuzz [7] and VUzzer64 [8]. We ﬁnd that none of them outperforms the others across all the target programs, and that using a single metric to assess the performance of a fuzzer may lead to unilateral conclusions, which demonstrates the signiﬁcance of comprehensive metrics. Moreover, we identify and investigate previously overlooked factors that may signiﬁcantly affect a fuzzer’s performance, including instrumentation methods and crash analysis tools. Our empirical results show that they are critical to the evaluation of a fuzzer. We hope that our ﬁndings can shed light on reliable fuzzing evaluation, so that we can discover promising fuzzing primitives to effectively facilitate fuzzer designs in the future.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yuwei",
            "lastName": "Li"
          },
          {
            "firstName": "Shouling",
            "lastName": "Ji"
          },
          {
            "firstName": "Yuan",
            "lastName": "Chen"
          },
          {
            "firstName": "Sizhuang",
            "lastName": "Liang"
          },
          {
            "firstName": "Wei-Han",
            "lastName": "Lee"
          },
          {
            "firstName": "Yueyao",
            "lastName": "Chen"
          },
          {
            "firstName": "Chenyang",
            "lastName": "Lyu"
          },
          {
            "firstName": "Chunming",
            "lastName": "Wu"
          },
          {
            "firstName": "Raheem",
            "lastName": "Beyah"
          },
          {
            "firstName": "Peng",
            "lastName": "Cheng"
          },
          {
            "firstName": "Kangjie",
            "lastName": "Lu"
          },
          {
            "firstName": "Ting",
            "lastName": "Wang"
          }
        ],
        "Attachments": {
          "IRGZ8PRF": {
            "Version": 724,
            "ContentType": "application/pdf",
            "Filename": "Li et al. - UNIFUZZ A Holistic and Pragmatic Metrics-Driven P.pdf"
          }
        }
      },
      "IXQJQYB9": {
        "Version": 424,
        "Title": "Are GANs Created Equal? A Large-Scale Study",
        "Abstract": "Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in \\cite{goodfellow2014generative}.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Mario",
            "lastName": "Lucic"
          },
          {
            "firstName": "Karol",
            "lastName": "Kurach"
          },
          {
            "firstName": "Marcin",
            "lastName": "Michalski"
          },
          {
            "firstName": "Sylvain",
            "lastName": "Gelly"
          },
          {
            "firstName": "Olivier",
            "lastName": "Bousquet"
          }
        ],
        "Attachments": {
          "JJWLJQEX": {
            "Version": 425,
            "ContentType": "application/pdf",
            "Filename": "Lucic et al. - 2018 - Are GANs Created Equal A Large-Scale Study.pdf"
          }
        }
      },
      "J6BIN24Z": {
        "Version": 459,
        "Title": "MTFuzz: Fuzzing with a Multi-Task Neural Network",
        "Abstract": "Fuzzing is a widely used technique for detecting software bugs and vulnerabilities. Most popular fuzzers generate new inputs using an evolutionary search to maximize code coverage. Essentially, these fuzzers start with a set of seed inputs, mutate them to generate new inputs, and identify the promising inputs using an evolutionary fitness function for further mutation. Despite their success, evolutionary fuzzers tend to get stuck in long sequences of unproductive mutations. In recent years, machine learning (ML) based mutation strategies have reported promising results. However, the existing ML-based fuzzers are limited by the lack of quality and diversity of the training data. As the input space of the target programs is high dimensional and sparse, it is prohibitively expensive to collect many diverse samples demonstrating successful and unsuccessful mutations to train the model. In this paper, we address these issues by using a Multi-Task Neural Network that can learn a compact embedding of the input space based on diverse training samples for multiple related tasks (i.e., predicting different types of coverage). The compact embedding can be used to guide the mutation process effectively by focusing most of the mutations on the parts of the embedding where the gradient is high. Our results show that MTFuzz uncovers 11 previously unseen bugs and achieves an average of 2x more edge coverage compared with 5 state-of-the-art fuzzer on 10 real-world programs.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Dongdong",
            "lastName": "She"
          },
          {
            "firstName": "Rahul",
            "lastName": "Krishna"
          },
          {
            "firstName": "Lu",
            "lastName": "Yan"
          },
          {
            "firstName": "Suman",
            "lastName": "Jana"
          },
          {
            "firstName": "Baishakhi",
            "lastName": "Ray"
          }
        ],
        "Attachments": {
          "VFPYTN4U": {
            "Version": 461,
            "ContentType": "application/pdf",
            "Filename": "She et al. - 2020 - MTFuzz Fuzzing with a Multi-Task Neural Network.pdf"
          }
        }
      },
      "J7HGN954": {
        "Version": 327,
        "Title": "Designing New Operating Primitives to Improve Fuzzing Performance",
        "Abstract": "Fuzzing is a software testing technique that finds bugs by repeatedly injecting mutated inputs to a target program. Known to be a highly practical approach, fuzzing is gaining more popularity than ever before. Current research on fuzzing has focused on producing an input that is more likely to trigger a vulnerability.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Wen",
            "lastName": "Xu"
          },
          {
            "firstName": "Sanidhya",
            "lastName": "Kashyap"
          },
          {
            "firstName": "Changwoo",
            "lastName": "Min"
          },
          {
            "firstName": "Taesoo",
            "lastName": "Kim"
          }
        ],
        "Attachments": {
          "H9ZY6PHK": {
            "Version": 328,
            "ContentType": "application/pdf",
            "Filename": "Xu et al. - 2017 - Designing New Operating Primitives to Improve Fuzz.pdf"
          }
        }
      },
      "JGYDUWUJ": {
        "Version": 213,
        "Title": "Side-Channel Aware Fuzzing",
        "Abstract": "Software testing is becoming a critical part of the development cycle of embedded devices, enabling vulnerability detection. A well-studied approach of software testing is fuzz-testing (fuzzing), during which mutated input is sent to an input-processing software while its behavior is monitored. The goal is to identify faulty states in the program, triggered by malformed inputs. Even though this technique is widely performed, fuzzing cannot be applied to embedded devices to its full extent. Due to the lack of adequately powerful I/O capabilities or an operating system the feedback needed for fuzzing cannot be acquired. In this paper we present and evaluate a new approach to extract feedback for fuzzing on embedded devices using information the power consumption leaks. Side-channel aware fuzzing is a threefold process that is initiated by sending an input to a target device and measuring its power consumption. First, we extract features from the power traces of the target device using machine learning algorithms. Subsequently, we use the features to reconstruct the code structure of the analyzed firmware. In the final step we calculate a score for the input, which is proportional to the code coverage. We carry out our proof of concept by fuzzing synthetic software and a light-weight AES implementation running on an ARM Cortex-M4 microcontroller. Our results show that the power side-channel carries information relevant for fuzzing.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Philip",
            "lastName": "Sperl"
          },
          {
            "firstName": "Konstantin",
            "lastName": "Böttinger"
          }
        ],
        "Attachments": {
          "PULX8S67": {
            "Version": 215,
            "ContentType": "application/pdf",
            "Filename": "Sperl and Böttinger - 2019 - Side-Channel Aware Fuzzing.pdf"
          }
        }
      },
      "JR5NX3W4": {
        "Version": 235,
        "Title": "Using Safety Properties to Generate Vulnerability Patches",
        "Abstract": "Security vulnerabilities are among the most critical software defects in existence. When identiﬁed, programmers aim to produce patches that prevent the vulnerability as quickly as possible, motivating the need for automatic program repair (APR) methods to generate patches automatically. Unfortunately, most current APR methods fall short because they approximate the properties necessary to prevent the vulnerability using examples. Approximations result in patches that either do not ﬁx the vulnerability comprehensively, or may even introduce new bugs. Instead, we propose property-based APR, which uses human-speciﬁed, program-independent and vulnerability-speciﬁc safety properties to derive source code patches for security vulnerabilities. Unlike properties that are approximated by observing the execution of test cases, such safety properties are precise and complete. The primary challenge lies in mapping such safety properties into source code patches that can be instantiated into an existing program. To address these challenges, we propose Senx, which, given a set of safety properties and a single input that triggers the vulnerability, detects the safety property violated by the vulnerability input and generates a corresponding patch that enforces the safety property and thus, removes the vulnerability. Senx solves several challenges with property-based APR: it identiﬁes the program expressions and variables that must be evaluated to check safety properties and identiﬁes the program scopes where they can be evaluated, it generates new code to selectively compute the values it needs if calling existing program code would cause unwanted side effects, and it uses a novel access range analysis technique to avoid placing patches inside loops where it could incur performance overhead. Our evaluation shows that the patches generated by Senx successfully ﬁx 32 of 42 real-world vulnerabilities from 11 applications including various tools or libraries for manipulating graphics/media ﬁles, a programming language interpreter, a relational database engine, a collection of programming tools for creating and managing binary programs, and a collection of basic ﬁle, shell, and text manipulation tools.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Zhen",
            "lastName": "Huang"
          },
          {
            "firstName": "David",
            "lastName": "Lie"
          },
          {
            "firstName": "Gang",
            "lastName": "Tan"
          },
          {
            "firstName": "Trent",
            "lastName": "Jaeger"
          }
        ],
        "Attachments": {
          "2U9JD6JY": {
            "Version": 236,
            "ContentType": "application/pdf",
            "Filename": "Huang et al. - 2019 - Using Safety Properties to Generate Vulnerability .pdf"
          }
        }
      },
      "K3LP7S7T": {
        "Version": 115,
        "Title": "Deep Reinforcement Fuzzing",
        "Abstract": "Fuzzing is the process of finding security vulnerabilities in input-processing code by repeatedly testing the code with modified inputs. In this paper, we formalize fuzzing as a reinforcement learning problem using the concept of Markov decision processes. This in turn allows us to apply state-of-the-art deep Q-learning algorithms that optimize rewards, which we define from runtime properties of the program under test. By observing the rewards caused by mutating with a specific set of actions performed on an initial program input, the fuzzing agent learns a policy that can next generate new higher-reward inputs. We have implemented this new approach, and preliminary empirical evidence shows that reinforcement fuzzing can outperform baseline random fuzzing.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Konstantin",
            "lastName": "Böttinger"
          },
          {
            "firstName": "Patrice",
            "lastName": "Godefroid"
          },
          {
            "firstName": "Rishabh",
            "lastName": "Singh"
          }
        ],
        "Attachments": {
          "6249Q2M7": {
            "Version": 116,
            "ContentType": "application/pdf",
            "Filename": "Böttinger et al. - 2018 - Deep Reinforcement Fuzzing.pdf"
          }
        }
      },
      "K4GD8QVC": {
        "Version": 506,
        "Title": "Chizpurfle: A Gray-Box Android Fuzzer for Vendor Service Customizations",
        "Abstract": "Android has become the most popular mobile OS, as it enables device manufacturers to introduce customizations to compete with value-added services. However, customizations make the OS less dependable and secure, since they can introduce software ﬂaws. Such ﬂaws can be found by using fuzzing, a popular testing technique among security researchers.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Antonio Ken",
            "lastName": "Iannillo"
          },
          {
            "firstName": "Roberto",
            "lastName": "Natella"
          },
          {
            "firstName": "Domenico",
            "lastName": "Cotroneo"
          },
          {
            "firstName": "Cristina",
            "lastName": "Nita-Rotaru"
          }
        ],
        "Attachments": {
          "QEIGJXAL": {
            "Version": 507,
            "ContentType": "application/pdf",
            "Filename": "Iannillo et al. - 2017 - Chizpurfle A Gray-Box Android Fuzzer for Vendor S.pdf"
          }
        }
      },
      "KLGPCZLU": {
        "Version": 514,
        "Title": "AFLNET: A Greybox Fuzzer for Network Protocols",
        "Abstract": "Server fuzzing is difﬁcult. Unlike simple commandline tools, servers feature a massive state space that can be traversed effectively only with well-deﬁned sequences of input messages. Valid sequences are speciﬁed in a protocol. In this paper, we present AFLNET, the ﬁrst greybox fuzzer for protocol implementations. Unlike existing protocol fuzzers, AFLNET takes a mutational approach and uses state-feedback to guide the fuzzing process. AFLNET is seeded with a corpus of recorded message exchanges between the server and an actual client. No protocol speciﬁcation or message grammars are required. AFLNET acts as a client and replays variations of the original sequence of messages sent to the server and retains those variations that were effective at increasing the coverage of the code or state space. To identify the server states that are exercised by a message sequence, AFLNET uses the server’s response codes. From this feedback, AFLNET identiﬁes progressive regions in the state space, and systematically steers towards such regions. The case studies with AFLNET on two popular protocol implementations demonstrate a substantial performance boost over the state-ofthe-art. AFLNET discovered two new CVEs which are classiﬁed as critical (CVSS score CRITICAL 9.8).",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Van-Thuan",
            "lastName": "Pham"
          },
          {
            "firstName": "Marcel",
            "lastName": "Bohme"
          },
          {
            "firstName": "Abhik",
            "lastName": "Roychoudhury"
          }
        ],
        "Attachments": {
          "XZU8ER4Q": {
            "Version": 530,
            "ContentType": "application/pdf",
            "Filename": "Pham et al. - AFLNET A Greybox Fuzzer for Network Protocols.pdf"
          }
        }
      },
      "KTDE2Z76": {
        "Version": 185,
        "Title": "Readactor: Practical Code Randomization Resilient to Memory Disclosure",
        "Abstract": "Code-reuse attacks such as return-oriented programming (ROP) pose a severe threat to modern software. Designing practical and effective defenses against code-reuse attacks is highly challenging. One line of defense builds upon fine-grained code diversification to prevent the adversary from constructing a reliable code-reuse attack. However, all solutions proposed so far are either vulnerable to memory disclosure or are impractical for deployment on commodity systems. In this paper, we address the deficiencies of existing solutions and present the first practical, fine-grained code randomization defense, called Read actor, resilient to both static and dynamic ROP attacks. We distinguish between direct memory disclosure, where the attacker reads code pages, and indirect memory disclosure, where attackers use code pointers on data pages to infer the code layout without reading code pages. Unlike previous work, Read actor resists both types of memory disclosure. Moreover, our technique protects both statically and dynamically generated code. We use a new compiler-based code generation paradigm that uses hardware features provided by modern CPUs to enable execute-only memory and hide code pointers from leakage to the adversary. Finally, our extensive evaluation shows that our approach is practical – we protect the entire Google Chromium browser and its V8 JIT compiler – and efficient with an average SPEC CPU2006 performance overhead of only 6.4%.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Stephen",
            "lastName": "Crane"
          },
          {
            "firstName": "Christopher",
            "lastName": "Liebchen"
          },
          {
            "firstName": "Andrei",
            "lastName": "Homescu"
          },
          {
            "firstName": "Lucas",
            "lastName": "Davi"
          },
          {
            "firstName": "Per",
            "lastName": "Larsen"
          },
          {
            "firstName": "Ahmad-Reza",
            "lastName": "Sadeghi"
          },
          {
            "firstName": "Stefan",
            "lastName": "Brunthaler"
          },
          {
            "firstName": "Michael",
            "lastName": "Franz"
          }
        ],
        "Attachments": {
          "SLW92CZF": {
            "Version": 186,
            "ContentType": "application/pdf",
            "Filename": "Crane et al. - 2015 - Readactor Practical Code Randomization Resilient .pdf"
          }
        }
      },
      "KUXYN39D": {
        "Version": 624,
        "Title": "S, R, and data science",
        "Abstract": "Data science is increasingly important and challenging. It requires computational tools and programming environments that handle big data and difficult computations, while supporting creative, high-quality analysis. The R language and related software play a major role in computing for data science. R is featured in most programs for training in the field. R packages provide tools for a wide range of purposes and users. The description of a new technique, particularly from research in statistics, is frequently accompanied by an R package, greatly increasing the usefulness of the description. The history of R makes clear its connection to data science. R was consciously designed to replicate in open-source software the contents of the S software. S in turn was written by data analysis researchers at Bell Labs as part of the computing environment for research in data analysis and collaborations to apply that research, rather than as a separate project to create a programming language. The features of S and the design decisions made for it need to be understood in this broader context of supporting effective data analysis (which would now be called data science). These characteristics were all transferred to R and remain central to its effectiveness. Thus, R can be viewed as based historically on a domain-specific language for the domain of data science.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "John M.",
            "lastName": "Chambers"
          }
        ],
        "Attachments": {
          "Q5XTXTC7": {
            "Version": 632,
            "ContentType": "application/pdf",
            "Filename": "Chambers - 2020 - S, R, and data science.pdf"
          }
        }
      },
      "L939VBEQ": {
        "Version": 617,
        "Title": "The evolution of APL",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Adin D.",
            "lastName": "Falkoff"
          },
          {
            "firstName": "Kenneth E.",
            "lastName": "Iverson"
          }
        ],
        "Attachments": {
          "345P633R": {
            "Version": 618,
            "ContentType": "application/pdf",
            "Filename": "Falkoff and Iverson - 1978 - The evolution of APL.pdf"
          }
        }
      },
      "L9K9MCCB": {
        "Version": 527,
        "Title": "Symbolic execution with SYMCC: Don’t interpret, compile!",
        "Abstract": "A major impediment to practical symbolic execution is speed, especially when compared to near-native speed solutions like fuzz testing. We propose a compilation-based approach to symbolic execution that performs better than state-of-the-art implementations by orders of magnitude. We present SYMCC, an LLVM-based C and C++ compiler that builds concolic execution right into the binary. It can be used by software developers as a drop-in replacement for clang and clang++, and we show how to add support for other languages with little effort. In comparison with KLEE, SYMCC is faster by up to three orders of magnitude and an average factor of 12. It also outperforms QSYM, a system that recently showed great performance improvements over other implementations, by up to two orders of magnitude and an average factor of 10. Using it on real-world software, we found that our approach consistently achieves higher coverage, and we discovered two vulnerabilities in the heavily tested OpenJPEG project, which have been conﬁrmed by the project maintainers and assigned CVE identiﬁers.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Sebastian",
            "lastName": "Poeplau"
          },
          {
            "firstName": "Aurélien",
            "lastName": "Francillon"
          }
        ],
        "Attachments": {
          "7MBGTRKQ": {
            "Version": 528,
            "ContentType": "application/pdf",
            "Filename": "Poeplau and Francillon - Symbolic execution with SYMCC Don’t interpret, co.pdf"
          }
        }
      },
      "LN63SQWK": {
        "Version": 404,
        "Title": "RetroWrite: Statically Instrumenting COTS Binaries for Fuzzing and Sanitization",
        "Abstract": "Analyzing the security of closed source binaries is currently impractical for end-users, or even developers who rely on third-party libraries. Such analysis relies on automatic vulnerability discovery techniques, most notably fuzzing with sanitizers enabled. The current state of the art for applying fuzzing or sanitization to binaries is dynamic binary translation, which has prohibitive performance overhead. The alternate technique, static binary rewriting, cannot fully recover symbolization information and hence has difﬁculty modifying binaries to track code coverage for fuzzing or to add security checks for sanitizers.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Sushant",
            "lastName": "Dinesh"
          },
          {
            "firstName": "Nathan",
            "lastName": "Burow"
          },
          {
            "firstName": "Dongyan",
            "lastName": "Xu"
          },
          {
            "firstName": "Mathias",
            "lastName": "Payer"
          }
        ],
        "Attachments": {
          "UISCEYU4": {
            "Version": 405,
            "ContentType": "application/pdf",
            "Filename": "Dinesh et al. - RetroWrite Statically Instrumenting COTS Binaries.pdf"
          }
        }
      },
      "ME36KXXX": {
        "Version": 217,
        "Title": "{GREYONE}: Data Flow Sensitive Fuzzing",
        "Abstract": "",
        "ItemType": "",
        "Creators": [],
        "Attachments": {
          "848NWUE7": {
            "Version": 218,
            "ContentType": "application/pdf",
            "Filename": "2020 - {GREYONE} Data Flow Sensitive Fuzzing.pdf"
          }
        }
      },
      "MWLERIK2": {
        "Version": 280,
        "Title": "Neural Network-based Graph Embedding for Cross-Platform Binary Code Similarity Detection",
        "Abstract": "The problem of cross-platform binary code similarity detection aims at detecting whether two binary functions coming from different platforms are similar or not. It has many security applications, including plagiarism detection, malware detection, vulnerability search, etc. Existing approaches rely on approximate graph-matching algorithms, which are inevitably slow and sometimes inaccurate, and hard to adapt to a new task. To address these issues, in this work, we propose a novel neural network-based approach to compute the embedding, i.e., a numeric vector, based on the control flow graph of each binary function, then the similarity detection can be done efficiently by measuring the distance between the embeddings for two functions. We implement a prototype called Gemini. Our extensive evaluation shows that Gemini outperforms the state-of-the-art approaches by large margins with respect to similarity detection accuracy. Further, Gemini can speed up prior art's embedding generation time by 3 to 4 orders of magnitude and reduce the required training time from more than 1 week down to 30 minutes to 10 hours. Our real world case studies demonstrate that Gemini can identify significantly more vulnerable firmware images than the state-of-the-art, i.e., Genius. Our research showcases a successful application of deep learning on computer security problems.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Xiaojun",
            "lastName": "Xu"
          },
          {
            "firstName": "Chang",
            "lastName": "Liu"
          },
          {
            "firstName": "Qian",
            "lastName": "Feng"
          },
          {
            "firstName": "Heng",
            "lastName": "Yin"
          },
          {
            "firstName": "Le",
            "lastName": "Song"
          },
          {
            "firstName": "Dawn",
            "lastName": "Song"
          }
        ],
        "Attachments": {
          "3458UWIQ": {
            "Version": 281,
            "ContentType": "application/pdf",
            "Filename": "Xu et al. - 2017 - Neural Network-based Graph Embedding for Cross-Pla.pdf"
          }
        }
      },
      "MWVNTJ76": {
        "Version": 624,
        "Title": "History of Logo",
        "Abstract": "Logo is more than a programming language. It is a learning environment where children explore mathematical ideas and create projects of their own design. Logo, the first computer language explicitly designed for children, was invented by Seymour Papert, Wallace Feurzeig, Daniel Bobrow, and Cynthia Solomon in 1966 at Bolt, Beranek and Newman, Inc. (BBN). Logo’s design drew upon two theoretical frameworks: Jean Piaget’s constructivism and Marvin Minsky’s artificial intelligence research at MIT. One of Logo’s foundational ideas was that children should have a powerful programming environment. Early Lisp served as a model with its symbolic computation, recursive functions, operations on linked lists, and dynamic scoping of variables. Logo became a symbol for change in elementary mathematics education and in the nature of school itself. The search for harnessing the computer’s potential to provide new ways of teaching and learning became a central focus and guiding principle in the Logo language development as it encompassed a widening scope that included natural language, music, graphics, animation, story telling, turtle geometry, robots, and other physical devices.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Cynthia",
            "lastName": "Solomon"
          },
          {
            "firstName": "Brian",
            "lastName": "Harvey"
          },
          {
            "firstName": "Ken",
            "lastName": "Kahn"
          },
          {
            "firstName": "Henry",
            "lastName": "Lieberman"
          },
          {
            "firstName": "Mark L.",
            "lastName": "Miller"
          },
          {
            "firstName": "Margaret",
            "lastName": "Minsky"
          },
          {
            "firstName": "Artemis",
            "lastName": "Papert"
          },
          {
            "firstName": "Brian",
            "lastName": "Silverman"
          }
        ],
        "Attachments": {
          "3998RRQK": {
            "Version": 627,
            "ContentType": "application/pdf",
            "Filename": "Solomon et al. - 2020 - History of Logo.pdf"
          }
        }
      },
      "N2TQ9RZ7": {
        "Version": 364,
        "Title": "A Review of Cooperative Multi-Agent Deep Reinforcement Learning",
        "Abstract": "Deep Reinforcement Learning has made significant progress in multi-agent systems in recent years. In this review article, we have mostly focused on recent papers on Multi-Agent Reinforcement Learning (MARL) than the older papers, unless it was necessary. Several ideas and papers are proposed with different notations, and we tried our best to unify them with a single notation and categorize them by their relevance. In particular, we have focused on five common approaches on modeling and solving multi-agent reinforcement learning problems: (I) independent-learners, (II) fully observable critic, (III) value function decomposition, (IV) consensus, (IV) learn to communicate. Moreover, we discuss some new emerging research areas in MARL along with the relevant recent papers. In addition, some of the recent applications of MARL in real world are discussed. Finally, a list of available environments for MARL research are provided and the paper is concluded with proposals on the possible research directions.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Afshin",
            "lastName": "OroojlooyJadid"
          },
          {
            "firstName": "Davood",
            "lastName": "Hajinezhad"
          }
        ],
        "Attachments": {
          "6SNR52GS": {
            "Version": 365,
            "ContentType": "application/pdf",
            "Filename": "OroojlooyJadid and Hajinezhad - 2019 - A Review of Cooperative Multi-Agent Deep Reinforce.pdf"
          }
        }
      },
      "NL57CU2N": {
        "Version": 287,
        "Title": "A Review of Machine Learning Applications in Fuzzing",
        "Abstract": "Fuzzing has played an important role in improving software development and testing over the course of several decades. Recent research in fuzzing has focused on applications of machine learning (ML), offering useful tools to overcome challenges in the fuzzing process. This review surveys the current research in applying ML to fuzzing. Specifically, this review discusses successful applications of ML to fuzzing, briefly explores challenges encountered, and motivates future research to address fuzzing bottlenecks.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Gary J.",
            "lastName": "Saavedra"
          },
          {
            "firstName": "Kathryn N.",
            "lastName": "Rodhouse"
          },
          {
            "firstName": "Daniel M.",
            "lastName": "Dunlavy"
          },
          {
            "firstName": "Philip W.",
            "lastName": "Kegelmeyer"
          }
        ],
        "Attachments": {
          "H2EWYPZI": {
            "Version": 288,
            "ContentType": "application/pdf",
            "Filename": "Saavedra et al. - 2019 - A Review of Machine Learning Applications in Fuzzi.pdf"
          }
        }
      },
      "NRIQH4HJ": {
        "Version": 305,
        "Title": "SmartSeed: Smart Seed Generation for Efficient Fuzzing",
        "Abstract": "Fuzzing is an automated application vulnerability detection method. For genetic algorithm-based fuzzing, it can mutate the seed files provided by users to obtain a number of inputs, which are then used to test the objective application in order to trigger potential crashes. As shown in existing literature, the seed file selection is crucial for the efficiency of fuzzing. However, current seed selection strategies do not seem to be better than randomly picking seed files. Therefore, in this paper, we propose a novel and generic system, named SmartSeed, to generate seed files towards efficient fuzzing. Specifically, SmartSeed is designed based on a machine learning model to learn and generate high-value binary seeds. We evaluate SmartSeed along with American Fuzzy Lop (AFL) on 12 open-source applications with the input formats of mp3, bmp or flv. We also combine SmartSeed with different fuzzing tools to examine its compatibility. From extensive experiments, we find that SmartSeed has the following advantages: First, it only requires tens of seconds to generate sufficient high-value seeds. Second, it can generate seeds with multiple kinds of input formats and significantly improves the fuzzing performance for most applications with the same input format. Third, SmartSeed is compatible to different fuzzing tools. In total, our system discovers more than twice unique crashes and 5,040 extra unique paths than the existing best seed selection strategy for the evaluated 12 applications. From the crashes found by SmartSeed, we discover 16 new vulnerabilities and have received their CVE IDs.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Chenyang",
            "lastName": "Lyu"
          },
          {
            "firstName": "Shouling",
            "lastName": "Ji"
          },
          {
            "firstName": "Yuwei",
            "lastName": "Li"
          },
          {
            "firstName": "Junfeng",
            "lastName": "Zhou"
          },
          {
            "firstName": "Jianhai",
            "lastName": "Chen"
          },
          {
            "firstName": "Jing",
            "lastName": "Chen"
          }
        ],
        "Attachments": {
          "4NP78J6D": {
            "Version": 306,
            "ContentType": "application/pdf",
            "Filename": "Lyu et al. - 2019 - SmartSeed Smart Seed Generation for Efficient Fuz.pdf"
          }
        }
      },
      "NTUWI5WC": {
        "Version": 324,
        "Title": "Squeak by example",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Andrew P.",
            "lastName": "Black"
          }
        ],
        "Attachments": {
          "A2HYP2ET": {
            "Version": 325,
            "ContentType": "application/pdf",
            "Filename": "Black - 2009 - Squeak by example.pdf"
          }
        }
      },
      "P2FQC825": {
        "Version": 570,
        "Title": "Building Fast Fuzzers",
        "Abstract": "Fuzzing is one of the key techniques for evaluating the robustness of programs against attacks. Fuzzing has to be effective in producing inputs that cover functionality and ﬁnd vulnerabilities. But it also has to be efﬁcient in producing such inputs quickly. Random fuzzers are very efﬁcient, as they can quickly generate random inputs; but they are not very effective, as the large majority of inputs generated is syntactically invalid. Grammar-based fuzzers make use of a grammar (or another model for the input language) to produce syntactically correct inputs, and thus can quickly cover input space and associated functionality. Existing grammar-based fuzzers are surprisingly inefﬁcient, though: Even the fastest grammar fuzzer dharma still produces inputs about a thousand times slower than the fastest random fuzzer. So far, one can have an effective or an efﬁcient fuzzer, but not both. In this paper, we describe how to build fast grammar fuzzers from the ground up, treating the problem of fuzzing from a programming language implementation perspective. Starting with a Python textbook approach, we adopt and adapt optimization techniques from functional programming and virtual machine implementation techniques together with other novel domain-speciﬁc optimizations in a step-by-step fashion. In our F1 prototype fuzzer, these improve production speed by a factor of 100–300 over the fastest grammar fuzzer dharma. As F1 is even 5–8 times faster than a lexical random fuzzer, we can ﬁnd bugs faster and test with much larger valid inputs than previously possible.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Rahul",
            "lastName": "Gopinath"
          },
          {
            "firstName": "Andreas",
            "lastName": "Zeller"
          }
        ],
        "Attachments": {
          "5JQYP43X": {
            "Version": 571,
            "ContentType": "application/pdf",
            "Filename": "Gopinath and Zeller - 2019 - Building Fast Fuzzers.pdf"
          }
        }
      },
      "P9CWF5QZ": {
        "Version": 15,
        "Title": "Spectre Attacks: Exploiting Speculative Execution",
        "Abstract": "Modern processors use branch prediction and speculative execution to maximize performance. For example, if the destination of a branch depends on a memory value that is in the process of being read, CPUs will try guess the destination and attempt to execute ahead. When the memory value finally arrives, the CPU either discards or commits the speculative computation. Speculative logic is unfaithful in how it executes, can access to the victim's memory and registers, and can perform operations with measurable side effects. Spectre attacks involve inducing a victim to speculatively perform operations that would not occur during correct program execution and which leak the victim's confidential information via a side channel to the adversary. This paper describes practical attacks that combine methodology from side channel attacks, fault attacks, and return-oriented programming that can read arbitrary memory from the victim's process. More broadly, the paper shows that speculative execution implementations violate the security assumptions underpinning numerous software security mechanisms, including operating system process separation, static analysis, containerization, just-in-time (JIT) compilation, and countermeasures to cache timing/side-channel attacks. These attacks represent a serious threat to actual systems, since vulnerable speculative execution capabilities are found in microprocessors from Intel, AMD, and ARM that are used in billions of devices. While makeshift processor-specific countermeasures are possible in some cases, sound solutions will require fixes to processor designs as well as updates to instruction set architectures (ISAs) to give hardware architects and software developers a common understanding as to what computation state CPU implementations are (and are not) permitted to leak.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Paul",
            "lastName": "Kocher"
          },
          {
            "firstName": "Daniel",
            "lastName": "Genkin"
          },
          {
            "firstName": "Daniel",
            "lastName": "Gruss"
          },
          {
            "firstName": "Werner",
            "lastName": "Haas"
          },
          {
            "firstName": "Mike",
            "lastName": "Hamburg"
          },
          {
            "firstName": "Moritz",
            "lastName": "Lipp"
          },
          {
            "firstName": "Stefan",
            "lastName": "Mangard"
          },
          {
            "firstName": "Thomas",
            "lastName": "Prescher"
          },
          {
            "firstName": "Michael",
            "lastName": "Schwarz"
          },
          {
            "firstName": "Yuval",
            "lastName": "Yarom"
          }
        ],
        "Attachments": {
          "4Z7JZDFU": {
            "Version": 16,
            "ContentType": "application/pdf",
            "Filename": "Kocher et al. - 2018 - Spectre Attacks Exploiting Speculative Execution.pdf"
          }
        }
      },
      "PBCYUY8P": {
        "Version": 413,
        "Title": "Be Sensitive and Collaborative: Analyzing Impact of Coverage Metrics in Greybox Fuzzing",
        "Abstract": "Coverage-guided greybox fuzzing has become one of the most common techniques for ﬁnding software bugs. Coverage metric, which decides how a fuzzer selects new seeds, is an essential parameter of fuzzing and can signiﬁcantly affect the results. While there are many existing works on the effectiveness of different coverage metrics on software testing, little is known about how different coverage metrics could actually affect the fuzzing results in practice. More importantly, it is unclear whether there exists one coverage metric that is superior to all the other metrics. In this paper, we report the ﬁrst systematic study on the impact of different coverage metrics in fuzzing. To this end, we formally deﬁne and discuss the concept of sensitivity, which can be used to theoretically compare different coverage metrics. We then present several coverage metrics with their variants. We conduct a study on these metrics with the DARPA CGC dataset, the LAVA-M dataset, and a set of real-world applications (a total of 221 binaries). We ﬁnd that because each fuzzing instance has limited resources (time and computation power), (1) each metric has its unique merit in terms of ﬂipping certain types of branches (thus vulnerability ﬁnding) and (2) there is no grand slam coverage metric that defeats all the others. We also explore combining different coverage metrics through cross-seeding, and the result is very encouraging: this pure fuzzing based approach can crash at least the same numbers of binaries in the CGC dataset as a previous approach (Driller) that combines fuzzing and concolic execution. At the same time, our approach uses fewer computing resources.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Jinghan",
            "lastName": "Wang"
          },
          {
            "firstName": "Yue",
            "lastName": "Duan"
          },
          {
            "firstName": "Wei",
            "lastName": "Song"
          },
          {
            "firstName": "Heng",
            "lastName": "Yin"
          },
          {
            "firstName": "Chengyu",
            "lastName": "Song"
          }
        ],
        "Attachments": {
          "U7VJXLT6": {
            "Version": 414,
            "ContentType": "application/pdf",
            "Filename": "Wang et al. - Be Sensitive and Collaborative Analyzing Impact o.pdf"
          }
        }
      },
      "PH87PETE": {
        "Version": 749,
        "Title": "ProFuzzBench: A Benchmark for Stateful Protocol Fuzzing",
        "Abstract": "We present a new benchmark (ProFuzzBench) for stateful fuzzing of network protocols. The benchmark includes a suite of representative open-source network servers for popular protocols, and tools to automate experimentation. We discuss challenges and potential directions for future research based on this benchmark.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Roberto",
            "lastName": "Natella"
          },
          {
            "firstName": "Van-Thuan",
            "lastName": "Pham"
          }
        ],
        "Attachments": {
          "5KRR5526": {
            "Version": 750,
            "ContentType": "application/pdf",
            "Filename": "Natella and Pham - 2021 - ProFuzzBench A Benchmark for Stateful Protocol Fu.pdf"
          }
        }
      },
      "PMWFEBBK": {
        "Version": 336,
        "Title": "Algebraic classiﬁers: a generic approach to fast cross-validation, online training, and parallel training",
        "Abstract": "We use abstract algebra to derive new algorithms for fast cross-validation, online learning, and parallel learning. To use these algorithms on a classiﬁcation model, we must show that the model has appropriate algebraic structure. It is easy to give algebraic structure to some models, and we do this explicitly for Bayesian classiﬁers and a novel variation of decision stumps called HomStumps. But not all classiﬁers have an obvious structure, so we introduce the Free HomTrainer. This can be used to give a “generic” algebraic structure to any classiﬁer. We use the Free HomTrainer to give algebraic structure to bagging and boosting. In so doing, we derive novel online and parallel algorithms, and present the ﬁrst fast crossvalidation schemes for these classiﬁers.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Michael",
            "lastName": "Izbicki"
          }
        ],
        "Attachments": {
          "ZPMDWDWE": {
            "Version": 337,
            "ContentType": "application/pdf",
            "Filename": "Izbicki - Algebraic classiﬁers a generic approach to fast c.pdf"
          }
        }
      },
      "PNWJESZR": {
        "Version": 645,
        "Title": "SAVIOR: Towards Bug-Driven Hybrid Testing",
        "Abstract": "Hybrid testing combines fuzz testing and concolic execution. It leverages fuzz testing to test easy-to-reach code regions and uses concolic execution to explore code blocks guarded by complex branch conditions. As a result, hybrid testing is able to reach deeper into program state space than fuzz testing or concolic execution alone. Recently, hybrid testing has seen signiﬁcant advancement. However, its code coverage-centric design is inefﬁcient in vulnerability detection. First, it blindly selects seeds for concolic execution and aims to explore new code continuously. However, as statistics show, a large portion of the explored code is often bug-free. Therefore, giving equal attention to every part of the code during hybrid testing is a non-optimal strategy. It slows down the detection of real vulnerabilities by over 43%. Second, classic hybrid testing quickly moves on after reaching a chunk of code, rather than examining the hidden defects inside. It may frequently miss subtle vulnerabilities despite that it has already explored the vulnerable code paths.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yaohui",
            "lastName": "Chen"
          },
          {
            "firstName": "Peng",
            "lastName": "Li"
          },
          {
            "firstName": "Jun",
            "lastName": "Xu"
          },
          {
            "firstName": "Shengjian",
            "lastName": "Guo"
          },
          {
            "firstName": "Rundong",
            "lastName": "Zhou"
          },
          {
            "firstName": "Yulong",
            "lastName": "Zhang"
          },
          {
            "firstName": "",
            "lastName": "Taowei"
          },
          {
            "firstName": "Long",
            "lastName": "Lu"
          }
        ],
        "Attachments": {
          "37TAYWX2": {
            "Version": 646,
            "ContentType": "application/pdf",
            "Filename": "Chen et al. - 2019 - SAVIOR Towards Bug-Driven Hybrid Testing.pdf"
          }
        }
      },
      "PTJ7FRZ8": {
        "Version": 491,
        "Title": "ABSynthe: Automatic Blackbox Side-channel Synthesis on Commodity Microarchitectures",
        "Abstract": "The past decade has seen a plethora of side-channel attacks on various CPU components. Each new attack typically follows a whitebox analysis approach, which involves (i) identifying a speciﬁc shared CPU component, (ii) reversing its behavior on a speciﬁc microarchitecture, and (iii) surgically exploiting such knowledge to leak information (e.g., by actively evicting shared entries to monitor victim accesses). This approach requires lengthy reverse engineering, repeated for every component and microarchitecture, and does not allow for attacking unknown shared resources.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Ben",
            "lastName": "Gras"
          },
          {
            "firstName": "Cristiano",
            "lastName": "Giuffrida"
          },
          {
            "firstName": "Michael",
            "lastName": "Kurth"
          },
          {
            "firstName": "Herbert",
            "lastName": "Bos"
          },
          {
            "firstName": "Kaveh",
            "lastName": "Razavi"
          }
        ],
        "Attachments": {
          "EQKNF53A": {
            "Version": 492,
            "ContentType": "application/pdf",
            "Filename": "Gras et al. - 2020 - ABSynthe Automatic Blackbox Side-channel Synthesi.pdf"
          }
        }
      },
      "PVS7JC6E": {
        "Version": 399,
        "Title": "Neutaint: Efficient Dynamic Taint Analysis with Neural Networks",
        "Abstract": "Dynamic taint analysis (DTA) is widely used by various applications to track information flow during runtime execution. Existing DTA techniques use rule-based taint-propagation, which is neither accurate (i.e., high false positive) nor efficient (i.e., large runtime overhead). It is hard to specify taint rules for each operation while covering all corner cases correctly. Moreover, the overtaint and undertaint errors can accumulate during the propagation of taint information across multiple operations. Finally, rule-based propagation requires each operation to be inspected before applying the appropriate rules resulting in prohibitive performance overhead on large real-world applications. In this work, we propose NEUTAINT, a novel end-to-end approach to track information flow using neural program embeddings. The neural program embeddings model the target's programs computations taking place between taint sources and sinks, which automatically learns the information flow by observing a diverse set of execution traces. To perform lightweight and precise information flow analysis, we utilize saliency maps to reason about most influential sources for different sinks. NEUTAINT constructs two saliency maps, a popular machine learning approach to influence analysis, to summarize both coarse-grained and fine-grained information flow in the neural program embeddings. We compare NEUTAINT with 3 state-of-the-art dynamic taint analysis tools. The evaluation results show that NEUTAINT can achieve 68% accuracy, on average, which is 10% improvement while reducing 40 times runtime overhead over the second-best taint tool Libdft on 6 real world programs. NEUTAINT also achieves 61% more edge coverage when used for taint-guided fuzzing indicating the effectiveness of the identified influential bytes.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Dongdong",
            "lastName": "She"
          },
          {
            "firstName": "Yizheng",
            "lastName": "Chen"
          },
          {
            "firstName": "Abhishek",
            "lastName": "Shah"
          },
          {
            "firstName": "Baishakhi",
            "lastName": "Ray"
          },
          {
            "firstName": "Suman",
            "lastName": "Jana"
          }
        ],
        "Attachments": {
          "Y686JDVR": {
            "Version": 400,
            "ContentType": "application/pdf",
            "Filename": "She et al. - 2019 - Neutaint Efficient Dynamic Taint Analysis with Ne.pdf"
          }
        }
      },
      "PX8R2G9Q": {
        "Version": 747,
        "Title": "Delta pointers: buffer overflow checks without the checks",
        "Abstract": "Despite decades of research, buffer overflows still rank among the most dangerous vulnerabilities in unsafe languages such as C and C++. Compared to other memory corruption vulnerabilities, buffer overflows are both common and typically easy to exploit. Yet, they have proven so challenging to detect in real-world programs that existing solutions either yield very poor performance, or introduce incompatibilities with the C/C++ language standard. We present Delta Pointers, a new solution for buffer overflow detection based on efficient pointer tagging. By carefully altering the pointer representation, without violating language specifications, Delta Pointers use existing hardware features to detect both contiguous and non-contiguous overflows on dereferences, without a single check incurring extra branch or memory access operations. By focusing on buffer overflows rather than other vulnerabilities (e.g., underflows), Delta Pointers offer a unique checkless design to provide high performance while still maintaining compatibility. We show that Delta Pointers are effective in detecting arbitrary buffer overflows and, at 35% overhead on SPEC, offer much better performance than competing solutions.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Taddeus",
            "lastName": "Kroes"
          },
          {
            "firstName": "Koen",
            "lastName": "Koning"
          },
          {
            "firstName": "Erik",
            "lastName": "van der Kouwe"
          },
          {
            "firstName": "Herbert",
            "lastName": "Bos"
          },
          {
            "firstName": "Cristiano",
            "lastName": "Giuffrida"
          }
        ],
        "Attachments": {
          "8UR73NIQ": {
            "Version": 748,
            "ContentType": "application/pdf",
            "Filename": "Kroes et al. - 2018 - Delta pointers buffer overflow checks without the.pdf"
          }
        }
      },
      "PYXT76CB": {
        "Version": 624,
        "Title": "Origins of the D programming language",
        "Abstract": "As its name suggests, the initial motivation for the D programming language was to improve on C and C++ while keeping their spirit. The D language was to preserve those languages' efficiency, low-level access, and Algol-style syntax. The areas D set out to improve focused initially on rapid development, convenience, and simplifying the syntax without hampering expressiveness. The genesis of D has its peculiarities, as is the case with many other languages. Walter Bright, D's creator, is a mechanical engineer by education who started out working for Boeing designing gearboxes for the 757. He was programming games on the side, and in trying to make his game Empire run faster, became interested in compilers. Despite having no experience, Bright set out in 1982 to implement a compiler that produced better code than those on the market at the time. This interest materialized into a C compiler, followed by compilers for C++, Java, and JavaScript. Best known of these would be the Zortech C++ compiler, the first (and to date only) C++-to-native compiler developed by a single person. The D programming language began in 1999 as an effort to pull the best features of these languages into a new one. Fittingly, D would use the by that time mature C/C++ back end (optimizer and code generator) that had been under continued development and maintenance since 1982. Between 1999 and 2006, Bright worked alone on the D language definition and its implementation, although a steadily increasing volume of patches from users was incorporated. The new language would be based on the past successes of the languages he'd used and implemented, but would be clearly looking to the future. D started with choices that are obvious today but were less clear winners back in the 1990s: full support for Unicode, IEEE floating point, 2s complement arithmetic, and flat memory addressing (memory is treated as a linear address space with no segmentation). It would do away with certain compromises from past languages imposed by shortages of memory (for example, forward declarations would not be required). It would primarily appeal to C and C++ users, as expertise with those languages would be readily transferrable. The interface with C was designed to be zero cost. The language design was begun in late 1999. An alpha version appeared in 2001 and the initial language was completed, somewhat arbitrarily, at version 1.0 in January 2007. During that time, the language evolved considerably, both in capability and in the accretion of a substantial worldwide community that became increasingly involved with contributing. The front end was open-sourced in April 2002, and the back end was donated by Symantec to the open source community in 2017. Meanwhile, two additional open-source back ends became mature in the 2010s: `gdc` (using the same back end as the GNU C++ compiler) and `ldc` (using the LLVM back end). The increasing use of the D language in the 2010s created an impetus for formalization and development management. To that end, the D Language Foundation was created in September 2015 as a nonprofit corporation overseeing work on D's definition and implementation, publications, conferences, and collaborations with universities.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Walter",
            "lastName": "Bright"
          },
          {
            "firstName": "Andrei",
            "lastName": "Alexandrescu"
          },
          {
            "firstName": "Michael",
            "lastName": "Parker"
          }
        ],
        "Attachments": {
          "7X9HFKUR": {
            "Version": 630,
            "ContentType": "application/pdf",
            "Filename": "Bright et al. - 2020 - Origins of the D programming language.pdf"
          }
        }
      },
      "QBF46V6J": {
        "Version": 772,
        "Title": "AURORA: Statistical Crash Analysis for Automated Root Cause Explanation",
        "Abstract": "Given the huge success of automated software testing techniques, a large amount of crashes is found in practice. Identifying the root cause of a crash is a time-intensive endeavor, causing a disproportion between ﬁnding a crash and ﬁxing the underlying software fault. To address this problem, various approaches have been proposed that rely on techniques such as reverse execution and backward taint analysis. Still, these techniques are either limited to certain fault types or provide an analyst with assembly instructions, but no context information or explanation of the underlying fault.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Tim",
            "lastName": "Blazytko"
          },
          {
            "firstName": "Moritz",
            "lastName": "Schlögel"
          },
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Ali",
            "lastName": "Abbasi"
          },
          {
            "firstName": "Joel",
            "lastName": "Frank"
          },
          {
            "firstName": "Simon",
            "lastName": "Wörner"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "3IK33SNP": {
            "Version": 773,
            "ContentType": "application/pdf",
            "Filename": "Blazytko et al. - AURORA Statistical Crash Analysis for Automated R.pdf"
          }
        }
      },
      "QGQGQVSN": {
        "Version": 558,
        "Title": "Sys: a Static/Symbolic Tool for Finding Good Bugs in Good (Browser) Code",
        "Abstract": "We describe and evaluate an extensible bug-ﬁnding tool, Sys, designed to automatically ﬁnd security bugs in huge codebases, even when easy-to-ﬁnd bugs have been already picked clean by years of aggressive automatic checking. Sys uses a two-step approach to ﬁnd such tricky errors. First, it breaks down large—tens of millions of lines—systems into small pieces using user-extensible static checkers to quickly ﬁnd and mark potential errorsites. Second, it uses user-extensible symbolic execution to deeply examine these potential errorsites for actual bugs. Both the checkers and the system itself are small (6KLOC total). Sys is ﬂexible, because users must be able to exploit domain- or system-speciﬁc knowledge in order to detect errors and suppress false positives in real codebases. Sys ﬁnds many security bugs (51 bugs, 43 conﬁrmed) in wellchecked code—the Chrome and Firefox web browsers—and code that some symbolic tools struggle with—the FreeBSD operating system. Sys’s most interesting results include: an exploitable, cash bountied CVE in Chrome that was ﬁxed in seven hours (and whose patch was backported in two days); a trio of bountied bugs with a CVE in Firefox; and a bountied bug in Chrome’s audio support.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Fraser",
            "lastName": "Brown"
          },
          {
            "firstName": "Deian",
            "lastName": "Stefan"
          },
          {
            "firstName": "Dawson",
            "lastName": "Engler"
          }
        ],
        "Attachments": {
          "UFJIPHQW": {
            "Version": 554,
            "ContentType": "application/pdf",
            "Filename": "Brown et al. - Sys a StaticSymbolic Tool for Finding Good Bugs .pdf"
          }
        }
      },
      "RC5JWK7Y": {
        "Version": 250,
        "Title": "Send Hardest Problems My Way: Probabilistic Path Prioritization for Hybrid Fuzzing",
        "Abstract": "Hybrid fuzzing which combines fuzzing and concolic execution has become an advanced technique for software vulnerability detection. Based on the observation that fuzzing and concolic execution are complementary in nature, the stateof-the-art hybrid fuzzing systems deploy “demand launch” and “optimal switch” strategies. Although these ideas sound intriguing, we point out several fundamental limitations in them, due to oversimpliﬁed assumptions. We then propose a novel “discriminative dispatch” strategy to better utilize the capability of concolic execution. We design a novel Monte Carlo based probabilistic path prioritization model to quantify each path’s difﬁculty and prioritize them for concolic execution. This model treats fuzzing as a random sampling process. It calculates each path’s probability based on the sampling information. Finally, our model prioritizes and assigns the most difﬁcult paths to concolic execution. We implement a prototype system DigFuzz and evaluate our system with two representative datasets. Results show that the concolic execution in DigFuzz outperforms than those in state-of-the-art hybrid fuzzing systems in every major aspect. In particular, the concolic execution in DigFuzz contributes to discovering more vulnerabilities (12 vs. 5) and producing more code coverage (18.9% vs. 3.8%) on the CQE dataset than the concolic execution in Driller.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Lei",
            "lastName": "Zhao"
          },
          {
            "firstName": "Yue",
            "lastName": "Duan"
          },
          {
            "firstName": "Heng",
            "lastName": "Yin"
          },
          {
            "firstName": "Jifeng",
            "lastName": "Xuan"
          }
        ],
        "Attachments": {
          "T6CKGJEY": {
            "Version": 251,
            "ContentType": "application/pdf",
            "Filename": "Zhao et al. - 2019 - Send Hardest Problems My Way Probabilistic Path P.pdf"
          }
        }
      },
      "RCRCQPVZ": {
        "Version": 436,
        "Title": "Probabilistic Graphical Models",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Luis Enrique",
            "lastName": "Sucar"
          }
        ],
        "Attachments": {
          "6XCIF89R": {
            "Version": 437,
            "ContentType": "application/pdf",
            "Filename": "Sucar - 2015 - Probabilistic Graphical Models.pdf"
          }
        }
      },
      "RJXBL54P": {
        "Version": 333,
        "Title": "Jitk: A Trustworthy In-Kernel Interpreter Infrastructure",
        "Abstract": "Modern operating systems run multiple interpreters in the kernel, which enable user-space applications to add new functionality or specialize system policies. The correctness of such interpreters is critical to the overall system security: bugs in interpreters could allow adversaries to compromise user-space applications and even the kernel. Jitk is a new infrastructure for building in-kernel interpreters that guarantee functional correctness as they compile user-space policies down to native instructions for execution in the kernel. To demonstrate Jitk, we implement two interpreters in the Linux kernel, BPF and INET-DIAG, which are used for network and system call filtering and socket monitoring, respectively. To help application developers write correct filters, we introduce a high-level rule language, along with a proof that Jitk correctly translates high-level rules all the way to native machine code, and demonstrate that this language can be integrated into OpenSSH with tens of lines of code. We built a prototype of Jitk on top of the CompCert verified compiler and integrated it into the Linux kernel. Experimental results show that Jitk is practical, fast, and trustworthy.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Xi",
            "lastName": "Wang"
          },
          {
            "firstName": "David",
            "lastName": "Lazar"
          },
          {
            "firstName": "Nickolai",
            "lastName": "Zeldovich"
          },
          {
            "firstName": "Adam",
            "lastName": "Chlipala"
          },
          {
            "firstName": "Zachary",
            "lastName": "Tatlock"
          }
        ],
        "Attachments": {
          "8M89ACNI": {
            "Version": 334,
            "ContentType": "application/pdf",
            "Filename": "Wang et al. - Jitk A Trustworthy In-Kernel Interpreter Infrastr.pdf"
          }
        }
      },
      "RNME55FH": {
        "Version": 347,
        "Title": "{MOPT}: Optimized Mutation Scheduling for Fuzzers",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Chenyang",
            "lastName": "Lyu"
          },
          {
            "firstName": "Shouling",
            "lastName": "Ji"
          },
          {
            "firstName": "Chao",
            "lastName": "Zhang"
          },
          {
            "firstName": "Yuwei",
            "lastName": "Li"
          },
          {
            "firstName": "Wei-Han",
            "lastName": "Lee"
          },
          {
            "firstName": "Yu",
            "lastName": "Song"
          },
          {
            "firstName": "Raheem",
            "lastName": "Beyah"
          }
        ],
        "Attachments": {
          "VZEYVPPA": {
            "Version": 255,
            "ContentType": "application/pdf",
            "Filename": "Lyu et al. - 2019 - {MOPT} Optimized Mutation Scheduling for Fuzzers.pdf"
          }
        }
      },
      "RPITZM5N": {
        "Version": 357,
        "Title": "Assurances in Software Testing: A Roadmap",
        "Abstract": "As researchers, we already understand how to make testing more effective and efficient at finding bugs. However, as fuzzing (i.e., automated testing) becomes more widely adopted in practice, practitioners are asking: Which assurances does a fuzzing campaign provide that exposes no bugs? When is it safe to stop the fuzzer with a reasonable residual risk? How much longer should the fuzzer be run to achieve sufficient coverage? It is time for us to move beyond the innovation of increasingly sophisticated testing techniques, to build a body of knowledge around the explication and quantification of the testing process, and to develop sound methodologies to estimate and extrapolate these quantities with measurable accuracy. In our vision of the future practitioners leverage a rich statistical toolset to assess residual risk, to obtain statistical guarantees, and to analyze the cost-benefit trade-off for ongoing fuzzing campaigns. We propose a general framework as a first starting point to tackle this fundamental challenge and discuss a large number of concrete opportunities for future research.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marcel",
            "lastName": "Böhme"
          }
        ],
        "Attachments": {
          "8EV4V43T": {
            "Version": 356,
            "ContentType": "application/pdf",
            "Filename": "8EV4V43T.pdf"
          }
        }
      },
      "RRW8NADY": {
        "Version": 196,
        "Title": "Grand Pwning Unit: Accelerating Microarchitectural Attacks with the GPU",
        "Abstract": "Dark silicon is pushing processor vendors to add more specialized units such as accelerators to commodity processor chips. Unfortunately this is done without enough care to security. In this paper we look at the security implications of integrated Graphical Processor Units (GPUs) found in almost all mobile processors. We demonstrate that GPUs, already widely employed to accelerate a variety of benign applications such as image rendering, can also be used to \"accelerate\" microarchitectural attacks (i.e., making them more effective) on commodity platforms. In particular, we show that an attacker can build all the necessary primitives for performing effective GPU-based microarchitectural attacks and that these primitives are all exposed to the web through standardized browser extensions, allowing side-channel and Rowhammer attacks from JavaScript. These attacks bypass state-of-the-art mitigations and advance existing CPU-based attacks: we show the first end-to-end microarchitectural compromise of a browser running on a mobile phone in under two minutes by orchestrating our GPU primitives. While powerful, these GPU primitives are not easy to implement due to undocumented hardware features. We describe novel reverse engineering techniques for peeking into the previously unknown cache architecture and replacement policy of the Adreno 330, an integrated GPU found in many common mobile platforms. This information is necessary when building shader programs implementing our GPU primitives. We conclude by discussing mitigations against GPU-enabled attackers.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Pietro",
            "lastName": "Frigo"
          },
          {
            "firstName": "Cristiano",
            "lastName": "Giuffrida"
          },
          {
            "firstName": "Herbert",
            "lastName": "Bos"
          },
          {
            "firstName": "Kaveh",
            "lastName": "Razavi"
          }
        ],
        "Attachments": {
          "S3Q2SX7W": {
            "Version": 197,
            "ContentType": "application/pdf",
            "Filename": "Frigo et al. - 2018 - Grand Pwning Unit Accelerating Microarchitectural.pdf"
          }
        }
      },
      "RVGZ9Y5U": {
        "Version": 567,
        "Title": "VUzzer: Application-aware Evolutionary Fuzzing",
        "Abstract": "Fuzzing is an effective software testing technique to ﬁnd bugs. Given the size and complexity of real-world applications, modern fuzzers tend to be either scalable, but not effective in exploring bugs that lie deeper in the execution, or capable of penetrating deeper in the application, but not scalable.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Sanjay",
            "lastName": "Rawat"
          },
          {
            "firstName": "Vivek",
            "lastName": "Jain"
          },
          {
            "firstName": "Ashish",
            "lastName": "Kumar"
          },
          {
            "firstName": "Lucian",
            "lastName": "Cojocar"
          },
          {
            "firstName": "Cristiano",
            "lastName": "Giuffrida"
          },
          {
            "firstName": "Herbert",
            "lastName": "Bos"
          }
        ],
        "Attachments": {
          "6CWXNC37": {
            "Version": 568,
            "ContentType": "application/pdf",
            "Filename": "Rawat et al. - 2017 - VUzzer Application-aware Evolutionary Fuzzing.pdf"
          }
        }
      },
      "RXHA83H9": {
        "Version": 120,
        "Title": "A Practical Guide for Using Statistical Tests to Assess Randomized Algorithms in Software Engineering",
        "Abstract": "Randomized algorithms have been used to successfully address many different types of software engineering problems. This type of algorithms employ a degree of randomness as part of their logic. Randomized algorithms are useful for difficult problems where a precise solution cannot be derived in a deterministic way within reasonable time. However, randomized algorithms produce different results on every run when applied to the same problem instance. It is hence important to assess the effectiveness of randomized algorithms by collecting data from a large enough number of runs. The use of rigorous statistical tests is then essential to provide support to the conclusions derived by analyzing such data. In this paper, we provide a systematic review of the use of randomized algorithms in selected software engineering venues in 2009. Its goal is not to perform a complete survey but to get a representative snapshot of current practice in software engineering research. We show that randomized algorithms are used in a significant percentage of papers but that, in most cases, randomness is not properly accounted for. This casts doubts on the validity of most empirical results assessing randomized algorithms. There are numerous statistical tests, based on different assumptions, and it is not always clear when and how to use these tests. We hence provide practical guidelines to support empirical research on randomized algorithms in software engineering",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Andrea",
            "lastName": "Arcuri"
          },
          {
            "firstName": "Lionel",
            "lastName": "Briand"
          }
        ],
        "Attachments": {
          "CUG2VITV": {
            "Version": 121,
            "ContentType": "application/pdf",
            "Filename": "Arcuri and Briand - 2011 - A Practical Guide for Using Statistical Tests to A.pdf"
          }
        }
      },
      "S7M6BRP2": {
        "Version": 410,
        "Title": "Boosting Fuzzer Efficiency: An Information Theoretic Perspective",
        "Abstract": "In this paper, we take the fundamental perspective of fuzzing as a learning process. Suppose before fuzzing, we know nothing about the behaviors of a program P: What does it do? Executing the first test input, we learn how P behaves for this input. Executing the next input, we either observe the same or discover a new behavior. As such, each execution reveals “some amount” of information about P’s behaviors. A classic measure of information is Shannon’s entropy. Measuring entropy allows us to quantify how much is learned from each generated test input about the behaviors of the program. Within a probabilistic model of fuzzing, we show how entropy also measures fuzzer efficiency. Specifically, it measures the general rate at which the fuzzer discovers new behaviors. Intuitively, efficient fuzzers maximize information.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marcel",
            "lastName": "Böhme"
          },
          {
            "firstName": "Valentin",
            "lastName": "Manes"
          },
          {
            "firstName": "Sang-Kil",
            "lastName": "Cha"
          }
        ],
        "Attachments": {
          "TWM2N45Y": {
            "Version": 411,
            "ContentType": "application/pdf",
            "Filename": "Böhme et al. - 2020 - Boosting Fuzzer Efficiency An Information Theoret.pdf"
          }
        }
      },
      "S9PJM45R": {
        "Version": 296,
        "Title": "Language Processing with Perl and Prolog",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Pierre M.",
            "lastName": "Nugues"
          }
        ],
        "Attachments": {
          "K6BWH3ED": {
            "Version": 297,
            "ContentType": "application/pdf",
            "Filename": "Nugues - 2014 - Language Processing with Perl and Prolog.pdf"
          }
        }
      },
      "SMUK49AA": {
        "Version": 648,
        "Title": "NYX: Greybox Hypervisor Fuzzing using Fast Snapshots and Afﬁne Types",
        "Abstract": "A hypervisor (also know as virtual machine monitor, VMM) enforces the security boundaries between different virtual machines (VMs) running on the same physical machine. A malicious user who is able to run her own kernel on a cloud VM can interact with a large variety of attack surfaces. Exploiting a software fault in any of these surfaces leads to full access to all other VMs that are co-located on the same host. Hence, the efﬁcient detection of hypervisor vulnerabilities is crucial for the security of the modern cloud infrastructure. Recent work showed that blind fuzzing is the most efﬁcient approach to identify security issues in hypervisors, mainly due to an outstandingly high test throughput.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Sergej",
            "lastName": "Schumilo"
          },
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Ali",
            "lastName": "Abbasi"
          },
          {
            "firstName": "Simon",
            "lastName": "Wörner"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "5QYMYV4P": {
            "Version": 665,
            "ContentType": "application/pdf",
            "Filename": "Schumilo et al. - NYX Greybox Hypervisor Fuzzing using Fast Snapsho.pdf"
          }
        }
      },
      "SSCMZQIS": {
        "Version": 259,
        "Title": "MEUZZ: Smart Seed Scheduling for Hybrid Fuzzing",
        "Abstract": "Seed scheduling is a prominent factor in determining the yields of hybrid fuzzing. Existing hybrid fuzzers schedule seeds based on fixed heuristics that aim to predict input utilities. However, such heuristics are not generalizable as there exists no one-size-fits-all rule applicable to different programs. They may work well on the programs from which they were derived, but not others. To overcome this problem, we design a Machine learning-Enhanced hybrid fUZZing system (MEUZZ), which employs supervised machine learning for adaptive and generalizable seed scheduling. MEUZZ determines which new seeds are expected to produce better fuzzing yields based on the knowledge learned from past seed scheduling decisions made on the same or similar programs. MEUZZ's learning is based on a series of features extracted via code reachability and dynamic analysis, which incurs negligible runtime overhead (in microseconds). Moreover, MEUZZ automatically infers the data labels by evaluating the fuzzing performance of each selected seed. As a result, MEUZZ is generally applicable to, and performs well on, various kinds of programs. Our evaluation shows MEUZZ significantly outperforms the state-of-the-art grey-box and hybrid fuzzers, achieving 27.1% more code coverage than QSYM. The learned models are reusable and transferable, which boosts fuzzing performance by 7.1% on average and improves 68% of the 56 cross-program fuzzing campaigns. MEUZZ discovered 47 deeply hidden and previously unknown bugs--with 21 confirmed and fixed by the developers--when fuzzing 8 well-tested programs with the same configurations as used in previous work.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yaohui",
            "lastName": "Chen"
          },
          {
            "firstName": "Mansour",
            "lastName": "Ahmadi"
          },
          {
            "firstName": "Reza Mirzazade",
            "lastName": "farkhani"
          },
          {
            "firstName": "Boyu",
            "lastName": "Wang"
          },
          {
            "firstName": "Long",
            "lastName": "Lu"
          }
        ],
        "Attachments": {
          "5QVMFWK2": {
            "Version": 261,
            "ContentType": "application/pdf",
            "Filename": "Chen et al. - 2020 - MEUZZ Smart Seed Scheduling for Hybrid Fuzzing.pdf"
          }
        }
      },
      "SVN849KQ": {
        "Version": 57,
        "Title": "{QSYM} : A Practical Concolic Execution Engine Tailored for Hybrid Fuzzing",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Insu",
            "lastName": "Yun"
          },
          {
            "firstName": "Sangho",
            "lastName": "Lee"
          },
          {
            "firstName": "Meng",
            "lastName": "Xu"
          },
          {
            "firstName": "Yeongjin",
            "lastName": "Jang"
          },
          {
            "firstName": "Taesoo",
            "lastName": "Kim"
          }
        ],
        "Attachments": {
          "EDNK2VPX": {
            "Version": 58,
            "ContentType": "application/pdf",
            "Filename": "Yun et al. - 2018 - {QSYM}  A Practical Concolic Execution Engine Tai.pdf"
          }
        }
      },
      "TATD8FM7": {
        "Version": 485,
        "Title": "Amplification Hell: Revisiting Network Protocols for DDoS Abuse",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Christian",
            "lastName": "Rossow"
          }
        ],
        "Attachments": {
          "RMM472PC": {
            "Version": 486,
            "ContentType": "application/pdf",
            "Filename": "Rossow - 2014 - Amplification Hell Revisiting Network Protocols f.pdf"
          }
        }
      },
      "TJSWVGQU": {
        "Version": 488,
        "Title": "Exit from Hell? Reducing the Impact of Ampliﬁcation DDoS Attacks",
        "Abstract": "Ampliﬁcation vulnerabilities in many UDP-based network protocols have been abused by miscreants to launch Distributed Denial-of-Service (DDoS) attacks that exceed hundreds of Gbps in trafﬁc volume. However, up to now little is known about the nature of the ampliﬁcation sources and about countermeasures one can take to remediate these vulnerable systems. Is there any hope in mitigating the ampliﬁcation problem? In this paper, we aim to answer this question and tackle the problem from four different angles. In a ﬁrst step, we monitored and classiﬁed ampliﬁcation sources, showing that ampliﬁers have a high diversity in terms of operating systems and architectures. Based on these results, we then collaborated with the security community in a large-scale campaign to reduce the number of vulnerable NTP servers by more than 92%. To assess possible next steps of attackers, we evaluate ampliﬁcation vulnerabilities in the TCP handshake and show that attackers can abuse millions of hosts to achieve 20x ampliﬁcation. Lastly, we analyze the root cause for ampliﬁcation attacks: networks that allow IP address spooﬁng. We deploy a method to identify spooﬁng-enabled networks from remote and reveal up to 2,692 Autonomous Systems that lack egress ﬁltering.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marc",
            "lastName": "Kuhrer"
          },
          {
            "firstName": "Thomas",
            "lastName": "Hupperich"
          },
          {
            "firstName": "Christian",
            "lastName": "Rossow"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "8V7S4UX6": {
            "Version": 489,
            "ContentType": "application/pdf",
            "Filename": "Kuhrer et al. - Exit from Hell Reducing the Impact of Ampliﬁcatio.pdf"
          }
        }
      },
      "TS2H43YK": {
        "Version": 523,
        "Title": "BaseSAFE: Baseband SAnitized Fuzzing through Emulation",
        "Abstract": "Rogue base stations are an effective attack vector. Cellular basebands represent a critical part of the smartphone's security: they parse large amounts of data even before authentication. They can, therefore, grant an attacker a very stealthy way to gather information about calls placed and even to escalate to the main operating system, over-the-air. In this paper, we discuss a novel cellular fuzzing framework that aims to help security researchers find critical bugs in cellular basebands and similar embedded systems. BaseSAFE allows partial rehosting of cellular basebands for fast instrumented fuzzing off-device, even for closed-source firmware blobs. BaseSAFE's sanitizing drop-in allocator, enables spotting heap-based buffer-overflows quickly. Using our proof-of-concept harness, we fuzzed various parsers of the Nucleus RTOS-based MediaTek cellular baseband that are accessible from rogue base stations. The emulator instrumentation is highly optimized, reaching hundreds of executions per second on each core for our complex test case, around 15k test-cases per second in total. Furthermore, we discuss attack vectors for baseband modems. To the best of our knowledge, this is the first use of emulation-based fuzzing for security testing of commercial cellular basebands. Most of the tooling and approaches of BaseSAFE are also applicable for other low-level kernels and firmware. Using BaseSAFE, we were able to find memory corruptions including heap out-of-bounds writes using our proof-of-concept fuzzing harness in the MediaTek cellular baseband. BaseSAFE, the harness, and a large collection of LTE signaling message test cases will be released open-source upon publication of this paper.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Dominik",
            "lastName": "Maier"
          },
          {
            "firstName": "Lukas",
            "lastName": "Seidel"
          },
          {
            "firstName": "Shinjo",
            "lastName": "Park"
          }
        ],
        "Attachments": {
          "2AMBNV3Q": {
            "Version": 524,
            "ContentType": "application/pdf",
            "Filename": "Maier et al. - 2020 - BaseSAFE Baseband SAnitized Fuzzing through Emula.pdf"
          }
        }
      },
      "U42RLR2Y": {
        "Version": 208,
        "Title": "Fuzzing with Stochastic Feedback Processes",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Konstantin",
            "lastName": "Böttinger"
          }
        ],
        "Attachments": {
          "RL42NMXB": {
            "Version": 210,
            "ContentType": "application/pdf",
            "Filename": "Böttinger - Fuzzing with Stochastic Feedback Processes.pdf"
          }
        }
      },
      "UIG44APV": {
        "Version": 652,
        "Title": "Magma: A Ground-Truth Fuzzing Benchmark",
        "Abstract": "High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash count, perhaps the most commonly-used performance metric, is inaccurate due to imperfections in deduplication techniques. Additionally, the lack of a unified set of targets results in ad hoc evaluations that hinder fair comparison. We tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. Magma is an open benchmark consisting of seven targets that perform a variety of input manipulations and complex computations, presenting a challenge to state-of-the-art fuzzers. We evaluate seven widely-used mutation-based fuzzers (AFL, AFLFast, AFL++, FairFuzz, MOpt-AFL, honggfuzz, and SymCC-AFL) against Magma over 200,000 CPU-hours. Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzers' exploration and detection capabilities. This provides insight into fuzzer performance evaluation, highlighting the importance of ground truth in performing more accurate and meaningful evaluations.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Ahmad",
            "lastName": "Hazimeh"
          },
          {
            "firstName": "Adrian",
            "lastName": "Herrera"
          },
          {
            "firstName": "Mathias",
            "lastName": "Payer"
          }
        ],
        "Attachments": {
          "2LU5C4RP": {
            "Version": 664,
            "ContentType": "application/pdf",
            "Filename": "Hazimeh et al. - 2020 - Magma A Ground-Truth Fuzzing Benchmark.pdf"
          }
        }
      },
      "V5IPYZIH": {
        "Version": 232,
        "Title": "REDQUEEN: Fuzzing with Input-to-State Correspondence",
        "Abstract": "Automated software testing based on fuzzing has experienced a revival in recent years. Especially feedback-driven fuzzing has become well-known for its ability to efﬁciently perform randomized testing with limited input corpora. Despite a lot of progress, two common problems are magic numbers and (nested) checksums. Computationally expensive methods such as taint tracking and symbolic execution are typically used to overcome such roadblocks. Unfortunately, such methods often require access to source code, a rather precise description of the environment (e.g., behavior of library calls or the underlying OS), or the exact semantics of the platform’s instruction set.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Sergej",
            "lastName": "Schumilo"
          },
          {
            "firstName": "Tim",
            "lastName": "Blazytko"
          },
          {
            "firstName": "Robert",
            "lastName": "Gawlik"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "57H5W2CD": {
            "Version": 233,
            "ContentType": "application/pdf",
            "Filename": "Aschermann et al. - REDQUEEN Fuzzing with Input-to-State Corresponden.pdf"
          }
        }
      },
      "V99QQ2QF": {
        "Version": 450,
        "Title": "A systematic review of fuzzing based on machine learning techniques",
        "Abstract": "Security vulnerabilities play a vital role in network security system. Fuzzing technology is widely used as a vulnerability discovery technology to reduce damage in advance. However, traditional fuzzing techniques have many challenges, such as how to mutate input seed files, how to increase code coverage, and how to effectively bypass verification. Machine learning technology has been introduced as a new method into fuzzing test to alleviate these challenges. This paper reviews the research progress of using machine learning technology for fuzzing test in recent years, analyzes how machine learning improve the fuzz process and results, and sheds light on future work in fuzzing. Firstly, this paper discusses the reasons why machine learning techniques can be used for fuzzing scenarios and identifies six different stages in which machine learning have been used. Then this paper systematically study the machine learning based fuzzing models from selection of machine learning algorithm, pre-processing methods, datasets, evaluation metrics, and hyperparameters setting. Next, this paper assesses the performance of the machine learning models based on the frequently used evaluation metrics. The results of the evaluation prove that machine learning technology has an acceptable capability of categorize predictive for fuzzing. Finally, the comparison on capability of discovering vulnerabilities between traditional fuzzing tools and machine learning based fuzzing tools is analyzed. The results depict that the introduction of machine learning technology can improve the performance of fuzzing. However, there are still some limitations, such as unbalanced training samples and difficult to extract the characteristics related to vulnerabilities.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Yan",
            "lastName": "Wang"
          },
          {
            "firstName": "Peng",
            "lastName": "Jia"
          },
          {
            "firstName": "Luping",
            "lastName": "Liu"
          },
          {
            "firstName": "Jiayong",
            "lastName": "Liu"
          }
        ],
        "Attachments": {
          "XCFTWS6F": {
            "Version": 451,
            "ContentType": "application/pdf",
            "Filename": "Wang et al. - 2019 - A systematic review of fuzzing based on machine le.pdf"
          }
        }
      },
      "VV7CF8HT": {
        "Version": 447,
        "Title": "FuzzerGym: A Competitive Framework for Fuzzing and Learning",
        "Abstract": "Fuzzing is a commonly used technique designed to test software by automatically crafting program inputs. Currently, the most successful fuzzing algorithms emphasize simple, low-overhead strategies with the ability to efficiently monitor program state during execution. Through compile-time instrumentation, these approaches have access to numerous aspects of program state including coverage, data flow, and heterogeneous fault detection and classification. However, existing approaches utilize blind random mutation strategies when generating test inputs. We present a different approach that uses this state information to optimize mutation operators using reinforcement learning (RL). By integrating OpenAI Gym with libFuzzer we are able to simultaneously leverage advancements in reinforcement learning as well as fuzzing to achieve deeper coverage across several varied benchmarks. Our technique connects the rich, efficient program monitors provided by LLVM Santizers with a deep neural net to learn mutation selection strategies directly from the input data. The cross-language, asynchronous architecture we developed enables us to apply any OpenAI Gym compatible deep reinforcement learning algorithm to any fuzzing problem with minimal slowdown.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "William",
            "lastName": "Drozd"
          },
          {
            "firstName": "Michael D.",
            "lastName": "Wagner"
          }
        ],
        "Attachments": {
          "5D9UT6I4": {
            "Version": 448,
            "ContentType": "application/pdf",
            "Filename": "Drozd and Wagner - 2018 - FuzzerGym A Competitive Framework for Fuzzing and.pdf"
          }
        }
      },
      "VWQP4VTA": {
        "Version": 641,
        "Title": "A history of the Oz multiparadigm language",
        "Abstract": "Oz is a programming language designed to support multiple programming paradigms in a clean factored way that is easy to program despite its broad coverage. It started in 1991 as a collaborative effort by the DFKI (Germany) and SICS (Sweden) and led to an influential system, Mozart, that was released in 1999 and widely used in the 2000s for practical applications and education. We give the history of Oz as it developed from its origins in logic programming, starting with Prolog, followed by concurrent logic programming and constraint logic programming, and leading to its two direct precursors, the concurrent constraint model and the Andorra Kernel Language (AKL). We give the lessons learned from the Oz effort including successes and failures and we explain the principles underlying the Oz design. Oz is defined through a kernel language, which is a formal model similar to a foundational calculus, but that is designed to be directly useful to the programmer. The kernel language is organized in a layered structure, which makes it straightforward to write programs that use different paradigms in different parts. Oz is a key enabler for the book Concepts, Techniques, and Models of Computer Programming (MIT Press, 2004). Based on the book and the implementation, Oz has been used successfully in university-level programming courses starting from 2001 to the present day.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Peter",
            "lastName": "Van Roy"
          },
          {
            "firstName": "Seif",
            "lastName": "Haridi"
          },
          {
            "firstName": "Christian",
            "lastName": "Schulte"
          },
          {
            "firstName": "Gert",
            "lastName": "Smolka"
          }
        ],
        "Attachments": {
          "XZVTVMY5": {
            "Version": 642,
            "ContentType": "application/pdf",
            "Filename": "Van Roy et al. - 2020 - A history of the Oz multiparadigm language.pdf"
          }
        }
      },
      "WNZYDZBU": {
        "Version": 83,
        "Title": "FairFuzz: Targeting Rare Branches to Rapidly Increase Greybox Fuzz Testing Coverage",
        "Abstract": "In recent years, fuzz testing has proven itself to be one of the most effective techniques for finding correctness bugs and security vulnerabilities in practice. One particular fuzz testing tool, American Fuzzy Lop or AFL, has become popular thanks to its ease-of-use and bug-finding power. However, AFL remains limited in the depth of program coverage it achieves, in particular because it does not consider which parts of program inputs should not be mutated in order to maintain deep program coverage. We propose an approach, FairFuzz, that helps alleviate this limitation in two key steps. First, FairFuzz automatically prioritizes inputs exercising rare parts of the program under test. Second, it automatically adjusts the mutation of inputs so that the mutated inputs are more likely to exercise these same rare parts of the program. We conduct evaluation on real-world programs against state-of-the-art versions of AFL, thoroughly repeating experiments to get good measures of variability. We find that on certain benchmarks FairFuzz shows significant coverage increases after 24 hours compared to state-of-the-art versions of AFL, while on others it achieves high program coverage at a significantly faster rate.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Caroline",
            "lastName": "Lemieux"
          },
          {
            "firstName": "Koushik",
            "lastName": "Sen"
          }
        ],
        "Attachments": {
          "JUQLYQPE": {
            "Version": 84,
            "ContentType": "application/pdf",
            "Filename": "Lemieux and Sen - 2018 - FairFuzz Targeting Rare Branches to Rapidly Incre.pdf"
          }
        }
      },
      "WR4XY5JE": {
        "Version": 299,
        "Title": "Spatially structured evolutionary algorithms: artificial evolution in space and time",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marco",
            "lastName": "Tomassini"
          }
        ],
        "Attachments": {
          "RMFKNYGN": {
            "Version": 300,
            "ContentType": "application/pdf",
            "Filename": "Tomassini - 2005 - Spatially structured evolutionary algorithms arti.pdf"
          }
        }
      },
      "X79PKYLQ": {
        "Version": 433,
        "Title": "ES-Rank: evolution strategy learning to rank approach",
        "Abstract": "Learning to Rank (LTR) is one of the current problems in Information Retrieval (IR) that attracts the attention from researchers. The LTR problem is mainly about ranking the retrieved documents for users in search engines, question answering and product recommendation systems. There are a number of LTR approaches from the areas of machine learning and computational intelligence. Most approaches have the limitation of being too slow or not being very effective. This paper investigates the application of evolutionary computation, speciﬁcally a (1+1) Evolutionary Strategy called ES-Rank, to tackle the LTR problem. Experimental results from comparing the proposed method to fourteen other approaches from the literature, show that ESRank achieves the overall best performance. Three datasets (MQ2007, MQ2008 and MSLR-WEB10K) from the LETOR benchmark collection and two performance metrics, Mean Average Precision (MAP) and Normalized Discounted Cumulative Gain (NDCG) at top-10 query-document pairs retrieved, were used in the experiments. The contribution of this paper is an eﬀective and eﬃcient method for the LTR problem.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Osman Ali Sadek",
            "lastName": "Ibrahim"
          },
          {
            "firstName": "Dario",
            "lastName": "Landa-Silva"
          }
        ],
        "Attachments": {
          "ESB6N5MT": {
            "Version": 434,
            "ContentType": "application/pdf",
            "Filename": "Ibrahim and Landa-Silva - 2017 - ES-Rank evolution strategy learning to rank appro.pdf"
          }
        }
      },
      "XC2S8FE8": {
        "Version": 276,
        "Title": "FuzzGen: Automatic Fuzzer Generation",
        "Abstract": "Fuzzing is a testing technique to discover unknown vulnerabilities in software. When applying fuzzing to libraries, the core idea of supplying random input remains unchanged, yet it is non-trivial to achieve good code coverage. Libraries cannot run as standalone programs, but instead are invoked through another application. Triggering code deep in a library remains challenging as speciﬁc sequences of API calls are required to build up the necessary state. Libraries are diverse and have unique interfaces that require unique fuzzers, so far written by a human analyst.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Kyriakos K",
            "lastName": "Ispoglou"
          },
          {
            "firstName": "Daniel",
            "lastName": "Austin"
          },
          {
            "firstName": "Vishwath",
            "lastName": "Mohan"
          },
          {
            "firstName": "Mathias",
            "lastName": "Payer"
          }
        ],
        "Attachments": {
          "KUTG2GGK": {
            "Version": 274,
            "ContentType": "application/pdf",
            "Filename": "Ispoglou et al. - FuzzGen Automatic Fuzzer Generation.pdf"
          }
        }
      },
      "XDLFTNSH": {
        "Version": 110,
        "Title": "{GRIMOIRE}: Synthesizing Structure while Fuzzing",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Tim",
            "lastName": "Blazytko"
          },
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Moritz",
            "lastName": "Schlögel"
          },
          {
            "firstName": "Ali",
            "lastName": "Abbasi"
          },
          {
            "firstName": "Sergej",
            "lastName": "Schumilo"
          },
          {
            "firstName": "Simon",
            "lastName": "Wörner"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "UQ67PA7V": {
            "Version": 111,
            "ContentType": "application/pdf",
            "Filename": "Blazytko et al. - 2019 - {GRIMOIRE} Synthesizing Structure while Fuzzing.pdf"
          }
        }
      },
      "XFEVCUJX": {
        "Version": 23,
        "Title": "The Art, Science, and Engineering of Fuzzing: A Survey",
        "Abstract": "Among the many software vulnerability discovery techniques available today, fuzzing has remained highly popular due to its conceptual simplicity, its low barrier to deployment, and its vast amount of empirical evidence in discovering real-world software vulnerabilities. At a high level, fuzzing refers to a process of repeatedly running a program with generated inputs that may be syntactically or semantically malformed. While researchers and practitioners alike have invested a large and diverse effort towards improving fuzzing in recent years, this surge of work has also made it difficult to gain a comprehensive and coherent view of fuzzing. To help preserve and bring coherence to the vast literature of fuzzing, this paper presents a unified, general-purpose model of fuzzing together with a taxonomy of the current fuzzing literature. We methodically explore the design decisions at every stage of our model fuzzer by surveying the related literature and innovations in the art, science, and engineering that make modern-day fuzzers effective.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Valentin J. M.",
            "lastName": "Manes"
          },
          {
            "firstName": "HyungSeok",
            "lastName": "Han"
          },
          {
            "firstName": "Choongwoo",
            "lastName": "Han"
          },
          {
            "firstName": "Sang Kil",
            "lastName": "Cha"
          },
          {
            "firstName": "Manuel",
            "lastName": "Egele"
          },
          {
            "firstName": "Edward J.",
            "lastName": "Schwartz"
          },
          {
            "firstName": "Maverick",
            "lastName": "Woo"
          }
        ],
        "Attachments": {
          "VJJM3VBJ": {
            "Version": 24,
            "ContentType": "application/pdf",
            "Filename": "Manes et al. - 2018 - The Art, Science, and Engineering of Fuzzing A Su.pdf"
          }
        }
      },
      "XINAE7RS": {
        "Version": 467,
        "Title": "Pythia: Grammar-Based Fuzzing of REST APIs with Coverage-guided Feedback and Learning-based Mutations",
        "Abstract": "This paper introduces Pythia, the first fuzzer that augments grammar-based fuzzing with coverage-guided feedback and a learning-based mutation strategy for stateful REST API fuzzing. Pythia uses a statistical model to learn common usage patterns of a target REST API from structurally valid seed inputs. It then generates learning-based mutations by injecting a small amount of noise deviating from common usage patterns while still maintaining syntactic validity. Pythia's mutation strategy helps generate grammatically valid test cases and coverage-guided feedback helps prioritize the test cases that are more likely to find bugs. We present experimental evaluation on three production-scale, open-source cloud services showing that Pythia outperforms prior approaches both in code coverage and new bugs found. Using Pythia, we found 29 new bugs which we are in the process of reporting to the respective service owners.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Vaggelis",
            "lastName": "Atlidakis"
          },
          {
            "firstName": "Roxana",
            "lastName": "Geambasu"
          },
          {
            "firstName": "Patrice",
            "lastName": "Godefroid"
          },
          {
            "firstName": "Marina",
            "lastName": "Polishchuk"
          },
          {
            "firstName": "Baishakhi",
            "lastName": "Ray"
          }
        ],
        "Attachments": {
          "CQ3NJLC4": {
            "Version": 469,
            "ContentType": "application/pdf",
            "Filename": "Atlidakis et al. - 2020 - Pythia Grammar-Based Fuzzing of REST APIs with Co.pdf"
          }
        }
      },
      "XPMLHVWS": {
        "Version": 87,
        "Title": "Coverage-based Greybox Fuzzing As Markov Chain",
        "Abstract": "Coverage-based Greybox Fuzzing (CGF) is a random testing approach that requires no program analysis. A new test is generated by slightly mutating a seed input. If the test exercises a new and interesting path, it is added to the set of seeds; otherwise, it is discarded. We observe that most tests exercise the same few \"high-frequency\" paths and develop strategies to explore significantly more paths with the same number of tests by gravitating towards low-frequency paths. We explain the challenges and opportunities of CGF using a Markov chain model which specifies the probability that fuzzing the seed that exercises path i generates an input that exercises path j. Each state (i.e., seed) has an energy that specifies the number of inputs to be generated from that seed. We show that CGF is considerably more efficient if energy is inversely proportional to the density of the stationary distribution and increases monotonically every time that seed is chosen. Energy is controlled with a power schedule. We implemented the exponential schedule by extending AFL. In 24 hours, AFLFAST exposes 3 previously unreported CVEs that are not exposed by AFL and exposes 6 previously unreported CVEs 7x faster than AFL. AFLFAST produces at least an order of magnitude more unique crashes than AFL.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Marcel",
            "lastName": "Böhme"
          },
          {
            "firstName": "Van-Thuan",
            "lastName": "Pham"
          },
          {
            "firstName": "Abhik",
            "lastName": "Roychoudhury"
          }
        ],
        "Attachments": {
          "DUB9I8EV": {
            "Version": 89,
            "ContentType": "application/pdf",
            "Filename": "Böhme et al. - 2016 - Coverage-based Greybox Fuzzing As Markov Chain.pdf"
          }
        }
      },
      "XT78YU89": {
        "Version": 713,
        "Title": "Datalog Disassembly",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Antonio",
            "lastName": "Flores-Montoya"
          },
          {
            "firstName": "Eric",
            "lastName": "Schulte"
          }
        ],
        "Attachments": {
          "UFRJYD6W": {
            "Version": 714,
            "ContentType": "application/pdf",
            "Filename": "Flores-Montoya and Schulte - 2020 - Datalog Disassembly.pdf"
          }
        }
      },
      "XV7LKC5N": {
        "Version": 621,
        "Title": "The evolution of Forth",
        "Abstract": "Forth is unique among programming languages in that its development and proliferation has been a grass-roots effort unsupported by any major corporate or academic sponsors. Originally conceived and developed by a single individual, its later development has progressed under two significant influences: professional programmers who developed tools to solve application problems and then commercialized them, and the interests of hobbyists concerned with free distribution of Forth. These influences have produced a language markedly different from traditional programming languages.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Elizabeth D.",
            "lastName": "Rather"
          },
          {
            "firstName": "Donald R.",
            "lastName": "Colburn"
          },
          {
            "firstName": "Charles H.",
            "lastName": "Moore"
          }
        ],
        "Attachments": {
          "QJWXWXVP": {
            "Version": 622,
            "ContentType": "application/pdf",
            "Filename": "Rather et al. - 1996 - The evolution of Forth.pdf"
          }
        }
      },
      "Y6QJVP5Q": {
        "Version": 164,
        "Title": "Learn&Fuzz: Machine Learning for Input Fuzzing",
        "Abstract": "Fuzzing consists of repeatedly testing an application with modified, or fuzzed, inputs with the goal of finding security vulnerabilities in input-parsing code. In this paper, we show how to automate the generation of an input grammar suitable for input fuzzing using sample inputs and neural-network-based statistical machine-learning techniques. We present a detailed case study with a complex input format, namely PDF, and a large complex security-critical parser for this format, namely, the PDF parser embedded in Microsoft's new Edge browser. We discuss (and measure) the tension between conflicting learning and fuzzing goals: learning wants to capture the structure of well-formed inputs, while fuzzing wants to break that structure in order to cover unexpected code paths and find bugs. We also present a new algorithm for this learn&fuzz challenge which uses a learnt input probability distribution to intelligently guide where to fuzz inputs.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Patrice",
            "lastName": "Godefroid"
          },
          {
            "firstName": "Hila",
            "lastName": "Peleg"
          },
          {
            "firstName": "Rishabh",
            "lastName": "Singh"
          }
        ],
        "Attachments": {
          "AP36ZUW3": {
            "Version": 165,
            "ContentType": "application/pdf",
            "Filename": "Godefroid et al. - 2017 - Learn&Fuzz Machine Learning for Input Fuzzing.pdf"
          }
        }
      },
      "Y8N4CLW7": {
        "Version": 390,
        "Title": "MoonLight: Effective Fuzzing with Near-Optimal Corpus Distillation",
        "Abstract": "Mutation-based fuzzing typically uses an initial set of valid seed inputs from which to generate new inputs by random mutation. A given corpus of potential seeds will often contain thousands of similar inputs. This lack of diversity can lead to wasted fuzzing effort, as the fuzzer will exhaustively explore mutation from all available seeds. To address this, industrialstrength fuzzers such as American Fuzzy Lop (AFL) come with distillation tools (e.g., afl-cmin) that automatically select seeds as the smallest subset of a given corpus that triggers the same range of instrumentation data points as the full corpus. Experience suggests that minimizing both the number and cumulative size of the seeds may lead to more efﬁcient fuzzing, which we explore systematically here.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Liam",
            "lastName": "Hayes"
          },
          {
            "firstName": "Hendra",
            "lastName": "Gunadi"
          },
          {
            "firstName": "Adrian",
            "lastName": "Herrera"
          },
          {
            "firstName": "Jonathon",
            "lastName": "Milford"
          },
          {
            "firstName": "Shane",
            "lastName": "Magrath"
          },
          {
            "firstName": "Maggi",
            "lastName": "Sebastian"
          },
          {
            "firstName": "Michael",
            "lastName": "Norrish"
          },
          {
            "firstName": "Antony L.",
            "lastName": "Hosking"
          }
        ],
        "Attachments": {
          "62AJ3P2E": {
            "Version": 702,
            "ContentType": "application/pdf",
            "Filename": "Hayes et al. - 2019 - MoonLight Effective Fuzzing with Near-Optimal Cor.pdf"
          }
        }
      },
      "YCQRNZFJ": {
        "Version": 624,
        "Title": "Hygienic macro technology",
        "Abstract": "The fully parenthesized Cambridge Polish syntax of Lisp, originally regarded as a temporary expedient to be replaced by more conventional syntax, possesses a peculiar virtue: A read procedure can parse it without knowing the syntax of any expressions, statements, definitions, or declarations it may represent. The result of that parsing is a list structure that establishes a standard representation for uninterpreted abstract syntax trees. This representation provides a convenient basis for macro processing, which allows the programmer to specify that some simple piece of abstract syntax should be replaced by some other, more complex piece of abstract syntax. As is well-known, this yields an abstraction mechanism that does things that procedural abstraction cannot, such as introducing new binding structures. The existence of that standard representation for uninterpreted abstract syntax trees soon led Lisp to a greater reliance upon macros than was common in other high-level languages. The importance of those features is suggested by the ten pages devoted to macros in an earlier ACM HOPL paper, “The Evolution of Lisp.” However, na'ive macro expansion was a leaky abstraction, because the movement of a piece of syntax from one place to another might lead to the accidental rebinding of a program’s identifiers. Although this problem was recognized in the 1960s, it was 20 years before a reliable solution was discovered, and another 10 before a solution was discovered that was reliable, flexible, and efficient. In this paper, we summarize that early history with greater focus on hygienic macros, and continue the story by describing the further development, adoption, and influence of hygienic and partially hygienic macro technology in Scheme. The interplay between the desire for standardization and the development of new algorithms is a major theme of that story. We then survey the ways in which hygienic macro technology has been adapted into recent non-parenthetical languages. Finally, we provide a short history of attempts to provide a formal account of macro processing.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "William D.",
            "lastName": "Clinger"
          },
          {
            "firstName": "Mitchell",
            "lastName": "Wand"
          }
        ],
        "Attachments": {
          "3TY6KG7L": {
            "Version": 629,
            "ContentType": "application/pdf",
            "Filename": "Clinger and Wand - 2020 - Hygienic macro technology.pdf"
          }
        }
      },
      "YM45M3T5": {
        "Version": 302,
        "Title": "A field guide to genetic programming",
        "Abstract": "",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Riccardo",
            "lastName": "Poli"
          },
          {
            "firstName": "William B.",
            "lastName": "Langdon"
          },
          {
            "firstName": "Nicholas F.",
            "lastName": "McPhee"
          },
          {
            "firstName": "John R.",
            "lastName": "Koza"
          }
        ],
        "Attachments": {
          "3B2T8FI5": {
            "Version": 303,
            "ContentType": "application/pdf",
            "Filename": "Poli et al. - 2008 - A field guide to genetic programming.pdf"
          }
        }
      },
      "YZ4RBUQ5": {
        "Version": 624,
        "Title": "A history of Clojure",
        "Abstract": "Clojure was designed to be a general-purpose, practical functional language, suitable for use by professionals wherever its host language, e.g., Java, would be. Initially designed in 2005 and released in 2007, Clojure is a dialect of Lisp, but is not a direct descendant of any prior Lisp. It complements programming with pure functions of immutable data with concurrency-safe state management constructs that support writing correct multithreaded programs without the complexity of mutex locks. Clojure is intentionally hosted, in that it compiles to and runs on the runtime of another language, such as the JVM. This is more than an implementation strategy; numerous features ensure that programs written in Clojure can leverage and interoperate with the libraries of the host language directly and efficiently. In spite of combining two (at the time) rather unpopular ideas, functional programming and Lisp, Clojure has since seen adoption in industries as diverse as finance, climate science, retail, databases, analytics, publishing, healthcare, advertising and genomics, and by consultancies and startups worldwide, much to the career-altering surprise of its author. Most of the ideas in Clojure were not novel, but their combination puts Clojure in a unique spot in language design (functional, hosted, Lisp). This paper recounts the motivation behind the initial development of Clojure and the rationale for various design decisions and language constructs. It then covers its evolution subsequent to release and adoption.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Rich",
            "lastName": "Hickey"
          }
        ],
        "Attachments": {
          "24JNKX3I": {
            "Version": 628,
            "ContentType": "application/pdf",
            "Filename": "Hickey - 2020 - A history of Clojure.pdf"
          }
        }
      },
      "ZKWUIS5X": {
        "Version": 538,
        "Title": "FuzzFactory: domain-specific fuzzing with waypoints",
        "Abstract": "Coverage-guided fuzz testing has gained prominence as a highly effective method of finding security vulnerabilities such as buffer overflows in programs that parse binary data. Recently, researchers have introduced various specializations to the coverage-guided fuzzing algorithm for different domain-specific testing goals, such as finding performance bottlenecks, generating valid inputs, handling magic-byte comparisons, etc. Each such solution can require non-trivial implementation effort and produces a distinct variant of a fuzzing tool. We observe that many of these domain-specific solutions follow a common solution pattern. In this paper, we present FuzzFactory, a framework for developing domain-specific fuzzing applications without requiring changes to mutation and search heuristics. FuzzFactory allows users to specify the collection of dynamic domain-specific feedback during test execution, as well as how such feedback should be aggregated. FuzzFactory uses this information to selectively save intermediate inputs, called waypoints, to augment coverage-guided fuzzing. Such waypoints always make progress towards domain-specific multi-dimensional objectives. We instantiate six domain-specific fuzzing applications using FuzzFactory: three re-implementations of prior work and three novel solutions, and evaluate their effectiveness on benchmarks from Google's fuzzer test suite. We also show how multiple domains can be composed to perform better than the sum of their parts. For example, we combine domain-specific feedback about strict equality comparisons and dynamic memory allocations, to enable the automatic generation of LZ4 bombs and PNG bombs.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Rohan",
            "lastName": "Padhye"
          },
          {
            "firstName": "Caroline",
            "lastName": "Lemieux"
          },
          {
            "firstName": "Koushik",
            "lastName": "Sen"
          },
          {
            "firstName": "Laurent",
            "lastName": "Simon"
          },
          {
            "firstName": "Hayawardh",
            "lastName": "Vijayakumar"
          }
        ],
        "Attachments": {
          "CIMPRTW5": {
            "Version": 539,
            "ContentType": "application/pdf",
            "Filename": "Padhye et al. - 2019 - FuzzFactory domain-specific fuzzing with waypoint.pdf"
          }
        }
      },
      "ZMD99X48": {
        "Version": 317,
        "Title": "HYPER-CUBE: High-Dimensional Hypervisor Fuzzing",
        "Abstract": "Virtual machine monitors (VMMs, also called hypervisors) represent a very critical part of a modern software stack: compromising them could allow an attacker to take full control of the whole cloud infrastructure of any cloud provider. Hence their security is critical for many applications, especially in the context of Infrastructure-as-a-Service. In this paper, we present the design and implementation of HYPER-CUBE, a novel fuzzer that aims explicitly at testing hypervisors in an efﬁcient, effective, and precise way. Our approach is based on a custom operating system that implements a custom bytecode interpreter. This high-throughput design for long-running, interactive targets allows us to fuzz a large number of both open source and proprietary hypervisors. In contrast to one-dimensional fuzzers such as AFL, HYPER-CUBE can interact with any number of interfaces in any order. Our evaluation results show that we can ﬁnd more bugs (over 2×) and coverage (as much as 2×) than state-of-the-art hypervisor fuzzers. In most cases, we were even able to do so using multiple orders of magnitude less time than comparable fuzzers. HYPER-CUBE was also able to rediscover a set of well-known hypervisor vulnerabilities, such as VENOM, in less than ﬁve minutes. In total, we found 54 novel bugs, and so far obtained 43 CVEs. Our evaluation results demonstrate that nextgeneration coverage-guided fuzzers should incorporate a higherthroughput design for long-running targets such as hypervisors.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "Sergej",
            "lastName": "Schumilo"
          },
          {
            "firstName": "Cornelius",
            "lastName": "Aschermann"
          },
          {
            "firstName": "Ali",
            "lastName": "Abbasi"
          },
          {
            "firstName": "Simon",
            "lastName": "Worner"
          },
          {
            "firstName": "Thorsten",
            "lastName": "Holz"
          }
        ],
        "Attachments": {
          "7XQ5D2V2": {
            "Version": 318,
            "ContentType": "application/pdf",
            "Filename": "Schumilo et al. - 2020 - HYPER-CUBE High-Dimensional Hypervisor Fuzzing.pdf"
          }
        }
      },
      "ZUD72VIN": {
        "Version": 96,
        "Title": "Evaluating Fuzz Testing",
        "Abstract": "Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.",
        "ItemType": "",
        "Creators": [
          {
            "firstName": "George",
            "lastName": "Klees"
          },
          {
            "firstName": "Andrew",
            "lastName": "Ruef"
          },
          {
            "firstName": "Benji",
            "lastName": "Cooper"
          },
          {
            "firstName": "Shiyi",
            "lastName": "Wei"
          },
          {
            "firstName": "Michael",
            "lastName": "Hicks"
          }
        ],
        "Attachments": {
          "I35NFEUQ": {
            "Version": 97,
            "ContentType": "application/pdf",
            "Filename": "Klees et al. - 2018 - Evaluating Fuzz Testing.pdf"
          }
        }
      }
    }
  },
  "Search": null
}
